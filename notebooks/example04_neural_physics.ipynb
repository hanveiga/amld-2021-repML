{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AMLD 2021\n",
    "\n",
    "> ##### Machine Learning in Science: Encoding physical constraints and good development practices\n",
    "\n",
    "\n",
    "## Example 04 - A physics-guided multi-target neural network\n",
    "\n",
    "In this notebook, we demonstrate a method for encouraging physical consistency of predictions by neural networks. We start by demonstrating how the loss function can be modified to penalize physics violations in a single-target regressor. We then extend this method to a multi-target regressor to show how the loss function can be modified to penalize violations of conservation laws.\n",
    "\n",
    "### Workshop Organizers\n",
    "\n",
    "* Dr. Maria Han Veiga (University of Michigan, USA)\n",
    "\n",
    "* Dr. Miles Timpe (University of Zurich, Switzerland)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.8.3 (default, Jul  2 2020, 11:26:31) \n",
      "[Clang 10.0.0 ]\n"
     ]
    }
   ],
   "source": [
    "from joblib import dump, load\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "# Global seed value\n",
    "seed = 42\n",
    "\n",
    "# Set random seeds\n",
    "random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "\n",
    "# Versions can also influence reproducibility\n",
    "from sys import version\n",
    "print(f\"Python version: {version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load test data and scalers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We focus first on lr_mass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'lr_mass'\n",
    "\n",
    "features = ['mtotal', 'gamma', 'b_inf', 'v_inf',\n",
    "            'targ_core_fraction', 'targ_omega', 'targ_theta', 'targ_phi',\n",
    "            'proj_core_fraction', 'proj_omega', 'proj_theta', 'proj_phi',\n",
    "            target]\n",
    "\n",
    "x_train = pd.read_csv('../datasets/train.csv', usecols=features)\n",
    "x_test  = pd.read_csv('../datasets/test.csv', usecols=features)\n",
    "\n",
    "x_train = x_train[x_train['lr_mass'] > 0]\n",
    "x_test = x_test[x_test['lr_mass'] > 0]\n",
    "\n",
    "if target != 'lr_mass':\n",
    "    x_train.pop('lr_mass')\n",
    "    x_test.pop('lr_mass')\n",
    "\n",
    "y_train = x_train.pop(target)\n",
    "y_test  = x_test.pop(target)\n",
    "\n",
    "\n",
    "# Load scalers and scale targets\n",
    "y_scaler = load(f\"../models/y_scaler_{target}.joblib\")\n",
    "\n",
    "scaled_y_train = y_scaler.transform(y_train.values.reshape(-1, 1))\n",
    "scaled_y_test  = y_scaler.transform(y_test.values.reshape(-1, 1))\n",
    "    \n",
    "# Scale features\n",
    "x_scaler = load(f\"../models/x_scaler.joblib\")\n",
    "\n",
    "scaled_x_train = pd.DataFrame(x_scaler.transform(x_train), columns=x_train.columns)\n",
    "scaled_x_test = pd.DataFrame(x_scaler.transform(x_test), columns=x_test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup physics informed neural network\n",
    "\n",
    "Here, we will first focus on the 'lr_mass' target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model (MLP) using Keras\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Subtract, Lambda, add, ReLU\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "target = 'lr_mass'\n",
    "\n",
    "_, input_size = scaled_x_train.shape\n",
    "\n",
    "# Load previously trained model with no knowledge of physical constraints\n",
    "model_uninformed = keras.models.load_model(f\"../models/regressor_mlp_{target}.keras\")\n",
    "\n",
    "# This particular model will predict scaled target values\n",
    "y_pred_naive = model_uninformed.predict(scaled_x_test)\n",
    "\n",
    "# Apply inverse scaling to the model predictions\n",
    "y_scaler = load(f\"../models/y_scaler_{target}.joblib\")\n",
    "\n",
    "y_pred_naive = pd.Series(y_scaler.inverse_transform(y_pred_naive.flatten()), name='y_pred').values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do we encode the constraint?\n",
    "\n",
    "We want to penalise the model if the prediction of 'lr_mass' is larger than 'mtot'.   \n",
    "\n",
    "This means, if the quantity (lr_mass - mtot) is positive, then we are generating mass.   \n",
    "\n",
    "We can use a ReLU on this quantity, this way, if this quantity is positive, it will contribute to the loss. If this quantity is 0 or negative, lr_mass is smaller than mtot and it does not contribute to the loss.\n",
    "\n",
    "\n",
    "$$ ReLU(m_{lr}, m_{tot}) = \\left\\{\n",
    "\\begin{array}{ll}\n",
    "      -m_{lr} & \\text{for} \\quad m_{lr} < 0 \\\\\n",
    "      0 & \\text{for} \\quad 0 \\leq m_{lr} \\leq m_{tot} \\\\\n",
    "      m_{lr} - m_{tot} & \\text{for} \\quad m_{lr } > m_{tot}\n",
    "\\end{array} \\right. $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seriousmaria/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction r2_score: 0.9479\n"
     ]
    }
   ],
   "source": [
    "# Define model (MLP) using Keras\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "\n",
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    "\n",
    "disable_eager_execution()\n",
    "\n",
    "\n",
    "# Define custom loss function\n",
    "def custom_loss_wrapper(input_tensor, overshoot_weight=0.5,positivity_weight=0.5):\n",
    "    @tf.autograph.experimental.do_not_convert\n",
    "    def custom_loss(y_true,y_pred):\n",
    "        s1 = input_tensor[:,0] \n",
    "        overshoot = K.mean(ReLU()(Lambda(lambda x: x)(Subtract()([y_pred,s1]))))\n",
    "        positivity = K.mean(ReLU()(Lambda(lambda x: -x)(y_pred)))\n",
    "        return keras.losses.mean_squared_error(y_true, y_pred) + overshoot_weight*overshoot\\\n",
    "                + positivity_weight*positivity\n",
    "    return custom_loss\n",
    "\n",
    "\n",
    "# Define model\n",
    "inputs = Input(shape=(input_size,))\n",
    "f = Dense(24,activation='relu')(inputs)\n",
    "f = Dense(24,activation='relu')(f)\n",
    "f = Dense(24,activation='relu')(f)\n",
    "outputs = Dense(1)(f)\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "model.compile('Adam', loss=custom_loss_wrapper(inputs), metrics=[\"mse\"])\n",
    "\n",
    "\n",
    "# Fit model\n",
    "model.fit(x_train, y_train, validation_split=0.05, epochs=500,\n",
    "          callbacks=[EarlyStopping(patience=70)], verbose=0)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_phys = model.predict(x_test)\n",
    "\n",
    "\n",
    "# Calculate the quality of the predictions with the r2 score\n",
    "r2 = r2_score(y_test, y_pred_phys)\n",
    "\n",
    "# Save the model with keras\n",
    "model.save(f\"../models/regressor_mlp_customloss_{target}.keras\")\n",
    "\n",
    "print(f\"Prediction r2_score: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation plots for single-target regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1gAAAGoCAYAAABbkkSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOydeZhV5X34P++9MzBswggDgqAIuLAMREHFDQdRAeM2auKSJk1TE9PYpGnTNs1KyGabJo3pL0k1SVuTtkhicNwR3EZEJcioDMuIsjMwsu/bzNz7/v54z8t577nn3GXm3hmW7+d5eGbuvee85z3vHd7v+e5Ka40gCIIgCIIgCILQfmKdPQFBEARBEARBEISTBVGwBEEQBEEQBEEQCoQoWIIgCIIgCIIgCAVCFCxBEARBEARBEIQCIQqWIAiCIAiCIAhCgRAFSxAEQRAEQRAEoUCIgiUIOaKU+rpS6jdFGvs+pdSDORw3Vyn15wW43lCllFZKlbR3rOMBpdRYpdQbnT0PQRCOX5RStUqpews85kNKqW8Vcsz2kElGtGffz+VcpdQDSqkv5zDWCqVUVb5zCBmnSinV2N5xjheUUjcrpWZ39jyEwnBSPFwJQi4opdYD3YBhWuuD3nv3An+mta7Kdr7W+odFmlcX4JvAxBzmML0Yc8gXpZQGztVarz4erq+1rldK7VFK3aS1froz5iQIQufj7fMDgARwEHgO+KLW+kAxrqe1/nwxxm0rnSUjlFIVwKeAEdmO1VqPLv6MMqOUGgqsA0q11q3Hw/W11k8ppX6olBqrta7v6DkJhUU8WMKpRgnwN509iQC3AO9prTd39kQ6AmUoxt7zf8B9RRhXEIQTi5u01j2Bi4CLMQYsobh8GnhOa324syfSERQx+uNR4HNFGlvoQETBEk41/hX4e6VUn7APlVI/U0ptUkrtU0rVKaWucj77jlLqf73fn1dK/XXg3KVKqdu83y9QSr2glNqllFqllPp4hjlNB151xilTSv2vUmqn55V5Syk1wPvsWIiLUurTSqmFSqkfK6V2K6XWKaWmO+Oco5RaoJTar5R6USn1Czv/kPvurZT6T6VUk1Jqs1Lq+0qpeMSxC7xflyqlDiil7lRKlSulnlFKbffm8oxSarBzTq1S6gdKqdeBQ8AwpdT13trsVUr9Uin1qhu+o5T6jFKqwRtvnlLq7Kjre69rgSlKqa4Z1loQhFMEz2g1FxjjvH22Uup1b1+cr5TqB6CUelYp9UX3fKVUvVLqVs8o9FOl1DZvv6pXSo3xjnlEKfV955xblFLvejJkjVJqmvf+p5VSa73rrlNKfSJq3pn2RlcOea9TQvcCMiLuyYcdSqm1wEcD14nc97OdG0JQjvXz5MAeTw6+pjzDmlJqvVLqWud+/qCU+p23NiuUUhOccS5SSr3jffaYUur37noH7meQUmqOJ4fWKaW+lGG+Vo7s8eTIZUqp4Uqpl5WRvTuUUv+nnGcFb95fVUrVAweVUiVKqU8ppTZ453wrcG8xpdQ/eX8HO737PD3q+t7r2hzWWjgBEAVLONVYgtnA/j7i87eAjwCnA7OAx5RSZSHHzQLuti+UUqOAs4FnlVI9gBe8Y/p7x/1SKRUVFlEJrHJe/znQGxgC9AU+D0RZBS/1zu0H/Aj4T6WUcua42BvjO8AnI8YA+C3QignvuBC4HgjNVdBaT/J+Hae17qm1/j1mL/lvzBqc5c3354FTP4mxzPUC9gJ/BL7mzW8VcLk9UCl1K/B14DagAngNY9mLur59mGoBzs9wn4IgnCIopYYANwDvOG/fA/wFZm/ugi8Lfgv8mXPuOOBMTIjh9cAk4DygD3AnsDPkepcAvwP+wTtuErDekwn/DkzXWvfC7HXvRsy5Hxn2xjz5LHAjZk+fANwR+DzTvp/t3CBBOfYVoBGzfw/A7Oc64tybgdmYNXsKT3YoEz5fAzyCkcmPAtVhA3jK29PAUsz3NgX4slJqasQ1rRzp48mRNwEFPAAMAkZiZPB3AufdjVGA+mD+Hn4JfAIYiJHbZzrHfgm4FbjaG3M38IsM1wdoAIYqpU6LmLdwgiAKlnAq8m3gi8rEjKegtf5frfVOrXWr1vonQFfCH9hrgI8oz6uC2WAf11ofxQil9Vrr//bGeRuYQ7SA6gPsd163YATrCK11Qmtdp7XeF3HuBq31r7XWCYywHAgMUEqdhQmN+bbWullrvRAjuNJQxjs2Hfiy1vqg1nob8FPgrohrpuGt2Ryt9SGt9X7gBxih4vKI1nqFF28+HVihtX7ce/3vwIfOsfcBD2itG7zPf0jqekexH7OegiCcujyhlNoDLMR4Vdz82f/WWr/vhbL9AWNQA3gSOFcpda73+pPA77XWzZg9uRdwAaC8fakp5Lp/CfyX1voFrXVSa71Za/2e91kSGKOU6qa1btJar4iY+w1k3hvz4ePAg1rrTVrrXRjlAchp3488N4IwOTYQOFtr3aK1fk1rHaVgLdRaP+fJsf8BxnnvT8SE9f+7N8bjGKNhGBcDFVrr73oyby3wa/KTY6u97+6o1no78G+ky7F/99bkMEamP621Xuj9nXybVCXyPuAbWutG79ngO8AdKnN4oV1DkWMnOKJgCaccWuvlwDPAPwU/U0p9RZmwtL2egO6N8Q4Fx9gPPIu/ed+FyQEC48W51AuN2OON8wngjIgp7cYIb8v/APOA2UqpLUqpHymlSiPOPSZ4tdaHvF97Yqxlu5z3ADZFjHE2UAo0OfN9GGPhtRWfDnj/rgobQCnVXSn1sBcqsQ8T/tBHpYYZutcf5L72BK9bDeps4GfOfHZhrIuudTCMXsCeLMcIgnByc6vWuo/W+myt9RcCeUGusnIIs1/iPQD/AfgzzxtyN2YvRmv9Msar8gtgq1LqVxEehiHAmuCbXlGlOzHRCE3KhCNeAKH7a7a9MR9SxgI2OL9n3PeznBtGUI79K7AamK9MaGSavHUIfidlnhIyCNgcUMwyybFBAbn7dYz3DGeND3gGyDSUUv2VUrOVCZfcB/wv6fI/kxw7RKpn82ygxplPA6b4yoCIewB/DUWOneCIgiWcqszAhEAce2D3hNtXMZa7cq11H0womwodwYQr3O3FTncDXvHe3wS86gl4+6+n1vqvIsapx4QaAOBZ6mZqrUdhQkNuxFRnyocm4HSlVHfnvSERx24CjgL9nPmeZis9aa1He/PvqbV+LWKMr2A8fZdqrU/DD39w184Vkk2Am6Ol3NfenO4LrGE3rXVkKXal1CBMyM+qqGMEQRAy8FuMMWwKcMgJ20Jr/e9a6/HAaMx+/Q8h528ChocNrLWep7W+DuPVeQ/jXQnbX7PtjQcBd1+PMtzhjeXu+65ikXHfz3JuGEE5tl9r/RWt9TDgJuDvlFJTsowRNv8znbB3yCzH1gVkRi+t9Q3efHo6/zYSHq74gPf+WE+O/Rnp8j+THOuGiT5x5zQ9MKcybcLZo7x5IzERMFFRK8IJgihYwimJNuW9f4+Jkbb0wsSjbwdKlFLfBjLFQT+HsVB9FxNKkvTefwY4Tyn1SaVUqffvYqXUyAzjHAtDUEpNVkpVet6ffZhQi0Se97cBk2/2HaVUF08JvCni2CZgPvATpdRpXmLucKVUMDTCZSswzHndC5N3tcdL4p2RZYrPApXKJJCXAPeT+qDwEPA1m7emTDL2xzJcH6AKeNmzRAuCIOSFp1AlgZ/gea8AvP37Ui+S4CBwhPA9+T+Bv1BKTfH20TOVKXg0QJkeRz0wSs2BiPMh+974LjBJKXWWUqo3Jlcrij8AX1JKDVZKleNEbeSw70eeG0FQjt2olBrhKUf7vPvNS44Bb3rn/LUyBSVuAS6JOHYxsE+ZIhTdlCnSMUYpdXHE8dsx33VQjh3AyLEzCVeiXf4I3KSUulyZfLGZpCpkDwE/UH6BpgrvHqKuD2YN52a5rnACIAqWcCrzXaCH83oeZmN7HxMOcYTocAQbUvI4cC2moIR9fz8mWfguYAsm/OFfMPlcYTwNXOB5YMAI0z9ihFIDJocgtPpfFj4BXIYJWfg+RqGMUj4+hfH+rMSEevwRY2mN4jvAb73Qh48DD2K8eDuARcDzmSamtd4BfAxTmGMnMAqjEB71Pq/BrNlsL1RjOSZfIOr69n4fynRdQRCELPwOU7DB3XNPw3icdmNkw07gx8ETtdaLMQU0foqJfngVY4SLYbz8WzDhzlcDXwi7eA574wuYvbweqMMY9KL4NUauLQXexsgrl0z7frZzg/wOuMHz4gCcC7yIUVjeBH6pta7NMkYKXl7TbZjctj0Yj9IzhMgxL3/rJkxO3TqMLPoNJsw/bOxDmFzh1z05MhGjIF2E+e6eJcs9e3l0X8QU6GjC5E9tc+b3M0zu83yl1H6MbLw0w/XBhKY+nOm6womBis45FASho1BKfQ4YpbX+chGv8XtMv61s3qUOx8t5aAQ+obV+JdvxIedXAr/SWl+W9WBBEIQIlFKfAj6ntb6ys+cC7d8bOxKl1A+BbVrrB4t4jT8BD2mt/7tY12grSqmeGEXwXK31ujacfxPwSa11prYuwgmCKFiCcJLihUbswljzrgeeAC7TWr+T8cQOQpnyuX/ChBb+AyYUZpg+RRpVCoJwfOHlrL6M8bb8rhPnIXujhxeyuArjkbJRCsN0eBXHDsdTil7ChAb+BOOhukjLw/Upj4QICsLJyxmYnl8HMKV+/+p4Ua48LsNU3NqBCe249VR8gBAEofPxlJrtmPzOWVkOLzayN/qcjwlT3IsJs7zjeFGuPG7BhH5uwYRF3iXKlQDiwRIEQRAEQRAEQSgY4sESBEEQBEEQBEEoEJm6SZ+w9OvXTw8dOrRdYxw8eJAePXpkP/AURdYnGlmbaGRtopG1iaZQa1NXV7dDa12R73mFkCkg33EmZG2ikbWJRtYmM7I+0RRbrpyUCtbQoUNZsmRJu8aora2lqqqqMBM6CZH1iUbWJhpZm2hkbaIp1NoopTa05bxCyBSQ7zgTsjbRyNpEI2uTGVmfaIotVyREUBAEQRAEQRAEoUCIgiUIgiAIgiAIglAgRMESBEEQBEEQBEEoEKJgCYIgCIIgCIIgFAhRsARBEARBEARBEAqEKFiCIAiCIAiCIAgFQhQsQRAEQRAEQRCEAiEKliAIgiAIgiAIQoEQBUsQBEEQBEEQBKFAiIIlCIIgCIIgCIJQIDpVwVJK/ZdSaptSannE51VKqb1KqXe9f9/u6DkKgiAIgiAIgiDkSkknX/8R4OfA7zIc85rW+saOmY4gCIIgCIIgCELb6VQPltZ6AbCrM+cgCIIgCIIgCIJQKJTWunMnoNRQ4Bmt9ZiQz6qAOUAjsAX4e631iohxPgd8DmDAgAHjZ8+e3a55HThwgJ49e7ZrjJMZWZ9oZG2ikbWJRtYmmkKtzeTJk+u01hNyObbQMgXkO86ErE00sjbRyNpkJtv6HGqBgy3QoxS6l3bgxI4Dii1XjncF6zQgqbU+oJS6AfiZ1vrcbGNOmDBBL1mypF3zqq2tpaqqql1jnMzI+kQjaxONrE00sjbRFGptlFI5K1guhZApIN9xJmRtopG1iUbWJjOZ1qeuCe55HFoSUBqHWbfB+IEdO7/OpNhy5biuIqi13qe1PuD9/hxQqpTq18nTEgRBEARBEIQTlkWNRrlKaPNzUWNnz+jk4rhWsJRSZyillPf7JZj57uzcWQmCIAiCIAjCicvEwcZzFVfm58TBnT2jk4tOrSKolHoUqAL6KaUagRlAKYDW+iHgDuCvlFKtwGHgLt3ZMY2CIAiCIAiCcAIzfqAJC1zUaJSrUyk8sCPoVAVLa313ls9/jinjLgiCIAiCIAhCgRg/UBSrYnFchwgKgiAIgiAIgiCcSIiCJQiCIAiCIAiCUCBEwRIEQRAEQRAEQSgQomAJgiAIgiAIgiAUCFGwBEEQBEEQBEEQCoQoWIIgCIIgCIIgCAVCFCxBEARBEARBEIQCIQqWIAiCIAiCIAhCgRAFSxAEQRAEQRCETqGuCX7xlvl5slDS2RMQBEEQBEEQBOHUo64J7nkcWhJQGodZt8H4gZ09q/YjHixBEARBEARBEDqcRY1GuUpo83NRY2fPqDCIB0sQBEEQBEEQhA5n4mDjucLzYE0c3NkzKgyiYAmCIAiCIAiC0OGMH2jCAhc1GuXqZAgPBFGwBEEQBEEQBEHoJMYPPHkUK4vkYAmCIAiCIAiCIBQIUbAEQRAEQRAEQRAKhChYgiAIgiAIgiAIBUJysARBEARBEARBOKmpa/KLaRQbUbAEQRAEQRAEQTgpcBUpWzwj2ND4p+cVdw6iYAmCIAiCIAiCcMITVKRm3WaULLehMQk42FLceUgOliAIgiAIgiAIJzyuItWSMK/Bb2gcV+Znj9LizkM8WIIgCIIgCIIgnPBYRQrPg2XzrYINjfevKu48RMESBEEQBEEQBOGEJ6hIuQ2M3YbGtaJgCYIgCIIgCIIgZMdVpDoLycESBEEQBEEQBEEoEKJgCYIgCIIgCIJw0lLXBL94y/zsCCREUBAEQRAEQRCEk5Kw0u3FRjxYgiAIgiAIgiCclCxqhOZWU7q9udUv3V5MRMESBEEQBEEQBOGEJioMsLwMkt7vSe91sZEQQUEQBEEQBEEQjnvqmsJLsIeFAdrPdx8xHqUkEFPm9aAiz1MULEEQBEEQBEEQjmsyKVGLGs37CQ0kzGv72cTB0KXEP08aDQuCIAiCIAiCcMqTTYkqjZv3rRJlsc2H5zR03FxFwRIEQRAEQRAE4bgmFyUqLHzQMqfBKGhzGuCn5xV3rqJgCYIgCIIgCIIAROc5dTbZlKjxA9PzsuyxQe/XwZbizlUULEEQBEEQBEEQMuY5HQ8ElagogvcxY1Kq96tHaXHnKQqWIAiCIAiCIAgZ85xOJNz70AmYuxo+PQ5WbofpI6D7zuJeXxQsQRAEQRAEQTiJqGuC7YfMz3wUpEx5TicS9j50ApIaXtsICzaacu2LtxQ/B0saDQuCIAiCIAjCSYINj9t6wPwMNt7NhM1z+spl+YcHRjX67QzsfVw5xCg72ns/ifFsSQ6WIAiCIAiCIAg5YcPjNOZnvmF+ueY5uRyPuVvjB8KXJxqPVbPnyYohOViCIAiCIAiCIOSBDY9TFDbML1N1weMhdytsfm7lwfIyWL7dOzhR3LmIgiUIgiAIgiAIHsdrmfJcsUrF2ndg1lWFuYdsHqrOzt3KND/7c04D/GEFtCZh0KD889PyQRQsQRAEQRAEQeD4DHVrC+MHwv5VhZt7Ng9VLo1+i0mm+dnv9Eirf7zWRuESBUsQBEEQBEEQisjxEOp2PBL0UJWXmYIWwXC8zlqrTB60RY2pylVHIAqWIAiCIAiCIND5oW7HK8FcppkLcvfydUTIpZ3fnIbU92ctg8cbws/p1aU4cwFRsARBEARBEAQB6PxQt+MZ66H6xVuZvXyuQgUdG3I5p8Fca04DTBsOT6yKPvbhOrh+eHHmIwqWIAiCIAiCIHh0ZqhbsSiEF8mOUV4W7eUL5rDdPrLjQi7d8M5Ea2blCkwZ+2LNRxQsQRAEQRAEQThJKUThjuAYMybB7iPpClswhw2KH3IZVPwSeeRbFSsEtFMVLKXUfwE3Atu01mNCPlfAz4AbgEPAp7XWb3fsLAVBEARBEAThxKQQhTuCY+w+AvdfnH5cMIft9pHmX7FCLoOK36fHwa/f9uaZA6t2nJwerEeAnwO/i/h8OnCu9+9S4D+8n4IgCIIgCIJwwlPsIhBRhTvyuW6uxT+ictjac1/BnC53bFfx04n8lCuA2Svgnsq2zy2KTlWwtNYLlFJDMxxyC/A7rbUGFiml+iilBmqtmzpkgoIgCELnsXo11NQwfPFiqKrq7NkIgiAUnI7ouxWm9OR73VyLfxRaWXTnGY+Z91oToJQZX3nHKUxvq2SGsWLJBBevXcjU+hoGXdQbhlax90j75xiGMrpL5+EpWM9EhAg+A/yz1nqh9/ol4Kta6yUhx34O+BzAgAEDxs+ePbtd8zpw4AA9e/Zs1xgnM7I+0cjaRCNrE42sDaA1Pdesod+CBfRbuJCe69YBsPu886j/5S/R8Xi7hp88eXKd1npCLscWWqaAfMeZkLWJRtYmmpNhbbYfgq0HTMEFBQzoCRXdCzN2pvUpxnUPtcDaPUbRUQqG9YHupbmdd7AFepSmH+/Osy3Em5sZsqyOYW+9xrAlb9Bt/15aS0t577aP8cptn6UkBiP7tXFwouVKZ4cIZkOFvBe6xlrrXwG/ApgwYYKuaqe1s7a2lvaOcTIj6xONrE00sjbRnLJrk0jAG2/w3q9r6Duvhopt60nGYrw9/ErmVn+JdyfeyvXD17OraxVfu7LjplVomQKn8HecA7I20cjaRHMyrE1dE3zT9SRdVTgP1nMv1LKiR1WoN6k9143yUn39Zfi/Lf7rT/SFH1alHz9rGcxdDdNHwPn94LNz/HnMvNrkd5WXwfLtsD0BtdugNQnJHLWsnkf2MXnlc0ytr2Fyw3P0PHqAfWWnMX/UR3l+bDW1I6fzhaFL+HFTFWf0gD/dkdu4+XC8K1iNwBDn9WBgS8SxgiB0Ih3RSLAzriUUgaNH4aWXoKYGnnoKtm3jnHgXFp5/HfOu+SYvjLmZXT0rjh1elVjPQ3Xm945UsgRBEIpNsfpu1TXBuj3wk4bwEMC2XreuCe5yFKLZt2c+d9Yy+MbLJnSvJAaXD4YFG81nCzbCmb2g2as22JwwSlpQj4orOL0b7DgUfZ1++7dy7fKnmFZfw+Xvv0TXRDPbeg3gyYvuYd7Yat449xpaStI7Cx9sye2+8+V4V7CeAv5aKTUbU9xir+RfCcLxh42Rbm6FWAy+V1XYpNFMTQujSsWeaBxqMc0bCxm3frwooXVNULdqP9eumkuf5x6n50vP0eXgfujVi12Tb+Cb5bdRO3I6B8t6ZRzn4TpRsARBODkI7tGFKAJRXubLw0WNJlSvvf2ngvOc05CqEM1p8HO6wChDCW2UqTEVvnIFxgtllSvL5v2pr8OcVAkdrlwN2bmOqfU1TF1Ww4R1rxPTmg19h/HbSV9kXmU1bw+dSDKWObS8JCxWrgB0dpn2R4EqoJ9SqhGYAZQCaK0fAp7DlGhfjSnT/hedM1NBOHFxXfH3VBbnwXtRo1GukkAyCd+qNW7/Qie4BpsW6oS5ltbF7RBfbGUlm6WxLeMVO2k6J7ZvZ/3vnmLff9fw56tepGvrUXb2rGBO5Z28MLaaPVdOYUeiKxv25jZc52YMC4IgtJ+6JqOUPLYSEsn279GugTMJxIAuJZ7xURmFJ6zqXy5yIuyYXOYAEFMmxC9T0Ym80Zrzm5Yztb6GafWPM3rLUgBWDhrLz67/NvPGVtMwaKxJAMt1yELOz6GzqwjeneVzDdzfQdMRhJOOWcvgay+b3xdshA174ZGlhX/wnjjYeK6S3k6aTBauO3qmpoVKmWsloSAd4sMUqY5QVgplaXTHy6fnST4KZPDY4Otlb21gz6M1nLeghop3FjI0mSR++lD+54ov8PzYaurOudy3KO5q+z0KgiDkQnsNZLmcn+s1rDw52uo82Ldzz7f7vVVkkpjXu4/AOX3gK0PD5xUsb/7gIvjyxMxNgxc1Gq+Uy5gKozCm3BNGeQSjaOWaO6VIV3hUMslFGxYd81QN3bGGpFLUDb2c79/yY56vrGZLxbC8SrO75Dq3fDneQwQFQWgHc1envn5+dfpmCanhd3MazM/bR4Zv+FH9KL5XZbxJyaSxnmXrjp6rQMrUtHDfUfjNO4Buf4f4KEWqEA0aszFxMCxaH21pbMt4ds3iMROCUdcU/X3a+1YKhvaGYeUweWh66GVwjWZMgpmvas5pXEHr8hrOWVND5XvvAPDewDHMvvYbzB93G8sHjcvLoigIglAI2msga6uXJ+oaVp7YZ3pF/nu+9YCBkYV2v0+2+hUB7Zj7D4Y3Awb/PJ0wSsbCjbB4S2rYfVjvq0WNxktmvWWvrIeX1qUqRjF8eR0ME8zE8HJYsxtKWpu57INXmLqshuuXP0n/fR/SHC/ljXOn8Ksp/8j80Tez/bQz/BPboSSdeVrbz82EKFiCcBIzfURqvPO0EcaDpVvN8+6+o6n9JbSGFm8XfGxlevJqWD8KN8ThD3fkZ8XLRSBlSsS953Gj0MViRigUw+uTa3PF9jB+IGwNWBrbY3W1a2bDUGYvN7+HrXOK5VHD6t3m3/y1fqiJDQt5cJEJAdHJJKPXL+a0F2uYt/Bxhu4wmvwH51/Gwzf/iLmV1WyoGNHOVUlncOYULUEQTgA6Mj+0vQayXM4PHjOnIfr+ysuM7I1hcpQ+Nio3Y6Zr5Lprjp8DZeX0jEnwzVfMHGLKl4e1q6LvbfxAc9zDdSa6JYkZNxh2HyZ/S+Lm2CTwwtpU/WZEOVw62NwXGGXt2mGpx8UwhrzVu/3zehw9wP0bnif+ZA1VK56l95G9HOzSg9qR05k3tpqXR32U/d16R99QG7moSH+DomAJwkmMLTTh5mCd3dv3NP3mHT/ELplI3SRbQoSJK0iS3gav4Zjguf/i3IRXvkIvLAHYDYtQ2mzi7SFKkWpvhadcHya6l/qWxkKEJVrvWyLpfV+tfjKyzcsbVWEEdJTxL4lRqOY0wJPLW7jovVq+W1/D9cueYMC+JpIlJbw+4hp+M/kr1H7kFr5w00B+7Qn5YjCqIvsxgiAcv3R0fmgmA1kue/PEwcaYmPSMimEGtmDEwGMrTSPcYMGnuiaYucA3Cs68OroYVLaICostMgH+vpvQJvcpalw3AmXGq75HzcYYJJK+XJ/TYKr8Bdco4bijgtv9mt3QuN94rmYu8A2y8ZgpcgHm/v/yQvjpczuZtPRppi6rYdKq+ZS1HGFvz77MG3sbz4+tZuF513K0S7fwm8lAv+5mnXfl8FxgFcFCIwqWIJzk3FNpCk4sajSb6+4jTrdzbTY6pdM9WK4wcisUuYIEfA9Wrp6duiYTshb3Ygza6hUqtGcpkyLV1gpPUUIySrDb9zfvd2LjW8Nj4zNd045tHw4SngB9bCX06sKxkufBak5Buh09yNXvzeOKR2v4av0z9D68h0NduvNW5TTW3F7N5Z//KN2PlDOwEb5QFi3UC8XKInw4cGoAACAASURBVI8vCEJxCTOu2feL4dGK2tcLqei519i8Hx5dFl7wKR+jYNQ6WdnZ6ig4j62EyWdnn2fwnsf19z1hFq2NrIjhK4vBQhyLGjMb0TRm3LlOSoJOwoAe8OEBOGP3JqYtf4JJv63h7ncXoBIJNvcZwqzLPscLY6vpPeVK5m5IVU8uGQRlJXCk1YQxZmPXYbj5PHgigwev2IiCJQgnOWF5M65i4sZbQ3oOVtj57vGuNSxbmfFDLXCfE2J415jo8IhsFKN3SHtL5QaJEpJhJe13HYa//KMRyiVxT+H1hLGNjc/2EDBrWWoe3KzbTAjKrGVG6CWSJg8vilvPhyXLd3HJO88wrf5xJq2aT7eWw+zufjrzx9zCvLHVLDj/epq7dKNrCcxo9BtCzlxghJ8gCEIUQcNYeVlhFJ1M3qhMERBRURSusavVM1C1ZijeZK9R1wS/X+EUfNJtCzfPtE7xmAnDs+F1iST06wFd4v46jqkw8vgcp8dTsKhFUFE5Fr6n4MohMKS3CS8PC5svCSh5QWLKRBy8vsm8HtbUwNRlNUytr2HcpiUAHD53JOqrX6XhqmpuXT2elqQyfwOXwtn94Fdvm/UriUH1BUZOPrAwNwVLa3htQ/bj7LoUw4sqCpYgnOQEBcnuI5kVk2yCaPl2EzJgjw1TwqKE5MEWZ6ykGSdMqOWqMBVaISo0UQnCwZL2ALv2+wKrNQF3V8KmvUa5slWhMll765rMWHaMZqfiU9yr8FgaN3l41oNlGbBnM1OXPcHtv6nhJytrKUkm2NJnML+f+JfMq6zmT8MnkYinioujrX6svlKpISPFYpDkYAnCCU3QMFaIIkJt8UYdK/Dg5SOXl4WPF1NOdT6delzU/R0r+KSN0pNPuLkrA+2x5WXGG3SsBHrS5Dg17k9tX2KLP1mDV0sCvjLQL3DkyiOloDXghYph3i+Nm4gJMAZXd41sePm9F8Iz75s5hHH5mZo/1Szh7+prmFZfw4ht7wHwzlmX8IvbHuCsP6/mppvPB2AkMMNpJ2PlupvOMHOBGfc374RfL1h9sDRuDJi5UIy8ahAFSxBOesIe8vNRTMLiy8NCBnIRkj1KM8fDHxe9m3IkF2UwSqC6Je0TSSNYLnHOi8X8uPDFW/w12XcUPv7HVA+VHXNOQ6pFUWEE4oxXzftxL/H5nkp4uwl2vLPqmEXxwo2LAVjd/wIeuuYfmTe2mvohE7JW/rP5ezFpUCUIpzy5GsiC8qc9OVLQNiXNFnhwH+CDoXwJnVrCW5Fbrq8blp9PuLnbS8pGN0wcHOhxpdIVKvca4wcaz9UxT5XjQXPl0b6j8Ou3U0P9YjG4c3RqVIm7Rt+u9VMIFmyESwelKljxRCuXrllgKv8te4JBexppjcVZNKKK317118yvvIUP+5gvuMtGGNTk5wTbayze4n8PbjpDixdymAwY8qxipQPvfWwU7DgI89Zm/q66xYv3nCEKliCcoERVGArbcNsTSheML48KGcgl9KF7afRc2mPJ7MiqVPZ6+VRBDArYey/0vUgaE0oRazLCM6aMYAVzTzYkc99RP2QCfA+V9SD+YUX6tWvX+zH2iaRm1+tv0/SfNfzkj49z1mYTC7p0yAR+9NEfMK+ymtVnRGf7Bi2EFw+Cd7f65d3RgTK9XoPLpPaFeO+usPdo5CWysiXCWioIQueSKZQ8U95pPjlSED5GW/Nxgw/wdj91K/3Fla9UaLJ7sNz7yjdnNiy64c7R/nsKqOwPY/rDqh3pbTTC1kOp1PWwx97zuBd9gL9va50aVVLXZJQ1a7gLKjcb9kKP1sNc0TCfactquGb505Qf2sWR0jJevWAa/3rDD3hp9I3s7XF62n27ERlhkRdhzxXTRxgFzCqg915o8nJtlAfeGnUtMUriqh3ZFazDCaPgRRUbaQ+iYAnCCUiU8Il66G9vKJ0bCjingXZV2ouaS1uFZKE8X/koae0Na9nfnP56XB/4+6H+fbv39OlxqcoVGAXGzYMLxsMnNWzf18rEDxYesygO3r2R1licxcMn8Z+3/RXzKm+lqXzIsXPCmjyCsVQO7JWaMHzRQKNgadITnmPA3V5+3fw1vjLZHuWKiLkJgtD5BPN73FLfMyb5IWthe3QuOVJzGsy/KPnWFiNimMwJVvqbPBReXOv3fMrFg5VJlrifrdqRmjM7Y1JqdEMyCdsP+sqDBlZsh/qtftW/riXhyqddj3P2pIeS23Yb1iNmDWFuqOSsZfD1l9M9Qxo47dAerln5LFOX1VDVMJfuzYdo7d2HvdNv5P8qq3lj1FTmNvXIWk128354aEmq7HLlWtj3GvQM1jX5UR7xQNl7q8BlY+5qUbAE4ZQjaqOOKp5Q7Ia4NqzCjZV2P2uvEuduppC9aAYUJ4Y/aH0NUozeWG6Z9pQQj9Zw5ep7Vf7c7HyaE9C15QhXrnqBG5bXcON7T1G2ZydHSst47fzrWXTvTH5YfhM7u/cF/LK8YBKJR1fAsq3pDSGPJOCpVanvPfN+hrwr5VtCv/tq6kfnlsMHu8NPy0bvrm07TxCE4uJ6fWLKDx8mUE0u1z06WB4dMo/RFvkTJs/s3msr/VX0MMpPS457fSaDX7CPpG2hAWbvXr4drhlqFDqNKXZU0cNv6gupyoit1hdUPj89znh2po8wcsU+R+w7anKYbBn2mDI5Yp8e57dssblO36pNVa76723ixoYn+eSaGobUvUxpspWtpw1kzsV/TsvN1Xzmb6tYv6OU7z4OzVtSZYgCxg5IlS0av/iSy70XRn+v+UboTBwMpTHfAxnF9BGZP28romAJwnFKpo066gG/2A1xrXWvJZEaK10o3HyisFyvMAqh7KRYX53iDUGLWFgCci5NlYPH3T7S3J9Noh5TAewMvyel0r1TEzxLnmV8973Miz3Hrtk1jF7yHN2OHiTR6zT2XHsj/1BezUvnT6Ole09mXg17azkm5VzhNmUo3DchtYmlZUAPWBq4r8b9qQqaReEndtc1wbJtqZ9PGQar69rmjdp2qA0nCYJQVIJen3svNA3treyyoV3t2aPHVBRevtl5N7fCG56RMihPgrlOkNnwl8ngF+wjGfQO2f5Z7nY/psIoeMcKXASwe/CxcVtT23B8cwg88Jgx0AX33Mr+Zm9/s9FXiI+2+lUQz96+mmk2T3fDImJas77fCH5T9XfMG1vNu2ddgo7FKInBuB2pZeiDXDbYkwXOJMJkwLoI41um5yE3wsZ+N+AV6AgfLmX9XFlaSETBEoQOIt88oUwbdZTVptBly/OZUyGwm+jRVmdjzHIddy3Ky3xvXj7zCio0VtgkvPjsOQ3hYS7W45TtfoJCYfxA02TSTbD+6XnR9+TGqAO8tQW++MhWHi15krNfroGXXmJYSws9TzuDmvF/xgsfqWbwzZNJlHbhGa8nS8yzkOoIiaMx15t8Nsxf66//yH7GihpX6aGA7ssS78HqtK7+39/XX04959JB5vO2crgl+zGCIBSGfItMWK/PaV2zh3ZlY1Gj72lJJLNXv800dzteWCRIMOfpD3eEX8c+wN81x9/PZ9+eOf8pqAi6XrlgOPZ5feG9Hanv2fueMQl+ttj0kArDKp86kb6/H2mN7lm1dGvgDa0ZuXkp1z5fw7/U13DBlmUALB98IT+dNpPnx1bz/hmj04oftSbhC8/CrReYeSRbU+9Dk165FsJD0l9c51c9dMmlrL7rHQS/gXImrNwrxvOSKFiC0AG0p4xslMUuLCSivWF62cjVW9TWohN2E7WboiI1Nj5bafm25mIFFZqZC3wlT+NXMMpVuQxrGBw8J5hgfTCgPLjf5Ya9RkAN2bH2mEVx/Po3iGkNw4fD3/wNcy6o5h/3T6RVedLlvdTQkiSmqlKYghVXULsBXlpnhJPbyLJhh8kVKIlDRRl8eDD8nrX3YHX/xb4lcUfg2BF9U8MZ86Vbaf7nCIKQP/nIrFwq1eYrm4LVazd7BW6yGbWCc7cP2zYaYsYkvyH6mIpAzpNOVcqCzGnw9y0bmhe8p1U74IK+xjN034TUz1ft8JXG4Db8/s70IkG2/9WMV6P3y6T2lbBvvJx/ZEAsmWD8ujeYVl/D1GU1DNm1noSKsfy8K5l560+ZX3krjX2HZh3nw4NGRk06CxZuijbkuVw8CJYEwgk1Zl2Dsj7bs0fQO2jHyoZCyrQLwglNW8vIFtsjlS+5zKk9RSeCQtWG50GqwHTD9ixt9a65ipsV3uf3Sw9TzDXMxS05axsGkwy3ZroCo0eY8qA1LFvGlN/VcMuzNYzaYgL1Vg4ax8+nz2DEZ6pZ2KcSlGJMBahaUqRVMFRj4z4zp6CwTmq/qbFOQP+eqdbSJEbhyhSiZxOk3e8/WOV9TIX5Ttzmx/kw/ow8TxAEIS/CjEO61RRG+PLE8D21GLLKjmn34dnLze+5yJOoh22dgG++4nt0usSN193mJcWUyVMKawRf1wQrtqVfa5bTvwngay/7n51T7hWFwux936oN9yYp/BxbBVwyyOS9DuhhlMGWDMYoqyA8tCR1v8/UCLhL61Euf/9lptU/zrXLn6LiwDaOxrvw+vnX8v+u/yYvjr6J3af1T8n7zZUFG3M/9q0t6TKgJGYq4rZ6MnPm1b5CnCk3+lgOoDYyLqnD7//MXr6yDnDf+OI9W4mCJQgdQFvzhIrtkWoL2ebUnjDCKEHtFn1ww/ZyyUvLRKYQvvED0/uMhIW52AeS8jIjCI6VscdvGHxmr+zlifev8sbamGTKh29ywWs1UFMDa9dysVLUDbuCH9z6E14adyvDxw9jzxH4aSMkvZDIkpgRyJszlDJv2BGeN2UTnmOeR22rp1wp97NAM+EYUDkAzukDz3zghzrePtL/rpROPd5W4Lp9pGcNjsgriKJqaB4HC4KQF0HPTzzmG14WbjQGpigFpxiyylaCO1YMoo0FMmLK7MWQquC0JIzX/d4LTSGhRDK1CITbCN5GNbi80wT/Z6LoWLARTu+W+vnDTq6prdQXhhuxoZRZZ/teSZZmuUlMpda1gdyloHLR48h+qhrmMrW+hmtWPkuvo/vZ37UXr4y6gXmV1dSOms6BstPSJ9UOFMZLVdcUngMWfD1uAJzRwy+t3pyAb7zir1uXiLBMm0uXSJr1u2EEPLc6fT4lMSNDDjbDux9CRXf44pXtv88oRMEShA6g2N6obCF5UZ8Xun9UXZN5wI/y2uRCmKC2ypON7bZhe7nkpWUiWNwiaKXNFubiNoa0fUpSwjxi6Z620HttbmbxG4v54JuP8vH6J6nYv5VkaSn7r7qWl279J9677GaGnj+APkfg3pCcLDCvMylXlii5OWGgKfn72ka/BPBVZxnL7O4jfuhkc8KvYnhPpVF+9ft+qCOkeiHB9wK6oZ42VCcfT5a1ZAqCUHhSjGNJuGsMbNrr9xly99x8ZEd75EyU4cyOeU5EXqbW/j72mY/4ipOLDcH7t0X+Q3yr58myIW6JJMxeYfb44D61ckfq612HA3Nwfk9o37gVV3DFkHBvT1AJi/JCuTxUZ3Jlg5x+YDujXn6W/3rtx1zx/ouUtR5lR88K5l50J89VVvP6eVNoLjFJsZlbyreNiu7w2Mf89i6u8dGG/ye1E755te/xsyQDCnFY+KDNpdOY7+2p9zm2+DHlF/Oo3QCPLvNl9Y4e4flehUIULEHoIIrljcoWkhf1eaH6R4VdJx4zwjmTcpEPwXCRRITyFrbGmYS7Fd66NdVKm61MuyVYNSkYQx8sOZvCgQMwd67xUj37LJfs28foLj14ZdQNvDC2mtNvv4H/3djbhPNthtgWowTtPpyb0LVE9bYK8s5W8yBihb7GKFduf5AwD15U1S03NMauJaT+zU06S3pbCcLxQtj/ZfD7DLlGklxlhxsy3aUkfzkTZjhzr/8V7zX4x8xp8EtztyT9Knl2rzm9DAb0NB6RV9anK14XeAUnkphzlm/Lz9PeLQ5Hk+nKkvsyrCdge/bCFq9wxqBdG5jq5VNdvHYhcZ1kU/nZ/O8Vf8XzY2+j7pzLScbiaeffNx5e3WCiHAqFbasxfqD5Tlzv4bgB8O2rze+uUuVW2I3HzD3Z7zIeC68uPHFwei5dScxEUJTG4a7RJoyzNSCrtQ7PoysUomAJwglOtpC8qM8LXREwaP10O8IXgqiwvTDcsD1bhteNp3fHnDHJhHJs2Gs23+bW1CaZuSR3WwXNhtJpbf49shSuH26OXdQIV3TbwUfeetooVfPnw9GjtJzejw+uvoM9F43gL0/7Ww7FyyiJwZ09U2Pvk9rLAcsTjYnpP5owTSqTXhjFlHPM5y+s9fupvLjWL4wRw3iM3JLEUYVVglUcy8vCm4K6oZ4kYGtEwYwwFP4DnyAIhScqCiD43i/e8r32ujVadtQ1pXrbj2bJ5XLPc68X3HdSIg90ah+omIJeXVLH6xpPVZB2HTH/LEHvzcpAKHVQURrcy7SpiOJwlgI+CZ2eB6vwwrDz1bK05twPV/LR+Y9zXX0NYxrfAeC9gWP4+XXfoP81Q/inrvemJ8QGrr1uN6zamf7ZyH7hSlcuhrvPXBj92ej+vrJsvzsb9j/79tRiI64CdswL5jyzjB8IN54LT6zyj7MVbfcd9XLfkh1vzBMFSxBOcLLlHkV9XshmuYUIDcyVbJ5A12Iai6XG03/zFaM02Ad16xFrTaTmGrlNMrMpnnYs66nZvN8IARtS82LtRloef4Ip79ZQuWYB6CScdRZ8/vOsmlRNdeMVHKGEvzujlkNNZYB5IDnY3DaBEFcwsGfqA8Duw0bYLd8O2w+asut23i+v9x+AVu/2LX9Ba6H16pWXpXv3glUcVcQaBv/m7hpthHcuFQUzPB8IglAgoowo9mH4F2+Zh1a3Oml5WfhYixp9rwKY/SwslytYUj2bdyzYVgOcHF2dqjx1SXfWpBG2z+rA7zEnhyqTchUkShEJm1culfcAVDLJRzYuZmp9DR9dUcNZWz8AoG7oZfzw5h/xfGU1GypM1Y2/H1gLTZk3T01qaw5LDPjUWGN8fDhD70JbmONPjgGwT1fjHbS9Ml3PlOsdDTP03n9x+nduDXdR4aLPfJA6H9su5ON/9OWbwnjOVmw37ylVXKOdKFiCcAKQLYcqU0hblFWyUHlhYaGBYyra1o8q073meq5rMdXeJnosnl4bBez3K8x7rrUwBoztD/29WO2oMET3Wu6DgN2oH1oCwz9sYOrSx5m2rIYxm0wDkPfPGMV/XPc1+n+imo9/8iLqPlQ8uAgOaz8UxWXemvzu3aI1jKpIfQj4YLepbnWsaAW+tXB0RWo/lNEVMHW4rygmvAqDVmm1z0vBhGNXUCptHkjipK5h2N/c+f1g5qshPVkC2BLKx1vRF0E42Qjbg4PVQd0COLuPhI8zcbAJC7TFIbzghrRcruA+GhVZ4c5rxiQT9tW3FDZjZE8ixFAzql/bPP9B2lJRD8KVktIYlMXTj8t0iZJECxNX1zKtvobrlj/JGXu30BIrYdG5k3n4qr9lfuUtbOs9qG2TDLm2VclmLjBrHWytoTHGOBvpMaKvqQpo5cOeo0Zpe3m96S02fmCqZyrX4lTBvw/X0GefMYKKfEwZ+TWnIfV9peDO0X6o+zl7iitPRMEShOOcsHh3SM9HyhTKFuX1CSvakK9yM6fB7xmVTJjeRzNDwsNyuc987ilsvsGNNu41v/31274ypUnPYVKY0q4rd5hu87nkkKUoFC1J/lSzhNgTNXx1aQ0jtplYhbXnXco7X/5n/rFXNR/0Pc80p7wOZi3PHrZwuDXigwBhhTUqeoQfa49LYr6zOQ3Gi+QqN3eN9ssSz2kwIUCQPtdgHxhbuSvheAM/Pjp9DcMKh8y4Gm7/Q3aP3b6QvAVBEApHVH6Vu9/F8CoM6sxGKBuCbfc6MOe6uVwPLnLCDROmFLqtXmhbPwTnZYvntCbgkoEwa43ZByu6w/ZAK4l3sxhuOpK48pXKP+Wg9JU1H+Lq9+Yxrf5xpqx4ht6H93CoS3cWXDCNeWOrqR3zUXaVlUeer5yfwb01W5ijVfiaW40iG8xTUxhlxVbIBSO7g9EIrcnUUL5cDcCWoIfLGpKDSleXEjNXpcy/2cvN30lJ3O+tqbVRGGfdZrxktbXR918IRMESTikKXTWvIwhuMDbe3Co1QE6hbLlUGsy36EVdk9lU7Tw0phO7baCba26XvXY+9xQ2X2sxdSvdnd8P/utd37oZFDYKGF5u/r24LvccsokDWrjygwVMebeGqcuf4Iw9m41FcUQVj0z6EvPH3MLWPmcStzlZGCH10BI/76lQlMSMhVUB1ww1HsQuWZr5asx3d/vt8MA1fi8Xm6MWfDjKZb4j+/nKWmvSPCzlEoKxakdu47/ZmMNBgiDkTbbm6EFPQ6aoCVfW2Ibq1uhy5RCTgwXpFViT2ihEVjGwrR+sx8HtbRUM4dOkK1fFYmQ/UwQjnz08hq9cRfXC0kDvg7uYsuIZpi6r4er35tGt5TC7u5/O/DG3MG9sNW+OvJ5vXN+NbtthwkHTHD5svIrucNEZcGYJXFXqV4cFo+jdfzHUrof6babnYdS9JDEREYu3+FV88eY6psKXF7bQSJCSWPZ0gUxh/2EerjClyyppbuRFWCXM5gT84wvQowt8unfmebUXUbCEU4ZCV83rKIIbDPgWGfDLnWbaxHK597YUvbDJqS4a40WxFXzc0txRJXXttfO5pxSB63ljfnhNeEL2MeupMmGAK3c4Vi1M7tH6veE5ZG4jyXuGHzLFKWpqGPfU0/xuz24Ol3bjtZFTefYvHuDnA25Msyi6wi+hTdhEvrhKYcxR2MBY6+69EPY3G4XppXWmEuDMq40AnZfheq1OzLtb/MMSfDiq7G/i162H0SpPbrl6i8Y8LH38j36ISBh1TSY3Lhe65pBPIQhCZoLGtrD+V8F9MOhpAD9EKzh20Lvgyi9b4MIWvLEFdc7qbfZgSH2Ib04YL9f0EX5vq7YapyadBRv3+tdpKy0hcwjznrkkCW+sCzBgz2Y+/sETTHyrhomraylJJmjqfSa/n/iXzKus5k/DJ5GIlxBT8IPJRuGc8WpmA9r2Q2bvHzsQunUza9+aMLL53gv94k+QfT33N5vv/sFFvqLm9jUE87fQGpjPJYPgn67M/TkrzAhsDX1WBtv3g0qXmyc4p8H/zK2E2ZwwSvxqr2fY5piR72GyrxCIgiWcMhS6al5HESbY7AYSj8HHRmUvh57Lvedb9CLovQLzEN4lYN2E9JK6Yde24WUxBdeeA5+fEO1pswmvbkjaYyv9dXDPO9bhHWNNG93fhDbMXZ3ahySRhDsDTYFnLYMHntnDlJXPUF5fQ/P7z9PlyCFae/dh0Udu4n+GV1N7wVSOdOkOGMvg6V1Tk6wLgW3YuPuwaWb5ztbUsIdHlpp7dxty7j4C487IrGDFYqkKcNAanWa1vtq8H9aHxD4sBT2Ercn0UrhWCG4/aBpk5lo5KyqZXhCE3AgztgW9Q9cNg4+cEd4c3VXIbHVWW7Etm3chU+7NtBGmn1OQpDYP9a9vMq/bqlwp75rTR5h81CjiOVTyW7sn/b1gD6ww3GHP2fY+U+trmFb/OBduXAzA6v7n8/Dkf+D5cbdRP2RCWmWfpDZGs9krcisMZK85f62RlZUDTAj47iP5N3kHGNI7VVFz9+NgqfSYMk1981GuotrJzFxg3l+8xS+aERVWmKkS5oOL0nuPzV0tCpYgtJtCVs3raNwH2YmD8ytOkWuFv3yLXgQtVjEFd4fkLbmluXVIoQL7sG29TEltikx8fkL4vbib8OSzfQWi1VEcg2Xa7diJpAkfKI3Dp8elbrZuyMPSd5uoffAJRjxTw9vvvUJpspUPew9i9oRPc+anqvnS4as5pEvTBFSwglWhSLF+7jYPAWMHwLKt4Q1+7Xc8f010FSvbp2tOgyn6Yb1SbuGKTAVSXNz/W/GYUaqiksLrmuCuObk/ILj0i8gtEwQhN8KMbcH8Sbv/Zso/tQ/oyaRRjGKY8Oygx8r1Lri4e0t5mfHIRKFpQ/nykDH2Hc1upMnlOmF7W9bztKay8W2mLavhtoYaBm1aCcDSIRP40Ud/wLzKalafkT2e+oW10YpRz1I4EBIlYvOO67eaqq1VZ+euqHaJe3LRLXCiUsM37ff7vSqvGJI25+XzjJWS9uDI8ijjcKawwqi/ty9PhDcaU3Owp4/IfY75IgqWcMpQqKp5nUGYdef+i/M7L6pwQ1jPkVwIWqzQ4XlLwZK65WVG6SovM2XDbelWd8N3q0y5BDdb9xxbLti9Z02qMDwmBBOwcnvq2EO3f8DQX9Vw4M0axtUtAmBtxbn8purveH7sbSw962J0LMa4XnAgh1j/XBv89iiFgwGhGHau+zqh4YwesKoktfqW2+B3/ppUi/DpZb7yVxIzfUN+806qsAkWroDc/iaC/7dW7TBhfwlP0Lp5WPY7zBfpgyUI7SfM0Dh+oImEmLXMzxXNFOGRtvfjG3qsx8rtXRSF3Vu+/nK4waWtvaF6dTF7qhtKDabU+KBe+Y0VJBii7WL9TcdynRKtXLx2IVOX1TB9+RMM3LWR1licxcMn8fBtn2de5a00lQ/J6/qZvE5BOVLRPfW1DbnMJUy9NOYXKQpWiLX3F5TT5/cz0SE7DuZnDAtGw8SdvK1MhvF8c+rHDzTh6v+8EDbuM+tzV5G8VyAKlnCKkY8CcTyRLcQvaqPJ1vy3PXlpKRarpLFeZvOM9d0K36jNXjQhystmLa1JT2Gs6OE3xlUYhW33EeeeQ7D5XdOHa/a++S5Tlpqu9xc0LQdg2wUX8fAN32NuZTUfnDEqLUwjWzlxS67PBLkoV2H065FuMHCbNgbH2H3EjG0rKwaVq2xkE2bB/1t3jTE/gwq9/Q6D17bfYxQXDzox/+8KwvFEWMj5L94yXoqujsEmk/fB3fvtXh5TqefZfeixlcZjAn7/vaAMemxl+HUU8NmLAfD3LAAAIABJREFUMvdgCuNAsx/h4Hp8NCaaI1fC9uJhfUxYc9R8urQc4Yr3X2RafQ3XLn+Kvgd3cKSkK6+dfz0/mfodXhx9E7t79ks7L4apeNeayD90zxKc05ltVCbPLYd/uc7/nlbt8DxXAcXSVYTcsFGbV2dbgeQSDWOjTBRG2c8WRdGeZ5f6bea8nYfDUxYKhShYgnACkM2KE7XRZAuLbG9e2j2VfoWnTFYk+/6b68If6q3SU3V2uBCOYkyFEUrNTh7WzKvNWImQMudlKsFXkq9zy9IaBvz4Ce5Zv56EirF42FXMrH6Qbh+7lT0DzuYPK7xGhBSm2p8tz5utH0s8g3XUpSTm/+56MlMsjYFz7JhJbbx3ycD3YL+DMC9RPsIsqj+YS0z5Py/oa76/DXtN37Koe5f8K0EoDMFcqmCPoVw8Anbvt7mU7r7thoUnAh6Tx1b6YcizlhnlKSpcOIkpsJCvF8t6aj48aAwzuZREDxJXRrn71dupURBrQpSrXof3Mnnlc0xdVkNVw1x6Hj3AvrLTeHn0jcyrrKZ25DQOde2Z8XqxGHzmI6bibabw6TCZFPZe3Ov5tCsHT2KQHl38320OVNLrKYknn4KKkJuHC+n9zjIRfE4Jyowww3hbn13c88JSFgqJKFiCcJwS9BhEhTdm2miyhUUWIi/N3WDd10EWNaYLgZgyykJYoY4wj4nN+9KYn7uPpIe27D5iHhR+/AbsPAJdWo9yxfsv8fH3Hue6FU9RunM7dOkC113H+i9+i7uab2Jb9wqTo7YTWre13YLoYsu/DyuHyUONdy2sRK+1XJ7eDb4/2U/ojcdMOeD6ranrdskgU51v9nLzcOM+FAVzoY4VvQis+fQRRtk76vUNufk8OK9v9INVPsIs27GutTKpTU4AZFcqX9lQXGujIJwqRJVk330k99Bzmzs1pyHdmGL3Ibest6XFqwp4ejd4YlX2a63e2fb8q6VbjaLRFkOZvWZJLL3BLkC//Vu5btmTTKuv4fIPXqJLooVtvQbwxPhPMK+ymjfPnUxLSZe0cSOvlzRtKIL9plwUZv93FcbSGHy3yg+3b036+bW7j0C/7n4bj5Is+bGWd7caxdstgJIEYjq1/5mrCNnvXHuV+tx+Z9nIJX0j+DzQ1meXYMpCMXPxRcEShOOQWctSQ++sxyBs48m20WRLBm1vXlqu3o2Jg2HRerP5WwGwv9l8ZjfqX7zlz98WQogro3jcU2mSlF0LWXmZOd4V8uVl8OP5+7m8/jmuX1bD5JXP0evofhI9exG/8aNQXQ3Tp0OvXuxsgmscC9/s5YVRrsAI4jVe+feX1hmhFIg2RAFXnmWSb/evgqqARxDgjsfMuZZtB33FSSfM34kVeLNuS+8H4lISM6E9tmqS/Rt7fg18alzmnItchVm2Y92EertOudCah5VSEE4V8s1DyaUkeyasbEp4Hg3rcdetRnGyPa5uH2lycV5anxq1oEmv5JaJbF7/bCS0MVS93wZF7Q+Ban1Ddq5jar0JKZ+w7nViWrOh7zAemfQlXhxXTd1ZE2mNta2XhMa0wIgqRW+LiHz1ShOy9/sVMKAH3OcUI7H5Ura4k63c+70qo2yVl8Er6z15hJGtLRECz3qfcu1/Fixakqsn1D0/16gI+4zRlmcX97xz9hRXnoiCJQjHGXVNRoBZodTcmvnBMlOMci6bT3vz0lJc7o6QDavis7UP/P3Q9PLtNgbfVrKbdJYv2BLa75P0m3dSx5y72usJMgleX7KNT214igF/VcObi16ka6KZHT37s2TSXQz+82rOveMa6Nr12Lnupg3QpysFx1ZvsiidWgY4HjPeJLdHWPD7+NxFqcUqpo0wZdmtBS6ZNEqhTvhrf//Fqf1Awsr52x5XSYwnK1jcwiXT35hNZndL5IfleLhFVD42Cv5vWfS6xZWZs5tLZhVqQRAMbclDCebl3jUmtTWFHTdMdgRlk2v4SWKaub7ZaDwYWhuF4HtV5qH+xQzV7zJRiBDtVTtN/6hX1ufXh3DXYc0FTcuMUlVfw+gtSwFYMWgcD06dwbyx1bw3sBKllPEStTOsPJmEoV4bxWAo4lm94b7x/h7qNvi1RaOsUhMMg7Pvu4r1x0aZ8x9dlv69uH0o81FkipXj3pZKgpmw59XWFnqmqYiCJQgdTDbFZ1Fjan6M7VWUieBGU6imyrkoacdCA1p9Ibt4S/g1u5f6IShunH7S9WYkYOvB1PMS2ihTwbyhde+sZ+7/PMH19Y9z59rXieskewYOZdak+5lbeRvLRlzGt6rirDwCb71vwijcCkduMYydXoW9s3ubfKBiYAtM/Ppt776174H6O6+alvXm2XX/2pVmTseaHVf6r0dVGGXLNlBcsBEWbvJD/qy10QrfVTv8cYOlmd0+YmGE/Y255dbd3IqoHA/7N3H7SHO8PTf4UHLTeXCoxSSoWxSpjS0F4XgmX89SW2hLHkpYvkuusiMom4LYsu2WI62+p6VQkQFtIanhv96BD3ZnP1Ylk4xf/yZTlxml6uyda0kqxZJzruB7t/yEeZW3sqnfsJRzNNGeoMjrkK6IJfGb4MaVP3eNaZDslkWH9MISbhGjYBhcWMErN/pDKRhdAef0MT293Ka+xVKccqVQLXY64v+kiyhYgtCB5KL4TBxsLH/NCRNK972q/DeDTII3100ml7nasWyn9YUb05Nb3esF79PNFwLfg3XXaFi+LTWkY1QFLN6sGdq4guu9MI0xjcal1TCwkv93/TeZV1nNqsHj+Ox4Rdft8KmK6I73cZX+Hhgh9cA15n5O72Ysr25vka5xU961MVCNKlsVvLjyQzWs5Tep/Xh4rY1Xx3rzbDNH28Rz+givBPIyP/xj0WZTGGTbQRM3b8d8YpW5j65eX5qZC1KrO9mw00ylmXMxBLjl1sMSmlMamDpesvEDjTLmlnK2llQFPP1+eO7YidS7Tjh1KZSBKxttefDM5pEIyo45Df7/0zEVZu84ElJAKIp3t5o9p7PJpFyVtjZz+QcvM7W+huuWP0n//VtpjpfyxrlT+I8pX+WFylvY0WtA1mvEvHyvbKGIttR7JpLaGNM27Q2Xq5BeWMJGTPzmHV/WuGFwNkfKtksJizawf7duU9/OplipDMVGFCxB6EBysTi2ZTPJNQHUeh3sJjPz6lQPR7CgRLbS8MEKVIu3pJb5dS1ssRj864jo+7TXtHNYvt0oHCqZ5KINf+KKN2r47Os19Nu8mqRSvD30Mn5w878yr/JWNlQ4A+M3vnx9U7Swi3p/2gi/QtY9j6c/TBxNwJaAcmXzxF5ZD0u2hDcbVsqMCek9ZFxcZdA28bTWzphzbhLjfXphrVnvoEVUY76LuaujqzvZvlnB0sy5GgJK4/58s+VbBb1krlXULS8P4d9NLEIhFoTOJrj/trc6a660Jw8l6tjyMrNX2YIGv1/hhwTaynr7m1Pfz0a2w7qXwKE8lLZC0P3oAaoanmfqshomr3iW3kf2crBLD14ZdQPzKqt5ZdQN7O/WG0ivyBqGAq4cEl28w+7PClM5deWOzOO5RSSCctUSLCxhSWq/YIkNgxs/0Mhom3cbbBIMqVElmf5uO8ITFNafs1CpDPbeRhdstuGIgiUIHUiuFkc3xMrNXwkjWwJoeZlf4W9Og/9A3JwwuU02D8eGF9giCPmWeLfV+2wo2/iBZu7Wc5JMGsXErQQXtmk+tAQ2bW9m9Mpavv9mDdcvf5IB+5poiZXw5nnXMP/jX2H+6FvY2nvgsUaUYUHvScLfz8Slg0xInnt/YQQfGO4a4ytld80JP8d6iO6/2HilfvW2WftcShC7eUhoo2TZkubW+zSoV3qPl9K4XzEwrLpT1ANaroYA1wsVFl5o862sl6wlEZ7rZecxpwFq14f3qkkWuaSuEE1Hh9a0h46ea9j+W6iQplwoZPiWW5I7FjPecTdMN6GNd+QPd8DB5tyqAOZCRylX5Qd2cO2Kp5laX8NVq+ZT1nqUXT368vy423l+bDWvn3ctR0vTEz1zESMas9fOXZ3+mZWt2lNaV+1M/fxY/pbXzuPcvqn7aZQS7e7f+46a7yapTXP3sL85N+82LOIgl7/bjvDOFuMaYfe2v0B/v1GIgiUIHYy1SmXr9ZTrJpMpARRSx7j6rNRzbXw3+OEF36r1LVthSpq9ZnlZ6oa17yj82yIjKBZtNh6oMRWp3hqNeYgONsdd1Aj9kgep/fU8pi+tYcryp+l9ZC+HunSnduR05lVW8/Koj7Kve59j1kSbiDvzat/blSunl6V7meLKVGiya//uh/51wio6lcT9kMYxFUaZ3Lw/usyu69V7ZKkZ1MbLP7LUG1dlLqEbU0Z4zpjkl+W1c/jri/0E9LgySp/9G7OVCcM8lWEPaNaKrbQfThJGLg93br5VplyvVTtM1cPIBtESItgpdFS4WyHojLmG7b/3X9z+kKbOwA05U9r0tnK91GD2mzkN8NT7nTbNvBi4exNT62uYtqyGS9YsIK6TbO4zhFmX38e8ymo2VV7J8P4leVU2DCOmzN46fUR6lUSNUWyuPcfk/z4akFU3ngvPfGDkZP02+Kcr0xWpXHJjrx+e+W8ul4rD2f5uO8I7W4xrhN1brShYgnBykEsDVpeoTSaY07R5f3Sp3eAY/XqYB/SWhB8HnpZom/SVKftQPuNVv/pQTPkP9W4RhZTKhwnjtSiNw/gzYEmTX6bcNvEtjcMDH9nF4oef5tp3a5i0ah53thxhd/fTmTe2mnljq3ntvOs42qVbyvzc+Sa1H3aXKzFgwiBjmXXHmnKOv75u8YYgNhzQVVrc3lX2u7CWQssNjlfPfYg5ravZ+Ne+k14xEIx1887RRolbvt28d34/4zGzZXmtwIhq+pyPldtasRNJ3zsWTK7Ol1H9TE+aKC+WrU6WyZOXLWdBKA4dFe5WCDpjrlEPrZ1dGKAthBXAuH0k/PNCv1y6Bl5dn72XUmcy4sOGY0Uqxm1aAsD7Z4zil9d+jefHVrN88EUopYxX6ShsbWx/U/mYMrJ44mC4flh6tcKENiXrv1eVHiL+lPegn09z3jCy/c3lokBlG6MjvLPFukZH/58UBUsQOoh8hX/YJhPsYQLmATgeS/VYRI1hBeacBhNDbx9oY/g9TbqUGKXB5k7ZMDQIxNwn/Djvr7+c7rnRGCVl8RYzvzH9Tax9312NTF32BFPra7h0zavckUywpc9gHp34WeaNrWbxsKtIxHPbmlo9a+qmQNW/0lh0VackpnBFScxXIuIxY62dtcyEeEQpVzGnJxekKkxu2WNI96hZj5jNS0p636EVdPtXwbaufrEMBVx1ll/y3io+VkEJ642WT2hpFPbv1PVstlXouzl47nhBL5bbfDgKCRHsHDoy3K29dMZcC5GA31FkCp90CxYFvdzn9k3tRxUs8FMousZNjmveaM24jW8d61E1YpvRWN4++1IeuPGfmTe2mnX9z0s5xW1xYT31rhHHRkpo/IiAMRVGPoR5u7RObf6+YKNf3dWSTJq1HV1hDE7H3sfII6WL+3frttVoKx3x934i/Z/KhChYgtBB5Cv83bwUS0pVNk8QaThWdjXXghnBB1oN3O30Q1nU6OdOBYkr3xtVXmY27cdW+g/QthCDVV40cFbTKq6YX8MN7/2Wz6x+D4DV/S/goWv+kXljq6kfMiGlC2/fMujf04SNZUuQ/v0KuDywltlK5tpcprs9RemxlalV7CLR6aXCg0nhUUwbEf2ZxVaQtAr0kN7+Z7kUHQl61NoSJhUsux9TbRf6KSFH+H8jNszI/l3aNczkpbLKqNCxnEgPO501V3dfdV8fTwQLHNmWCvazqNDKuiazx3YE+ShXJYkWLl2zgKvn/pw7Fv0ZA/dupjUWZ9GIKh6Z9CXmj7mFrX3OjDw/qVOVqODeUxLzPfjxWCDcenOqEU7hhNt7hse03Kik2dvtfucqWLYBfLbmvO3JL8zUViNfwjxBhc59PBE9wEFEwRKEDqKtwt9WV7OWMbe/hfbKfIflybgbnu09ZSkvSy2uEFPGOmc9MxBe6U5hehTZePGZC4zgscqawigtt1+gWfRkHeqJGq5bWsO5W42WuHX4BfzLR3/IvLHVrBlwQej9donDr282c1+VpdISmGtni5/v281UvkppXKvNPe8+Ykqiu6VuYwoq+xuvW68uvoAsiZswEFuow00KV17+1OzlRiB38XIXFHDL+anFM9zwO1dRcpXqx1b6FtFZtxlBbatQxWPp87APSK5lti1hUsHcu2xCPxNRpfjjMT9/zL4ftB4Hda0bzz3xBe6Jyon0sNMZcz2e8tTCmn9DeoEjN0zXNd4kWuELz8HfXGKUiQcX5V4tsNh0bT7MpFXzmVpfw7Urnqb80C5aunTlpfOn86PKH/LS6BvZ2+N0zu4N2/cZj1Bbw/5GV8CybX5usm3i7hb3+WCnWW+rXLnGKHdtg8qT/Wz2CiiLw4i+2UOw2/s3lktbjbZyPP39H0+IgiWc1Bxv1a/yFf5hlfrcB/CWiDyZYHl0WxkQzGczXvUteArzcDtzgfnc5vnYSnduiIPG9ChyKxGB2VTjza1ctv41vlxfQ/8XnmD8pk3oeJy1Y69m5lVfYP6YW7h75Bp+2VR1bLySWLrwPvs087O87P+z9+bhUVxX2vhb3dpAbDIIIfYdAxIYhG1sY8y+eBfYCSaTjCcZO8mXZLI4mWxOiO3ESSabM/N5fnYmk8mXyRBsDzReAduAwBs2FpvEZrMjEEJgAVrQ0t3398fpo7p1+1Z1datbC9T7PDxIreqqW9u9Z3nPe5LXnPKTyzTGUTlmI0c+14Lc6ONk+IEVt1mLh5/9EHjzKGW62OlRi8IhzIziMikjGI9SU1G+6YTxfX/2Q2CDrOYVtjpf8nPCGcS20E2SZaSqQQWAft59hmoUBJRMLKJ/ZrzyMfC5yZ3jPfbgQUY89O9UrklqluK5vaT4V5QPVNfrt+dACrdTAIAzdcD3N7nr15Rq9Gq4gLn7XsHCPQHcdmA9ujc34GK3Pnhz4l3YMKkYU2Z1wy8+WWT5zvGLxLQYmUNz/qzhNNfrpOV1a5DfoLrXA+dN9dW3TxBNUqZmP72drqEAzbszhlgp3U5Oh9wK5MNKK+1bh7bWF7ppq+EWHdWWoKvBc7A8XLHoylEVrgWakBttjMsGuFzX0hQ0o5IyxS+sKAPKkUz+LmDKtnNWK8NPoguqkxUStAD5AWSHG/H5k2/gW28FkL3+JXS7eB7IygIWLAAefxzGXXdhVN++GFMGlO4F/L7DlvOcOxw4U2+lSxyqISNhaC/9tfHBWhfWLS1a5vdLRcCw3sCzpcCxi5EopIZ+cq4eqOlpzZj4DMoUqs/KxmPmtWkOmguMXbNkO5VIN5lM1QmrUoyjVjGIkGkgMU2RVQZZfKSjaUs6Z+2p963XG7AqWuqgZvs8eEgV4nWC5PfPyXBt65rkpvm3PLdzjSoAbDxq3fbkReBT/2tmkQf1JMdERkcJWfS/WIkFZWuxsCyAmz7ejPRwEFW98vG/NzyI9YXFeH/0bQj60wEAhZkl2n2EBK0lh2qAzccp67R0PLUB2XiU5poMP7BolFVu3oBZZ8sZPLtGv+o8zc4V4M7piMcxaWt9oZx5A2KrGNuho9sSdCV4DpaHKxZdNaqysoyihwBR3+4dB2RnWLfRNRgUIIU+IFoePexgnPojBq5hWCN5LSFSuPvZbKvj1fPyRczf/yo+fyyACR+ug7+hHujdG+fn3omSG4ox4L5FCHfPJkOgGUClqTA4Xzl+TSP1U5EdHBbHkDNNjNE5wLyRVqU91bnygeiA4/pR3RNvGwZRBeX9ChDVTu1FpdZZbauw0iV9kjiFmqFxU0Qcr9rT64eBXZITytfL7zNrrrh3zYqZZBy0d4DBrVEq1/+11kAIugfqMyhDwF4u3oMHGW2tVYnnvVF7R+mCM4zV+ykQxrU68axJbpt/+5W5rPws/a8qdL51wipepDpX7Y1h1YewKKL8N+X4NviEwJHcMfjjrG9hw6Ri7Bp6A4TPocjVAUyxH9QT+NI0+sfPx1PbrNtOyrOKGC0eDbxbAa1Kr1OwzI3TEY9jEisw5+aZTwYz4UpqS5BqeA6Why4JN5NJV42qqAXFLx6k8bMwwMol9DnLvFfXm5LjLWFy0DLTiOKnqx3ivkS8UD92m5ntWLFFTyEo7nMGORtexIwdAdz88SZkhFpQ33cA/J/7LFBcjB3jZuGBVzJInGEdfScYIoNj6gB7o1lWpnKDkTnkPDkhDLoGL+yLzlqN7gvsrDJFJEqORxv7uiaNLD7RHJG3f2KWtW5KLgiXa+b4XiWy8PB+W/tmwRynXEdWXm2lKbJz6FYUIxkLYjxGqfxe8jkIUEb00xPps7+V66PnquPrwYOKZNWqOLXHkPen0oTtnlFVDChe0RaZlSBsnLOifMq+yAGxsipg/7lox6uDmX+AEJh4aler8t+1leUAgLLBU/HbxY9jfWExPh4wwSJ+ZAe1ZnNwT2JG8LqTptR83j/BzOCofauWTYwWDHJynu2cFjdMhXjrsu2O1Z7BtCupLUGq4TlYHroc3E4miYpKtCeYCrh4tJl1KDtr3UZAEmcISfVXESdh7nC6DrwNb98rk/j3smDCC/tocWGnSr0u4/qZGZgHuh9Bn2cCqPprAL86+i58QuBYv1H4r5lfx+YpxfjOt6ajaBBFFN+TpMrDspBEOH4nKhZ0tQSA1YCwXDMJBbnA0qX0TJyqtTa2ZfEIu0VUfZZ0BpdqnK3ebzpciS58bFgx5HNi51An5+/UHy3ZC3I8Rqks5PG81CpAVurqmRHdD8yHrhMk8dBxSFatil17DPV9cRvIUzO3909wP66GFqpb5DhVWOhFjXjufmgqsP4QZaXCoOMuKyBRhhMXgbF9rRms9oIvHELR0XexaM8aLCxbiyGfHEPI8GH7yBn4SfFTeL3wXpy6ZljC++fm8/+6mH6X2QQ814dC1EKDVfQ4W8XrMNdFqYJBTs6zHdw4HanKKqXK3ukKdlVngedgeehyiGcy6cxRFZUKePwiOUVq5N5vRAqQI4Zydb2ZZQqGSfwgww/MH2lmZNL9pDr31DYgK80UTAiFzAxXlFEtBLrt3YN7nwug4N0Auu/bAwC4NOg6/G7hT7BhUjEO5hfA5zPws9lAkaSAy0ZGOOIItLVpox2YOy+DVfpeO2QWaXNdj1qgXV5tOpYALcBhqdeXcFhE1UyVzuBSjS2g7Qvf9MF6RUfA2ttMpinKvdJ0/dGSvSDHa5RynWBYcm5lg1OXpZw2sPO+yx46D5JRq6IakJZed5oAgq7mEbDuQ+5/56bRPKO0Ejh6AXhdClQZIIeAm6xzX0OZKcBzsA903Op6YOcZYjmcsQlSpQIZwSbc8tFGLNwTwPzyF9GvrhpN/gy8PW4+/m3Bo3ij4G580iM36nsGgH7dgZF9gN5ZwOZjdH4qnRsw1VWH9Qa+WGSdZ9jxlEU8AKuS4vJC09GS73UyBIOcYKf4GC+7IJVsHbsgmTcXx4bnYHnocuiq1D8V6w5Zf//DDqp3kpV+/JHGtuP6mZOcrsYnGAauG2Byyy81WTMA3MRQznBtqwCK+oeA994DAgE0rV6LCcePIGwYKB05A77v/Bbf7n4vjuSMiDoeOyHy5Pvg5OisQ1vAzY/lxVRdWCfnkdrftgpSN+TvzRgCNAat2TMfTJpIekQIYuZQ4EhNRAjDZhF1k6lig8u2JiuOZ1U9XlE+URJ/VEIOSZoPmDWMGiPLizL/rzY+1vVHS/Y7lIhRmpMV3QSbz1/Xd2dXlSlL76HzoqOVW5MRYVcNSDcBhBUzzf5zquANU4UTwbYKmptUlc23T1A/JkDfGF3AbDex/5xJI28PZDfWYva+17CwLIDZ+15Dz6Za1Gb2xKYJd2DDpGKUjF+M+qyelu9wg3V5/NUN9C/dZ6VWFuVZ53Z2Jk9c1Kvp8j0ZnaOv7ZWh3mtd42U7xPPs2/WlAuJnF8TzzMc7xq4qFNYZ0KEOlmEYiwD8HkS//6MQ4hfK32cBeBEA69+sEUI83q6D9NDp0NYFtKMNAIbK/ebsiZ3SjzzW5/dam+kyZYQNg7tXWY81vDdw42CaxP3NTbj10CZ86oMA8OZLQFUVQukZODZlLv48/Xt4veBufNIzD4V5wJEq634MkIHPBsb9L0QifQb182grWCGQqY9vHnXeftlE87rIi+Li0VRPJo97ZA45U2FQE125TiHNJtPjNlOlFj7rotttWdBY0crNc+vGeUoFzcPOKBVBa1Nq1ShVr41MpZLhqQh2fqTKIIt3zk52hN1NAGHdIX0T+HDQFFiw63/nhOmDgfeOmRTmiblUV8WqdnZOkwFiNhT0J6GLVDtXfWvPYn75S1hYFsAtB99EZqgZ53rk4pUpn8b6SUvw7tg5aE7LtP3+vJEUNGL6uzxeea0LhYExfYE9Z00a3+Q8YI90TfjaWgJhYVoDT1wyn09dFjHRuTHeZ9+uLxX/HC+7wM0zn+gYu5pQWGdBhzlYhmH4ATwNYD6ACgDbDcN4SQixT9n0LSHEne0+QA+dGokuoJ0pIrO8kGiBf9hBzlVmWnQjQh2K8oFPTSQeuYzyalr0pw8G8rKtfxufVYcnz6/DtzcGkP3Gq8isv4Tm7j1w/Nbb8bv8Yrx57e1o6NbLkiHarXGumEb3w03AoU9MByUsotX84sUNA4Hia02D++A5kkaHoGP37U79rGQKZXm1nqbDxg5DADhcY0Z1DSiZMSnTI9M2AHeZKvV+JfqcOS1obp95p7HpsmPJhLr/FTMjmTepKbXa103XBFtnDHblbPXVglQYZO01Z9vRtRixslqLR0eyKpFsiRBmn8Ln9lIWWt4+J8ucr3m/K8uo+WxeNrER1DEYADL95FTEaqI7KY+cq4LcaMXZZGHw+WNYULYWi/aswbSj78Avwjh5zXD894yvYP2kYuwceTOCRnQt5KipAAAgAElEQVTk7YaBwI4zJqXRb5gZ+aXj9ZRHBtdqMi16xAXgxwVm30cO5gDR90ilUyeztCDeZ9+pL1WqGDqJjrGrs4U6Ch2ZwboBwCEhxBEAMAxjFYB7AKgOlgcPSYM8wYiQtTt7stDQEr1wypCN0AWjzHqTeDjYrATIk7PPoAWJaW4rZgI7y89h9p6XsKgsgNmH3gCampDZux8+uPl+/GVkMd4aMxfN6VmmoxFj8eU/t4SJcpIoxvcDDpyLPtz203Te80aY6k28wAoA5xoinHjDNBRWlZNRIkA0ksdn0feag9H7599H9QGOXoRlA5n2I9M20nz2QhFOi3CihqbdgpaMCH6qDVXd/msa9U2pdQs2n+OpWpMuZIBqCwf4gZW3etHTzo5UGGTtEUW3o2vFmo85A8JztywS9PF5k8YWDAP/udMaBJLphPdPiBZ22XSMlGDXHwLu626qxPI+W5vEa87Hb9AcXX6W7sODk4F91cA13ahht52qayx0TxMYfHIvFkWU/woqdgIA9ucX4t8WPIoNhcXYN2hyq/LfZwqtKreMXVV0bu9VkMO4q8raOP3JOXQ9ni21fk+t1SzKB0pK9MEcpgnqgk2pmEfiffaL8u3ZKslkF8j2SCJj9AQtEochOqhNt2EY9wFYJIT4x8jvnwVwoxDiq9I2swCsBmW4TgP4thBCw84HDMN4GMDDAJCXl1e0atUq3WauUVdXhx49erRpH1cyOtv1aWgB6luA7HSge7r93wAqGJYjeT4DGNEn+nuJHLOhBaivq0NVSw8Ymv02tNDxBVflApShMYD8Hmht4ltZR9vo9sH7qWmknioNLeYi1ONcFUZ98Bau3fE2cveWwQiHcbl/Hs7cMgM7J8/A6bGFEP4kcPniRIYf6JEBZIs6nGrqETOS6iSSkekHmjQ1B/y3ZgfajG7f3dPp2ndPJ77/mTrr9td0o/Hrni07yPfZ7h6q4HdKfa6c9uX03KuobgCq6syC8LweQG53d+fjBrr9Z6dHjx3QvzeW90KKzvfKBHJ8dejVs+3zzezZs0uFENPcbJvsNQXofPNmKvDJZeBiE9A7k94dt7C7NuqzkZNF/+Kdr52ge+8HRN4P9fg9Iz0Ja5vROnfzO2nZFhpBHoNEG+pbzHfFDfLSaU1RYTdPqp/zUmMY1AuwusHlgQEgHMaAQ/sx6oO3MGr72+h9hgq/KsdOxOHrZ+DIDbfi4oBB2q8O6knX6VKTfowMIX3G88aRmuhz063V/Nyken5zg3jm43Ybj2KPAJ1rjB2JZM3HdutKR2awdM0N1PdpB4BhQog6wzBuB7AWwBjdzoQQfwDwBwCYNm2amDVrVpsGV1JSgrbu40pGZ7o+pZXAF20i87q/5cHand1vAI8MN6lKbrIFvN/mIKkMPTGLKH9PbwdETQl+VTkrar8A/f03+8mRkvwrGKDFgxdxLmo2ACzvCzw5y9zHyjIzUuczBEZW7seCSERx0kkKgV4eNxFn/ukH2Dq1GKPnTsHqAwZRChUJeB1SoQDoA/CzOUDLyRL85tisKLEKHdS+LYw0H10rXf3BmBzgZK3JzeeaBxkGzNYqGZrn5QdKBuvTE81ItfpcOD0rspjDURc1WK+9UYK92bOitpOfGfmZkp+DDJ0qpILSSuBR+V1IckbIbv9u3ie794LxnYEluGnarHaNoCZ7TQE617yZCtjNxW6eAadrw/Q9WaTG6XmPN+OrvvcZfmDVrWatFT+bOsjvpLytT5rHGQaA7wyncT26Rmo6DL1TxL9/O78Ez56fhYYWcxwZfmsPw3WHzDWNvw9ElAvD5lpX0D+a/q0iLdSC6YdKsGhPAAvK1iLvUiVafGl4b+wcrL/1UbxRcA/O9o5cWAGgMnofPgAZ1dSTShWWSJPGxNcQMOeNbRXAb/ZZ/66rkQXM58Zpfov1PHSWuuxkI5Y9crUj1fNxRzpYFQCGSL8PBmWpWiGEuCT9/JphGP9uGEY/IUQbCEoerjSs3i8tVBHpVZ4s7bqOf2M6US1aQtFULDc0KrnpYzhMhu64fpFjHqMFQaad8XjkFL3fR5m0IBc+a6h6zN/nhWVlGfDoxjAKj2/HwkjX+1HVJJ+3c/h0/O/f/xLH5xQjf8oYoqDUAP418XHvU5HTDoNEJX49JtI02WW9lqosBdC5TMylOoWNR62Gz4gc4PNT9I2TAcloEfqeVzJto7qeZO+5fxgQrQrm9Kzwz26eJ5Zi/s1+d312SivpmWOqT3MwNnUq1XQP3f7dGi6tghg2dSJCeAXWXQG6+RZoOzWVBQu43YQI2tO7E6HCOtG15BYUumCNvH6o7+qDk4H/2GHOUbwtvyuq0/jgZKLN5WVTdmHtQfNYtc3kVM0dFq0gCkREcE6Z8x2P9ebBwDsnqWbL79MrDgJAt6Z63HZgAxaWBTB37yvoffkCGjK6o2T8YmwoLMamCXfgUvc+zhcSJnWRacEjFeW+BSOB2cOtc7NhmIEsPqc0P33fF1HSZRl1O9jNb7ECUU7PS1d3vHT2iIf2Q0c6WNsBjDEMYwSAUwCWAVgub2AYxgAAVUIIYRjGDSB763y7j9RDp0VpJS1QrVFAI1qK267ruCqn/fR2YNcZq7NmZ9SpvYmCYVrwF48G+mRRtK0gVx91lY/77IfUx8oJwTAQKGtBz7e2IvRva/BO2YvIv3gKLb40bBs9C6Wf+gbqb78HmUMGUv1RDYDNZvQ0bLOgJgscqTUMZ0cuJIDLQTr/x7ZYo6i6rIUaMfZH9h8WpBiVmUYLb8kxErA4dpEcrq0nzEWyvNqs0fIBGNqbpHzDsO95xfVLT283HThZFUw2HGPVhritH2EpZjtxC52CmdwXy+dzt3imQtjCbv/xGLp8jo9voXoMFYbhGQedCXJ2VlaB1AUDklVDJatShkHZmg9ORz9Xbo7nprePvM3KJcAzH5KqKc9xnA26abA5H+je1QWj7FVhi/JpnZCbzTOe3h6d1QqFybkaZFU4d4TcULglZK2f7V3/CebtfRkL9wQw8+Dr6NZyGTXdr8GGwnuxYVIx3ho7H00Z7nmeBqgP1Z93m+/97OH0eVU9OVF8jvLcLIS+nQRATiH3+4rl8OjuYaxAlN3z0lGCWMl06orygao+lLnqqk5iV0aHOVhCiKBhGF8FsAEk0/4nIcRewzC+FPn7MwDuA/BlwzCCAC4DWCY6qmjMQ0rQ1slElnQ2QJmN8rNWlTK7qD1PxjyRckaK4XcwWovyqUhXLkreeoL+fTsfeOE48JxQiomlDBofd+Mx+3PLam7AzAOvY2FZALcfeBnda2swJL0btoxfhF8WFmPjxDtxqXsOrssDXrydFuTWc1CyYKkAN/jNjtQkFORapc91CEbkiZdNpKLrlkjjyIen0qIsU2ZUjO0LHDxvOo7NQbq/f7grQs15Lzq6zYpUvEh+scgsLrdT8mKo2UaAnjW/j4QYWJ3LqWDYqahYzWw6RRpVw2H6YIrGNkcivE/M6nyLZzyGNVPAyjQUVr8BDOzR+c7vaoU6X/pgzQzo5ttkCF/wvmV6d4vmuVLf21O11v5pbjIWsggFB+q2ngBlvg3aflcVBXp2V0XU/ZTsiOx0OT33fJwPTlv7N52qpWPzGsJ9AVeV0xyYkWaKV0zIpf+DmmCaUH7Ou3AKCyPKfzce3oK0cAin+wzGc9O/gA2FxXh/1EyE/ImZhn4fOZQLRlmvI1PpPy1tq87N8nPB67qAKWkPxO/wbKuIHYiym6M7QqI8FU5d93SPFthR6MgMFoQQrwF4TfnsGenn/wvg/7b3uDzoIRuEydpfWycTdXJcNhF47Dwsk2WsqD1PpLIvpKoV6dArU09hA/Q0DNmgz8kiOV5VzalXQw3m7n0Fi8oCuG3/enRruYwL3XNw+Y67cHpRMZZeXIALPmvlbl62uSAbqSigcsDag6aBtXQ8ZZTsnCyfQTSX375HP4fCZl+tBaOAYb2t0UYVB89bM2SGYRpPTtFt1eDjflLy4m8YwNwRVmlkXZaTM5Kryq3qXItH20dUdQan7tkfEYk05mRZjTMduoK6k1vFKqbw6OrlAPrsYpPXZLizQJ0vVUdHnW+T+awW5dvTu9Xjye8qq9MBlD1vZIqyRCmXnSrDkGqEQmbbhzAoGpyVRs6MzOpuDALf2gAsGg38aZc5Pq6T0p17LEql3wc8UAAMagIe6EfnwnNrY9AM8Mn9FHUYWXWwlVI+5cQHAIBD/a/FM3P+GRsmFWPPkGlmYWobwFReDiL+YJMZNJOp9PyM2D0XycqEuglE2Y2jIyTKvb5TVxY61MHy0HWgGoS/G9v2fSZjMtFNjm4bsjJU49xnENedhQ3ssmzy5K1S41SBhsl5Eedva3SmrP/F01hQ9iKW7F+DSftLkB4O4kzvgdi9+B9QtWAJBt81E0VD09EXwJ8ikX5eaNN9QLd04L4XaHHz+2L3R0kW+Bhh0DXga7SsADgXkebl8/QZwPX5Jg1OvjYtIbMRp1N+WlV+5GguG0920W05Y8jZKi5IlzN+rx+J1FwttTpZKp2ktQ4kBPxxJ41Zjj6r0Dn4unYBD/SgscVDq+vMi68bw1ql8OgQFkBdM12Xjuxbd7VCnf/UejkfYhugds+q3HtqRsg9o0GVR9cdT35X2ZFSeyvJlHLZqfIJynawSuCEXLPHFfe8elfJjgBEU5ZZDc0hCjgB+vc5piMR6c93jQ8YlG2dH+1iaT0ygLomgcKTpVgUcarGVNFF3jX0evzyjiexYVIxDudda3+BbdA3CygaCBytAT6W6qpkkQq5nlmm7wP0vLjp7acLbp2qtW+ZYQe3zr1uHB0RxOoIp85D6uA5WB6ioFvkVGeovqXtx5k+OCL0EHKm48WCLmLqhnMvb88TqVpTECvLxgt9fbO1IPmuscBrh8zvLZtIUVA26IdXf4yFewJYtCeAqce3AQAuDRuLP815BOsKirFvxPX42/0+3KMZa1G+2SzxUpN1QQ+GgWuygAuN+sxaqhAWNJZlq83Iq5xNEwL4sBK4dUD0dwWA5/dSDyu/DwjFqBkzQFLH3Dg4EfGSFTP1GT92yoDYkVU1yh1PgEA1Ut8+AUwfCLy9/8qKYOroUjJUCo+Ka7oBFy7Tzzo6mIfUwm7+s5svY+1LFUCRe08NzAe+vtpZJVAdD8+/OqjGanV9tCMvU8p9IKfKEGZ2+o876fn8824rHa+mEbhzjHXOt0NrM/ZIhuuLRWYdkltKZe1Ba0DPADCuL7BPqqfyh4K4/sjbrZmqQRdOIujz44NRM/HXW76MDYX3ojJnSOwBO+B8IwWi/Eqy66GpxOiQ65lP1Zr0fYDGnBGH06BS+HldsVMTjLWfRNDeQayuwEzw4B6eg+XBAl70eAHjaL66WGXH6J+QbPWdtuzPDRVRNgTVRcKuAFa+TuP7Wff3yWWrGt2PNgtce2InvhlZ/MadoXZue4YU4a+f+imm/J9iNI4Zj1+vMWifPuv+VpaRgzYhl2h21fW0YH2okcf9JCLccMNAYGBPoo98cjm+axYvDESUFSPGkmrIWFQSNWgJk1jF/RPMwmd1/36fmaU7dtEqbGInXqILEIgQZU2EMAU6WuWAfcDuM8BT7+sNPdW4lOs04gkQ6OpJODp9JUUwY717bDTqau8MAN+5ia6xqtbmoX0Qi2Vgl7VVoXsOeN8MAan1ggvRmHCQ5ld5G3WdkOcCzpQx/EY0pZwbAXMmieeG5qDpbG09oe8xo+47rDAJBGje+v4m+n15oTWDJ2PmUBKFWDaRzqPkYDT18cA5ICvYiEVH3sAtHwYwb+9LuKb+PBrTs7B13AL8dvHjeLPgLlzI7htjtPFDnst9BjlXX7k+2hnijBM3U47HMWLoMnpXsuPR2ZkJHtzDc7A8WLB6v2kkN4esC5hMy6h1iNzFI3UuF7LK8urqotmWWi03VERZypXl04UgqVgdLUG9Tpl+6/6y0oCPqkI4/vI7mLNjDX5cthZDao4jZPhQPu5WPHHzU1hfcC/O5g7DqqXAxAhfnQ2MlpAp1HDwnLkox+Lay/iwEvjZtcBH55PrYHHPLtmA8Pvsm/+6xZtHqYYrMy2aRmmAFKgG9SSn929l5t9UWqEua2KbeQLwQCFRGsurqcGzRdVR87zIC2C8dFR1nN+YTtLKLSE6yaXjzezklRDBjPXuyUbj38qtNNBBPemerJgJpJ9Mft8uD9GwowOqLQISERtQnwPed7PiZHEwRSdAw6yHUGSefGGfabTL4htyb0J5bC/si5b+dnqH7eYMNRiQ4Qc+f51ZjzmunzV4omLdIfpfrld9bi+NWZYvLz9L78CMyO9F+cCOAxdx+wevYsGeAGbtX4fs5npc6tYbGyfciQ2TirHl2oVoyLRvnnrvOODFg22nketogaoztKyA3mPOdCYCjzbnoavCc7CucCQjkxQPLQNwX1ulqj2pcua8L7sskttzjDVBq3UgMmUpGCIDfFBPa2brXH30cX4+B1i9sxHDd7yH/lv+innlL+GBumo0pmXi7XHz8a8Lf4w3C+7CkFG5WDYRuK3aOobn91qLpt86Qcb3Ne5Vci0IC3La3C6kdoIdABk9tw4l40HuMcXGyhOzgP/cGf298f3IwROgBZl7pNiJGciqj5eaKGocCpPBVJBrRn2f22vep5Cw9sTRGYCA+dwW5FozTwW5wIp90cIkbrIm8dJR3eBKimC6oQHz+fbMAJ4tNZ+Nilrgf8rIeP39uCvnmnRWxKID8vP89PbExAbUObgon7L8assGlu1m4ROZSVGUb81ys8Ic11zJggqPbibnhB0wPl4seXYZ6pyhq7cFaEzfn2H97BvTqf+UbrK7phvww83WfQXDJHwkZ/VCgs51dPdPcPxXf8CwTQF8YeNG+FpacLbXAASm/R02FBbjvTGz0ZKW4XQLANBc/bnJwIAewB92xNcfUYWOFpiTZb3PfP10z5XbedKjzXnoqvAcrCsYiUQal443C4HTfGY0XV1QJzrsw23ESZ44T9VKCkmRzBlLuMpUAzmKaienq9YExJqgnepAfD5rk1/OcqX5TYekR+Ml5L3yGm75zwDue+s1ZDTU4VJWL2yecAfemFyMkmsXoTbTbFxSU2X2IuHM3dLxelpdcwg4U+dwsWPAtXNlEO2zttn8rFcG1dqFI7Q8uU/LyjIz+8gS5scvRu+3ppEixXxPqsqB5f2AQ+cpw8ZZMJmfLxs8rCwYDtN9ZkqSKpH/lqQauK3CzIAJ5VniRV9+HlSqEhChD8XJ9Qf0tV5ONSpyFhfCnbF6JaK0kupcDET3U2tJUs2nB2dYaLRBa9DCrg7Rbn5XjWe7Obgon2hwe6ut3+fm64DJpOAxFuRSllul5uZkRffRW1lmiuDwOBKhNBbkms5Czwxr8+AMh6CjzoHxwT6DlJcdUVgVwJBzR1pFKqYdexeGEGgcPgr7P/11lN1cjMebp6MFPs1eTAzvTbTEVghzPmxLwxs7WqBOWVXnkAPx2SZXUtDJw9UDz8G6gpGoSp/PMGlggH5BdaIIxhNxkgtZV+83jwFIi30YKOxPTR3ViJhM3ZBra3S1M3bjUBdmhl+SdS2ttNI5el44izsPvITZO9bg5o82IjPUjIu9++Ni8QPYNnQUvtPvGwhlZGL2MODubFqUOWLIjhNnckSQqCB+n6Jwhehx+Q3a14Um+2uaCFhCXUZdi2kgBCUHB4hEl6W/rTukNybO1NF1mzsicp0bqUcYOzSDegJ3jo2OhPJzU9No1kHIIgeqRL6Q/p6TJUlIC6pXU+s2npxjfR5kqpJfog/FC7taLzsjQqUutoX+kuy6x2RApQHbzUGy9LdPmBRUgL7b0OLJtKcaqpqqXSPfWPN7PIG90krKhLMYQrrPvO9ynU91vSmI4Tcoe6LOVzWN0XOmPC/YsR6AGDWbQZrDwsLMpsnNgwtyrXRk3vepWv26YscSyPAJfLPbHvzT/gDSXgpg/Ok9AIC9g67D+/f9PV648Vt4KaMAIWHAHwJmjyRHpqLW3MfCkXTMIzUU8DpxMbKORwaS5qe1pllT8+gWqliFOuepyqrJkly/0tAZ52sPyYXnYF1BcMOfj/U9nUH0leujF9QSBwcLsDo0biYSddEGIlG2CCWjrMragZ4nZ5bTNYSzqpvdGFaW0eKpW2yWFdAC8fR2YMsxIP/csVaFpmlH34FfhHGi7wj85davYv2kYtz3Dzfhgev88L9RglsbM7HxKEmVc48oNWKYFnGowqCGlT4DuHEgMLovLdqbj5Fik4ywAIJtJc9L8BnAvBFU/yTDQLTD1Bw0DQlZHUoI6sni95EzrH4vJOg8Xj9CTZhlKl5FLUWDlxVQrZmcjVy5xJ5i1qqoFTSl9eXFm50vH4DcbGvdxvN7rZkppg6xwWSXtXLzHMerMig/9yMuJI/G21nkzN3OQep2M4fSu8OP0qUmT6a9LYhnDpZrh5qC1uyRrBqoa15aWknfb62hjJGxkGtZAWD2cOC6AVYKcrqfVFF5u5CgYFWaz2QA8FzB9ZtGpI0DBzfk504VYgCICi7XbeVk0fd9kXWFnT3OpnGARpexlnvrxWpL6AuHcP2x9/C10wHcsH0tMo8fgTAMHJ94C/5zzm/wXyOKcbLvCHw7vwT/W1nY6p2FQtFrg98AZg2n+5ObLamsCnOe33zMSseMFwaA+SOt/QJjzXk6+wG4uuuqOut87SG58BysKwRu+fOxvrdipn7iSzRFH89Eoh5DXeybQ+QQpSvCE0zDkhdl2RiX5YDl7MTKsmgePCPdBxT0E/jJv5Vj9s4AVpQFMPHULgDAvoGT8K8LfoQNk4qxf+AkDOhp4Os3UK0WY9Mx66IMmNQPxtQBkd4qMFX2Ss8A351B16Fcoc3wdnXN0Z8ninkjyKBRF+uhvTWUv0hj354Z0VHiD07TNXsg0gNrg7I/JzCVx+8znXsRtDrbKory6b6zsiJnwPj5yZAoREvHW8fUErYajSotUQVnV9UaQd328jvnVmWQj11SYj1mPNFNu4hwR0dJ3Waz1etWXk3XrFVVDp5Me6KIdw5m4ZVmKSDxwj5yQjhokZEWvR9ZZCIMk+6Zk0XvT6tSpMN9zM02HTe5ITj3kmKERbTioGrIA9GqsGr2JByZm7lu60clNO9x7affZ52nVajvndyKQ/WsOOiTHmzGzR9vwsI9AcwvfxH9a6sQSkuHf/48HP/q9/BAy904k53XOh+6TTWFRCRYGHEK5a+FBWW1WjTpM51okR0ESGjpS9PMz2LNebo56Gqvq/IyeFcHPAfrCoHdCxvLMVK/J4sMxJr43BhvbhdWu30vHk2LG++DM2usTiQfu1QjWc5jkKOf398EvH8KeOmjaOfKCIcx9fg2/PRCAIP+JYDlFYcRNgzsGH4Tfnb3r7B+UjFO9Btl+c4nl8kY4DH3abTWdPki6nA7Kq1ZOB2FxIlGBdjTBh+aaq0LcIvc7EgE0mcuvn4DqLik2VhQnZwdgmHg5EW6Z1tO6KW3ef/qOPm+8sdh0GJtRzErrTQXcpXGpFu8+2Vbj/d+hTuHiY3GxqD0Yche8ZKPz58lojKYSHQzHrW39na63AZneBs5uzB/JDV/9mTaE0c8xhw/G7OHUdBFwKQty+9mi+YdkGmerbV0YQp6Ccl4l4NfS8ebyn6GYa1l4ufm6e3R2f/0SJ+9YMh04uTvyOAGw/yuP3abVVyptRkx6GdZ/CEUBsb0BXZVRYs8lVaavfQMYdap6hoQZzfV4Qtn1mH0lgBm73sVvRovoS6zB0om3I6Nk4vxue/fjilje+GV7UDluxFnJwT0zwbOaASV7NA6r7pcB9hZ5mCVrFLLgkSqT6a793ZzHmA/l13NdVWeMuLVAc/BukKQ6Atrp+4Ua+JraAEe0vTLkqF2cnfbTFhXNLutggqhmQcv07hkzrubeg/A2iAyPdiM6YdKsHBPAAvL16L/pTMIpqXjxNQ5+OVN38Hrhfegupe1Q25ud6C6gX5mmgoLKXxL2pTruF4/bHWuAOC0RrzCF8kSlVbSOT6/lxwfnwE8PJWEAORsI2e5hvWO37kCiIoImHQWppK8oWSgfIhEeR2OIUBiE+9WUBPOTy6bfbv4WOkngRfuJzrgqr1Alh/YWUXX0DDM++cziM5SFTEuVOnmWMqS6jPMhhw72q30Gei/z2DBDBmGoXfO7CK18RoRiUQ3dU5lMorL2xuqzPN1AyhafmSnJ9OeKOKhisvObbrfzOKEhfXd1Km+2lHFZAEZA6S4p9JznSiyMh3YiMxPX5xGc4hOAEc9Hzk40hyiOZNbAwCUkeceVz6lDlYgInChqA+q2TrGuH6mAE9O3TnML38JC8sCuPXgG8gMNuF8dj+sm3wfNkwqxsUZ8zBucBb6ZwC/3QssblLqRyPn7qaZsRvMGwkclYJwBoAZQ00hE4BUD/nv7IxuPkbZr6MX6Dnw2cx/DHnOS0RxMhXo6Ey+iqs9g3e1wHOwrhAk+sLG8z15kqqROPFqvywGZx+A6IXVCWqBMRfN+n2kOKU6V7rGhrIhsXR8dPPabk31mHVgPRbuCWDO3lfQu/Ei6jOyUTJ+MTZMKsapGbfjw8t9tONL91l7ejCPv3XMsIosBA7YUEyEXhp9VTldzxUzTccnzUeF1QtGmffg9cPUO4gb5sYLA3Qe2yooEswZwn7ZVoNm7ghgZI5Vtc8OHPFee5D2/85J+j5z9kvO03Y1jcCK26yOiUwvMYxo2qIQZkG83b22Axtyam8au8yIPCafz4xIGwAm5lKheKocF7tslBuanfy3zl5crp4TZwTU+1qUT6I6nhGSGNzO8apzK/cwWrEFCIECRoX9KbOy8aj1OZIpevK73FrnFAkUcVCnrePfVqEXwAEiGe4tSuZZgaws+sQsk2YuixnxHKm+W3K2jhEKA+UfnsCMLWsx68UAbji8FX4RRkXOUKy85UvYUFiMD0bcgrA/jZRTa4AdF8yg1dYTwJgccy3tNB8AACAASURBVH8GgLF9qf1H3Uex67mc4APRqH86O+KUChKqkFtbPLY1ss4a1jmbW2MsW02UyrCI1NoClvlP93x1hkxNZ613upozeFcLXDlYhmF0AzBUCJGkWIqHRBDLyLJ7YRP9nroPeZL67sDY26vGUqz+WQy7SGgoBGw6Sgs0j9dCQYwYBeo4tlUA1w8EPvr4POaVv4yFZQHMPPg6sloa8Ul2X2yYtATrJxXj7bHz0JRBTacG+KPHleEnJxEwqXLsOC4dT2MJKws61yfp4PMBRRF+v4ApLMF0ynWHzKgx0zK4uHplmdXhSSR7JUDiAcN6Wx3C+mZrU2nOhiSyfxa4KDlO0VBRC/xkdXT0U6WXbDgcXYxtoSrZ0ESdwPUlH5w2DT++d7rINz/r/zglEuGOGCXLJgKPnUfKHBfVoAQSMxDsDNOONngAe3EAvi9zR9B2nGXw0DbI75jdeqAaw/K7z/VOIUHvZbrfSo1Ta3U5Aw/Q/3bZJvk5eCQ/WilSHqsqqsFCFCxwlJNl1kty5l8FO3iqEEd5Nc0lAO2LPRmdQAY7kOl+QLQIjKraj4V7AlhUFkDhSZqUDw6YiKfn/wAbCotRPnhKZKfk6ORkAZ80mv2+ZHxcY/4sItuO6we8dyA+58oAMCmPWBMhJVihoy3LDqMflDlWHcrW+lhhCkul+2mMTjTAjs7UdKagkoerCzEdLMMw7gLwawAZAEYYhnEdgMeFEHenenAeTMQThVlZRsY5d5RPxDhTF2FLYXDQ2peGO7rz4qhmleLpJ8THZfpbdT1RFDiDUFlHNVQAnZtKQZSbyL6wDxhQU4E5u9fim2UB3HBoC9LCIZzuMwTldz+EXTcV49dpt6LJSLNQ39J91O1edmBG5wBfmGJG82RJ+dY+WUw7UaiAOvgjjuPOKisVBzAXxMWjqdic1e9e2Gcea92hGPt2UbAMmAp+cnR07UGTm89GEtc4OEFOoqnHbg6RgfWNPEk1zIHal5MV7WCl+UwaoUoTleEUUCjKN+sN5L5eMtQFuVcm8Px91n3qjBQ7xyURekqyaDZq8KQzGDyAXhxAbsvQmh0BPfe/H9cx47zSwJkIHbVbfTcAeva4UThDwKx9sjuG2pi+ptEMlMnKhE9tk5oDi+gslN3axT0JZUr4ii30N1kYRcW0fFPlL4xouiPPL0A064KPKYJhTD39If5cuQaDNwYw5MxHAIAdw27EL+7+JdYVFuNY7pioY/tAjsknjVF/skV5tdmqIh74fcQQANzRlmNlmtS/y/39YjkwHZ2p6QxZNA9XJ9xksH4C4AYAJQAghNhlGMbwlI3IgxZuozAry0wHZOsJYMHI+I0z3cLGctkhm8VrVTktUpzhkakmg3pGH1NndOrkc0OaKCRARkBNY/RiWNMIDDl9APN3B7CwLIDrTlDq5eO88Xhr2XdRvaAYo+YXYdpAA9MATJEiklzPxIb7sN5UHL23mjjoPyqhvy8vtG+aCQCl79pfW65x2ng0kpVTMjGAdb/l1Sa9kWu9tlVQbZNckMz7njGEjKMVW6zqhbIDKTtTIUFNf1nSmKHSbnQ9ZmSM70d1TSx3fOeYaBERtceX02LHjs+qvUTHC0ei1I/d5ty4N1YgQhXHUOs2AHd1iTqjQc36uRmPGyTbQOhogweIPicWs+GstfyseI2GkwdZ8EeldsvvxrZT9JksPCODabO6elfdWiXXFgmY64XqDF1q0qv9qcqYPyqJrpXiWi+7OSrDT4IVpZWmGuKMIcCQ3maTe0MKTHH9FQCUnmjBume34se7A1hQthb5F0+hxZeGbaNn4dkZX8frBfegqs8g2+vO9U4Xm6IDR34DGNmHGgLrsm7TBwPvHbNuL5Rxykjzmf0bgbZlu93+vTM7MJ0lqOTh6oMbBysohLho2IWrPLQL3BpZamajqj7+yU+3sH3lenJg5FomFh6QKXw6GXU1mm8neW0nn+tDtJPAEVbDAHxhgaLTH+JrhwPo/koAXzl6AACwe9gN+NVdP8f6wmJUDBzXGnUbFXmUOdM3IbKIqhmR5YW0/Z4qOj+W8WWjXOc0Ll8DfNWhxiDNR71etp4w74l6XPlnph5yXdKqSM1VRhrwpaJo8Q/m1I/rZ22EKdOv5KJzANh+2npP+ZrL986uGTMjM5KFC4OMlLF9SbL9f8r01+DTE2NnNfn6l581DbmaRn3/HUasQISbQEW8C7IuYh/P8WLtmzO6To5lV4PuGsuy3HKAIN0PZKd37Hi7GmQqW6znhufBbmn6+Vdd+ReMpDnMrgWBXe2fGtgJa4J1XG/L4kYqBRCgfakqfT5QE13AZAPMHkb1pAW55jUArOyDb0y3fiYL7XRvbkCf9a8Dvw5g4tqX8d+XanA5vRu2jF+EX00qxpsT7sTF7jmWccjnqDZC5zVLdbAK+wM/jmSbnvnQWnvKdPiqPsBn+pqfcTDw0HngfYmCPjoH+Jf5ic0RsQIvdn/vCg5MZwgqebj64MbBKjcMYzkAv2EYYwD8EwCHGL2HVMDtJLZ4tDWzcdNgYEA2OVqfnuhukrFz5lRjf3mhaby7lVFfvkaR75aMTs6ScVNZn2FmRP5xCnC0xjyPcX2C+P2v38KPdgawoHwtBtachPD78d6o27B+6Vfw5qR78dXiwZjTD+guFV2zcMPYvqaqH1+vDI0a4vTBVpGDsIM6IRvTAC2uhREOvBylZSfB7YIkUw85ygrQNaxtBl5c5k69jo3XU7XA3xTBDzYCeMx52STfXpBrFjDXNOoFORh7q6Od6oPnoh1jgO7fk3Psz1lGvNmbeKkudvuLZ0F2cqLakn3qrMXZyYJTVlAOECwdTyIXHtxBVbiT+1bJsujpflLQY8YDQPRodqrkRr0PTgb2VZu0cyen326tSvebTnMaz+/hiDJd5Jitwbog0ZeZ6hySardkVUFeG7gPHuA8r6q1Ya30v8h4+zfX4P1nX8G83QHMPLAB3ZsbgD59UL/gLnwrZwk2j12A5szuuHMMcFF5JrnHoty/8G9l5j3g4NDxiyQFz1moPVV0v1bMJFGO1iCXYYopdU8Hnpylr10sPUPXMc0X7Vy1l3qe58B48BANNw7W1wD8EEATgL8B2ADgiVQOyoMebiYxplZxZuZPu8xFbd85PSVKnYTtFkj58xEXgK9GjGQ2huSslC47wYaonC1RjU5edAwAt48mmlkoTPLkf1t8GVPL38C5nwaQue5l/KX2PBrTs7D12oXY97Un8N51d+KPJ/q27mvzMboeRflEO5EbQKqS6YBJmeGx8rk/Mcss0M5IszeS2Zg2EOktEolKqteGG2+6BWf3ZAqfABVy83W2ey7UInHOIOp6VPl9dF3Kz0bqEmBGg2cNowgx1ykIYRXWCAsSf5CpjqxKJcMAOW5MA4r1POuexVg1Vm2hujhdP7ttnZyotkR3r+bibPWZLvEcLNdQFe5kuu9Xrjdl0acPphooGcN6E1UtHHnv73dQbWWnH4h+n3XO86qlNPecq7dmljjLln8OSDtr7VnIYAqgqlZo947roMs0l1YCe3aexpw9L+IrmwIQmzfj/mAQZ3oPxOobHsTAzxVj7oO3oW96Or5QCUysIBrjek0NrN9Ha6GciZUdQc7AfX8GqcHKiqbNQWtNmQEK+LGCaHWDmWmUxTlqGqPrQ+3O102AprPJmXvw0JUR08ESQjSAHKwfpn44HtzCaSJcXkj/uDieoUrZ8n50k7ATHaAoHygpsY7hyTm0YDlNzrIhKiu4ATTWXWdMDnpLGHjxINCz4QLm7HuVFJq+sx64XI+Mbr3x5sS7sL6wGG9duxCh7GysXAJcVpyWjUdpUeJFXM5E2aG63loHxmO0W8Tk67h6PzBzKJCDaCUlvjYq/emFffoeYrprl5FmlR1uCevl8eUxqdHOmkZTQETOiAEkP152NpoiFApRX6x0vylYApiOIxe8F+SaDr6sPAZYqTMstR6rhxRDfhbdGA129E01iBALbg0UN05dIsaKV5ztIRHwcyMkIQf5+ZGfR5XxMDIn0u8I9I6q9bOy0y9CJIUuK9W5MeI3H9dvX1Kip6HrKMvqnODGKZDHPrzyY5xdEcCANwL4h2PkZV4aNhYfL38EP72mGDuHXg/h8yGtEXj+nHm8g+eAf7Hh78j9ENmpunMM8MrH0eqJRflWRVOZmgjQ7ytm0s9MO//1Jmt2y+8jRgKgp03HG6C50jPmHjy0N2I6WIZhbIam/EII4ZLk46EtiCUG4TQR8kIr1zKoRlqiUfKGFuCLNo6Z03motBL5XPghy710BvPLXsTCsgBu/ngTMkItONtrAI7d+Vnsv7UY32mZhUZ/Rquog9woUXYaQiKi+hShudw5hpw2Po7fAEb0MRsopvuJGsfXg2vKVu+n87Or/WFlLr7OkzUS9nxt3Di9OrAR//gWYFeV87a6mgoRsl6LlUvIIVIzc+XVgE+Y9Qx8XwSIhiIbXEX51n3IBsT0wVa5Y7kgu/Veh0wKotuFPZHnNVHDIZ5jpYIi05bsVzLQUdFsL4reNsjPTawaLJXxcCQiE646ZYC19YaIUPss9UQO70gsejhDpqFz5twAZbx0rIiVZWbfKpniHfUMCYHZNbtgrFuDubsCGHdmLwCgbPBU/Pr2J7B+0hIcGTAeP51jYE8JnR8QrWyo1jhf0w2oazYdRsDMMIXDxMCAiGSpQpS14vVKvU8W4Q5h9iiU1wt2Ngv7k2PLPRN1c1q8AZqrOWPuwUMq4IYi+G3p5ywASwEEbbb1kETY0THkHh7hoH0Wg2kZz35oX4Ol1j3pJmGdwVPTKC2WkqHsRFXgqJ6scMST+qDqw1i0h5T/ph57Dz4hcKzfaPzptm9gQ2Exdg27ERkZPorqbQX8UpGybPA/NFXpD8W9O0IUSTRgKvl9cVr0YgyY/azYsWhyuMbyOTCEMNX+VFob909RnV5ZWl+WDlev/Y9vM2WWWWlR7h0jq0gCVFPBaF28JaqNTGd5bCsZBT4fqfWN60fPzoZI0XVYRMu1s4SwqjrI10EG13WwLL3bHlLyNUgkq5Oo4dAZMkh2jluqnZCOimbbzXnVDdE9kjzYIx6Hf3khvetykMgfyaDognp+Hxn4ZVXRDbtzsvT0Xzf0cB63rlmxKh7DY5KbAssU7+VrgFBLCDcefwe/rQ8g7/UAJhw/jmt9PhwcfytKP/cUnh97L1bVDbPss6ZRooQLk6bHUNVbLzZS3ZUsovHcXpMpwU3gDUH7e+sE8G4FHYPp6/J10lHR2XEDaF8ZfqCgf3Sz87bSoTvDfOfBw5UENxTBUuWjdwzD2JKi8Vz1kA0n1TCU6QeyOpHcH0mHLSdoPwfO62uwYo1HZ/DUNFr7Tzk1G9xWYY3qPboZGNdXoOjsbhT/LYB5zwUw7jTJze0ddB1+t+gxbLyuGH+3bCLeOWxg1wnTgI8lENEr0xRjYJVDIawNi/0AJg+wOmbyflhU4rm95JTEusZqptCAtebqwcmRRrWRhfPz1wFvRpyWL0wh2oksrQ+Y/bZ015RrGV7YZ0YwOTO44bB1bMN7A90zaDFm6BqDcmaNVQBrGunzyQOIHigXauvOnRdlNrBO1VodLL9B9MKRIWDVDH0BvF0PKfUaxJvVSdRw6OgMkh3ao7bCUqfXjtFsuznva7nAo2s82lKywc/FqVprkCgsrO+6Sg0EzJpMplIX5BL9V9djy44e7kRDV/u/qUGrbRVWijMAHDzVhPJtb+LxlwOYW/4S+tVVI5iRCSyYj2P/9GMsa74LZ7vnUlCxwfpddqaK8vV97gBaX2TwdZLZDXLNbpo/ItQhZfKDihotgx1d9bgrlwBHdgI/n2Ovhsi1WrHEjpzQWec7Dx66KtxQBK+RfvUBKAIwIGUjuoqhq5mRDUPAWrzMUPuQ8L7khVMX6SqtpGxYMGTtZwLYO3n8d/auuP9UebW9QcZ0MV8ohKnH3sOiPQGM+EUAqDyKgYaB2htm4O1P/Ra9l92L5qEjkFUBPMELXS6w7ZQZOY1VQ5OTZUr7pvvNvklc+yTvxw7y/uUeVHZGpuz0AEBOPW3PxsgfdphKek1BUsdiw2DFFuohJWPdIVpsnTIvJy/SfQvDSv9Tuykcu2g6iXy/JijHA+ydEK790skx87nrIs5+HxkXrALJEduSEv31s1vYddfgK9fHt/i3xXBIBfWvrUh1bUVpZXQD7/aKZqsZdcDMfLil03pwBzUr5TPMeUknvc59EMOg7FWa39pE/geb7HtsuX0H7bLVfh+J+gTDpvM2fTBl6DMbLmH2vtewcE8As/e/hh5NdajL6onNE+7Am9ctwYPfW4QpY3ri1e3A2ff0EvGT80iUKJZjombw0zTvhuwonaqlIJgKph4C0bWhukxU7UFgltIUXb6eQHIyzp1xvvPgoavCDUWwFKawTRDAUQBfSOWgrlaohpOarQHMqJXciJeLXZlCoy6cLJ8tbwdES/lyBkL+7qxh+p5W245RViLdb0YutQZZUxOKdm3Cn18NYPy7LyK37iya/Bk4Om0eLn3rB9hUeDcmT+qPGdKkbjfBM/XObpvSyoiAgjC35yghn3M8kOsBYmU+eGEqrQQOfGheMy5eZhiGNeraEiJZdBncL0Xn9Kj1Xq09wqTs3A0DgbP1QP9s2l4tGGdZYFWII5ZypM4wYnEPQHHmbaT6nWr3nLKDbaWtdGbDgY3KES6b6aa6toKfecAMoCR67dpKZSzINZU5PdpScmHJSoUjFF7QfPLYbc73SxbBACjbVF3vfLxY72BDC/BQhP6c5gM+NdHMzO8+Y1KVm0PA+nfP4jNHX8If/2sNbjq4EZmhZlT36I+Xpj6ANyYVY/C9czCgbyY+N5hqlX4bIHofvzdyo16A2pm4yQKvO2TOowZojHz+usxR6/wYMtVXBShb5sT6cAP5eqrZPi8Q4cFDx8MNRXBEewzEg95w0tHXOFtQXk2LWslxa7GrxaCKGLqAlVK2dLyZDZPFIuTv6tTjeCxVfYBHhptZLnYiDACfGV6LorfWAYEA8OqrQG0tbsnugfVjb8drhUvw1sTFeODmXvjzbqDlIJB+yKQeqsYY71uAVPNk0Ql1AVFroYKSqlP5WTMSHnTIRsmIN/PBju3XIo2GlxWYfcLkni2ydH66H/jSNGrcyTVY4/qZC7Z6fDlKDFDfqmUTrZmjXVV0zU7XmQXpBoABPYDTtfp6KT5fJ8qO7nxlZy/dZ3XGdRSg+pb4DAG7e9BZhRBkhzNWI2Xeno2sR/Ld1RmlsrZCzV7pal/cIhEqo/y+B0P0TqyYCaSfBFbe2rnudVcHZ/vlIA3f9/Jqq9OwrYLuB0OuueJ77FOy5wUODdcZ8ntc3WDOJTzXZ6bRc7PrDDD4/DEsLAtg4Z4Arj/2DnzhMHx9R+Avt34Vr08qxs4RNyFo+OEzgJ8OokySXJO69QQ1Z69tps9l7KuGI37+tpWJ0FoLlev8jKvvKuDMDkn0+fbqpzx46HywdbAMw1ji9EUhxJrkD+fqhs5wspOX5kldri3iSVql2bBEeCulLCJRIk/IslhEut8q8sDOE1MaivKp8SH3VTpVC+TWV2P2npewsCyA2w69CTQ1Abm5wKc+BRQXwz93Li5+nIX1JTRerklioY5nPqQFUF2oeOGQxyM7BzpKidzMkuuoZOjEGnSI14iXC7k5uqvj1S8YFW2EF+Xb110xv7+0Eth71nrMgv56SgrTYK4fCOw4Q+NhSV/APe3L6RroHNrlhfoG07yP7PSIhHTk2XVzH1QHL5UCDG1x3FSH040EvyWLINwbWamqrVCDJW3JXiViQMoU3zBoTth2Cvj9OM+5agtUxx+witr84xTqNchBGrmGdOUSui/yNLpgJIkEybV6YRHdJNfpfVKZFv/U3/p3IQRGnixHzfcD+ObGACZU7AIAHBhYiDNffxQXbl+Ce/ZPQkvYaK215f58K7bQnLhqr3Wf71UAC0dFXx9mDeiwsswqnAToA5JOYhOq08VIllPUFhq0Bw8eUgOnDNZdDn8TADwHK4lQG8LyZzpDUp7UfYIWSK45spuk5QUyDIq86fpW8UQtN8fVLbgAULb9ONb/ci3u3h3AE4ffgl+E0TR4GHxf/jJQXAzccgvgNyWQZMU5cL1QhDKx8aj0N2mh0o1HVt6TVZdWLrHWQlXXA68fib4WBqLFGlSo+1bl5XVgB0+lM8l0kae3071QnRCG3YItKzHyOcjZhdZrJUVmBYAPT5s/y+fvxnB2cmRkyWZ2YH1GdNZG3cfvxtK11Em78/axjIRkRn7dnq8bqA6nm5ohOfJsGKmLPLt1yNRIeKLZK92+Yp1baaVp9MtoDsV+Xz3YQ3X8n99L1DZZ1KZXpmmgy0Eaue6WxYN8IPEbwJrtTPOZlGg1u6V7n+T3mPvuGeEwphx/HwvLAli0Zw2GnzuMsGFgx/Cb8LO7f4UNk4oxY/YoPDkHqKwEcIC+pzY+53dPpV/nZdNzmJlGjqFhAA9PdVZuVaXZfUZ0QDIRJynZTlFnpkF78HA1wtbBEkL8Q3sO5GqGG0dKNiRZNALCFBCQjf+nt5s0G1m4onWBNEyVOCdKGDtgrQtuWGBM5T60PBZA0aa/oOfHH6MQwIH8Ajy94IfI+7tijJ5zHbadMmgsfut+VYPrtqHkAHFmys5RlMfDztPBc9a+Ic2KAAIbFTo4ZW840iv302oMRveQsrturPik0plkB4mNlDR/tJqWnVHKz4JK6VSP8dhWq6ERBtXKGVLNgQA52Kohof5uUX8Mmj1cAPN5lRESdF/sjCiEiCJoJ+3u1sG51GTWQKj1h21BWx03NYPqxuCSjawRFzreQEqm0RfvvuTnTYWahfbgDixkJNOKW8LAuXp7OrpcNyQ/w6rYjZztBKjGadlEcy2K9T7x+5Le1IybD5dg1qtP41PvL0fepUo0+9Px3pg5+I8538HrBffgbK8B8EXGwE6/TCcVoHlOFemYPhjYdIyenzQf0bGdnkvdHKQ2Y354anRAMtH3xXOKPHi4cuFG5AKGYdwBYCKoDxYAQAjxeKoGdbXByZHSGdsHz5kGB/8vy8TafU9eIO36lcgoygeK8sLYv247Rry8BvN2BzCi+mMAwMUJE7Dzm/+Cb2ffiyP9xlCWZzKwPBCbj85OUkGuKSHPqomxskRcgK+KR/iU6L+6+HN5gF/pwyVD2wwzAl7I3TScrT1on5mSs4jNIX1NGRsQdo6X3wcM6R19bDWDwsjvAfTtTuIWAuTclRyLztDJPWdUWpAA9XD54LRZv6dKJAOmAiJDfRaz02M7kU4OjkrXCYWdm23Gg0TrGGTHVM6guqnBAkwjS6ewmErYZQuTafTFsy+VhuahbVCz3jL6Zds7BnZOg+6zdD9RzllZ8OB563to+z7V16Po3fV4d1MA3Te8gm71FxHMysL+G+7EL0cUY+O1t6Muu08rjVwXVJLp4wBw11ggO4N+lt+95+/TMzV0z6Wdaimg71PotC8PHjxc3YjpYBmG8QyA7gBmA/gjgPsAfJDicV1VsDPs7BY6lbKgGrWxFki1gWOUYdrSAmzZQiIVa9di/OnTuDYtDSeLZuP4t76JYZ+7B+/s/Qjf/GhWq3gDO0c6SXd1HNzLi7n/vTLdR7hlaqTfZ9YQqE6Tek3tnDdd3zGN39DK8U+UK8/jYWOEaxXkmjLAGj2V6VkqVVLnVKjHYFTUApV1ZKQIUCT3zaOmg9QUpGeotRYokq0a0tvMekIaK2CltcnZBbWWQX0W2fnUPZ9uHBz12W918pJAFYwVjdY5JLqI95NzEh9De0FHr+1oI7HcQWggzWf/Nw96yEEdWX9CFqBxDK7lO3/G78tT24C3T0RnpKPep8xPgP/3Mq0rGzYAjY3onXMNXiwoxvrCYhTNysT1cxbiMwBGadYpNWNflE89Bp8ppbl57UHqFZWIA6Q2glfnoOWF0fv14MGDBye4yWDdLISYZBjGHiHEY4Zh/AZe/VVS4WTY6RYHlbIgG7W6Wi4V5dWaTEHvBlr0AgHglVeAmhqge3dg0SKguBjGHXdgaE5O6z7qd30U1ZhWFddQOfgrZpKB3FoUHRG7eP4+d8asW6cp1jWVj2HXd6xVUldE12ABsbN/KuTxsAKkWlMWK4PDtD3usaVuIx9j1xlr/Rk7ImkR6f03pL8JEL3ng9Omc/b2CaIwci8rjiKzcSbX7x08Zx/d5XHxGEsORn+mu0Z211Z99tPa6Pg6jVVGvDRe+XudrfC8tJKaffMz0RxMXh1bsiGr1XmID3LAJd5gllsU5ZPj88FpfTuLolAFit5fC3wvQEG7UAgYPBh46CGguBh/yLoVv96ehpAAJqeXWGjegH3DX4aq/qcGG91Atw7EYlJ48ODBQyy4cbAuR/5vMAxjIIDzADzp9iQjHpoBLyCqURtLlEDtjdWrrgYL9r+C5evWAFs2AJcvAzk5wF13AUuWAPPnk5OlAavBOWUbZEeOm+HKUsAA/a4zSnXnUZRvOml2xryMWNdUNY51fcfcZCzicbLkbeWaMiDagdTROFWqoNr/TFZULDlurb0AyBnJzbaKUxgwi9zlaHRI6mWVkxVtdMj/Jyu6G+ueqc9+LAMsWbBzpFqlrjVOnt2z0tFO1+r9VnpnKsU14sHS8RR04OvFTcI58+khPvB8yfPun3enJlOpUr8zPz4A/DlAwbrt2+nD8eOB736XxI+Kilo7ot9YCaTvgK3IS6z5wCnY6Ba6dcAuOOnBgwcPbuHGwXrFMIw+AH4FYAfIPv6PlI7qKkS8RpeOsuAUTee/9b1wGovL1uKzRwIYubsE/lAQGDQI+PznafGbORNIT495/O7p0dkGVVwDsGaEmE9vwBTpyEgzF9WVZWQ4d0uzV9JjysgHp63qc4lAR0nT0WDcXmO3kOkoXFPGdL9YNE4dVfCFfZSVKjluVXrkeqBz9cBm6W9Lx1P9m0wR43NXo9FOdUSJOAoNLfFnQoEmogAAIABJREFU/1Soz357OClyzyB+VmTVO6bJymPRPStA6iTmE8W8ER0zBl0LilVL9c9UiedgJQQ7QZmkQgh0312KIb8LYN7uAMZURTyt668HnnyS1pVrr9V+Vc5aJyLyYhdsjAeJ1l568ODBgxNsHSzDMF4FsBLAb4UQ9QBWG4bxCoAsIcTF9hrg1QA1u6Qqy7mF7ULx0Ue4eWUANwcCmHL8fQBA46hx8H/n27T4TZtGFmKckJ2RlWXA64fNZpMqlexSE9EBYSN7LjeEBIB0n7kfNmaf2iapjLVDzY0ObV2M5XstO53hIDlDT84xnVU7R06lCoZCihy9Upy9rQKYNdx6veV6vsWj9TRDp2vitoZHNqIB4OgF4Df7E3MuYjl0qcoM2TlSfI9kmqwM3bOSKon5eKBmir44Lbn7d3MfnLLUHExpqyPuIYXOQzAIvPWWWad78iTG+Px4f9Rt+OuM/4MRD96LB++MPpju2eB7nqjIS1vroxJZBzx48OAhFpwyWH8AsAzA7wzD2AzgbwBe85yr5EM2ukI2ynJu0LpQnBSYXbMTE/49AKxZA+zbhykA9gwpwm/u+CnGPbQEd97ThgY3ClTnaGGkCaU89mWrySHwGWSgqguiKl4wMZca8rYWGcsS50bqa26cto9nMZazVTWNROlrFeqAtRfYC/tMxzqWYSQraKk9rmSndNlq04iVm962JRtYWqlI5EecQ5VaqWbhlo4H8oUppBGPcxGLmpnK5sMWsQDJkYp1j9RnBTB7hyHccdFyp0xRW+H2Pjg5mrp9eEgMSXUeLl8G3niDnKqXXwbOnweysoCFC3H0kSewrPFOVHfrS/esKPrrqXxHdceK55zjXQc8ePDgIRZsHSwhxIsAXjQMoxuAuwH8PYBnDMN4DcDfhBBvtNMYr3ioxrKsLOd60g+FgLffRlEggKK1a4HjxwGfD7XTZ2LtF76IZwbfi4qcofAbwCMDkzt+1Tm6HLSOe/V+sxYoJKg2S10AVS79pyeaTpicKfDBKtfbEfUsbhdju95XsoE9cygJTsg9y2QqoN25qVRBbgjN2U8AeGyLed2bQ2aNxLYKq6Ona2jsZARtq7A2gzUM6xgQORc5Q4cQURV5V2HEJ1wQK/OTysxQvCqfMuSMjJylXlYQf5Y6mc96qgxKt/fByTnV7WNi8od61aBN9/riReDVV8mpWrcOqK8HevcG7ryT2A+LFgHZ2RgB4N9jPJ/tlb1tT0fOgwcPHuwQswZLCHEZwHMAnjMMYxKA/wdytvwpHttVA9VYDkboYzEN0MZG4M03afF76SXg3DkgMxNYsABYsQK7rr8Ln97Sz2LgpyJqHm+h8bn66AXQiUuvGmOyc9VRC6kbY1fX+0oWjuD7sFXqBWZR4FIMI13NCgBU1wNV9dTkc3mhmblSBS6q6xEldKJmUtwYQdMHEy2wOUTZxLnDTdn3cOSYAiT84PPRL4ZhzbRxs2u3cJvRS0UdhZ0jFY/DY7muYbr/8TpXXcFodHsfnJxT3T48kYt2xJkzwIsv0rqyaRO17RgwAPjsZ8mpmjULyMiI+losR669ap06Aw3XgwcPHmI6WIZh5AH4FIgumA/gBQD/kOJxXXXgxUkWH3hsq4a6dekS8NprtPi99hpQVwf06gXccQctfosXAz16RNUs6Ro1JguxCo3Vmo9+2foF0I5Lb2eMbaswz0+040Lq1tiVZZJVmXN5ezdy8nKmSqZNyY7U/nP0vMgZQ0aaj5yw1ho2xdHj47oxgnTUty0n0KpsCJjjfHAy1d6FwyTCMTkP8CdA8XSb0WNqIgtKJOt5kJ85RjwOT1uNy65iNMZDSbMzyHX78EQuUozDh2lNCQSA994jZYxRo4BvfIPWlRtvTKhOV0Z71Tp5ohUePHjoDHASuXgIwAMAxoH6Xv2zEOKd9hrY1Qqt6pOvijJUgQCwcSPQ3Azk5QHLl9PiN2eOJaIYRU0zgAxNo0Y7JEJFcio0Vms+gAhdLY4FUGeM5WRJ2SFhzfilkjro1thVjX6nnl2xxAC4dxhgHhMwG//yz7IDwBidA5y4BJRVWTOZqqPH18xNHxh1zHby9tsqzOc5FKbr8MhN7u+LXdbObkxAajI9qlO9dHx8Dk9bjcuuZDQmg34oUyuf3g6MaEnO2DxEIASwe7fpVJWV0efXXQf85Ce0rhQUtMqpJwPtReduL0fOgwcPHpzglMG6GcAvALwphAg7bOchiWBDavDZo1hcFsDn/icAbH+HFsSRI4GvfY0Wv+nTAb+epSlT09SapVhINhXJrvGxapAnohhW00jnx+fJlLNU06mcjN3SSqC6wexN1VZjk+8lO1eyiAUiP3O2Sv5czhjeOBg4Wu6cyWzrNVPPU/5Z7e31GZc9ZhIZU6qymqpTDcTv8LTlWbgajUb5/j+Sb75THhJEKETZKXaqjh4lB2rGDOC3vwXuvRcYkZoWl+1NcU1VjaEHDx48uIWTyIVHA2xPCAGUlaEoEMDO5wPovm83fT5pEvDjH5NTNWmSq4iiXc2SG7jNzrjpZ+S0qOqK/+NdeLkWSK1fSjWdyqkmZ/ka4Gu5wKNrkmNEyPdSJ+HPva6A6M95fAfPmc1w7TKZ8jUTIaKXJoNOql6reGppErmPl5rss5ptgfpOyS0I2svh6SxGY3tlIizPpOi8tMhOjaYmqqMKBKiu6uxZYjvMmwf84AfA3XcD/funfBhdheLqwYMHD8lCzBosDylEOAxs22ZGFA8fBgwD3W++Gfj1r8mpGjky7t22JdrthopUWumun5GbRbUtC6/decZDp0rUWNQZu3K2KVlNPd3UHtnRE9mBdWqGy2itFwuRY/L2CZJvT4aTKI8xnlqaeGlxpZWRXmsRGIhPSMMJdvfhajMS2zMTId9/w+jctMhOhdpaUvzjOt1Ll4AePYDbb6c15fbbqW63HdGVKK4ePHjwkAx4DlZ7o7kZ2LzZjCieOQOkpwNz5wL//M8UURwwwPKVRJyARKPdTgY9j+NULTBQtE2KOZ5tYo3XSfAg0QxbIuBzUWl8qUSsZ8Ouh5MKvmZPbSPnylID2EFORLyBAlU+3u+Lvgdtyb50lgxSR6K9MxHccmBEyLv2jjh3zqzTfeMNylzl5gL3309O1dy51LOqg3A1Ulw9ePBwdcNJ5OIapy8KIT5J/nCuUNTVAevX0+L36qvUWyQ72xpR7N1b+9WOkGfWGZJqH5+vu1CDc7OoFuVTVoUVCJN1bm6M4bYaizoBhpVLgCM7gRXj3SnZxTL4ne6/+jedOEU8DmxRPtECPzitl43vCMTj1Kjy8U/MSm6d2ZWGRJzN9spEqPdqxtjUHKdL48QJk/3w1lsUXRg2DPjyl2ldueUW2zrdjoAXoPDgwcPVBKcMVimI7WQAGAqgJvJzHwAnAKSmGvZKwfnz1O0+EABef516VvXtCyxZQovfvHlAt262X5ezRamMGLs1stQ+Pm7V4ORFVXcsprC1hMiwj5KlTxFKK+na6npBqdvZZfN0xnpRPlBVDnxza2xD3o2DtHq/pCCo3H9LjUqQ5P2FiB5PPJHjrhxpjjX2eBzqjmhg3Z6I5WzanX97PR/qvar3VATp5d6/H1izhtaVHTvo84kTgR/+kNaV665LqvKfBw8ePHhIDE4iFyMAwDCMZwC8JIR4LfL7YgDz2md4XQwnTwJr19Lit3UrqTYNGQI8/DAtfjNmAGmxWZlqtihRJyCe48SK6KuR63jU4JyO1RHFz+r1XVYQLVnuNGbA2fGpb4l9TtynjPcR1jhIAKkBsoKgSnlTa1TC4YjAg3LMeCPHXTnS7DR2t9mXqyHT5fTexTr/9ng+1HuVnZ7a43VahMPA9u1mpuqjj+jz6dOBX/6S1pUxYzp2jB48ePDgIQpuarCuF0J8iX8RQqwzDOOJFI6pa2H/fnPx+/BD+mzCBOB736PFb+rUmBFF1UFSs0W6hrDydxM1BuNxbtTIdTxqcE7HaivlKBHnUr2+g3rGJ9JRWuns+GSnO5+TrreVAPWJkh02RD4DKHU8a5iVdijfk5wsygQ2R5yttqjnqdf0SsnmuM2+XA2KZ07vXWc4f75XrJDJ0vhXBVpagC1baE1ZuxY4fZoCc7NnU+Pfe+4BBg7s6FF68ODBgwcHuHGwzhmG8SiAv4Lsv78DcD6lo+rMEIIcKXaqDhygz2+4Afj5z8mpGjfO9e50DpJOEjrZxqBbipyMRNXgAHuDri2UI7fOpeoguHXq7LbbVmF1fO6fYD1u93R3VDUhfWaA7gVnsOQ+VyJEf998DNh4NJoCKO//RyUU9H5sa2J0Sx1t8TGXdMeu4IS5yb50FsWzeK9pPNs7vXed5fwBcrBaQsCg/Cu8D1ZDA1HJ16wBXnkFqKkhCvmiRUQrv+MOICeno0fpwYMHDx5cwo2D9QCAFQACIJtwa+SzNsMwjEUAfg/AD+CPQohfKH83In+/HUADgAeFEDuScey4EAwS5Y8jihUVVDw8axbw1a9SRHFwYlaIzkH6yvXunY7pg8kwD4f0qmk6uKXI2X13WwUwwqEmQmfoORl0dqIaycg02Dlhbq6v3XY6B1j33VhUNRE0m/9m2NRgrZhJTlMoHDlP2J9rTSM5aG1RAFSv6bpDsa/xyjLTsctI6/qUOrfPRyqdyngz04lksu2e0WTWWbXlGl3xfbBqasiZCgRIBOnyZXKi7rqLAnULFgDdu3f0KD148ODBQwKI6WBF1AK/bhhGDyFEXbIObBiGH8DTAOYDqACw3TCMl4QQ+6TNFgMYE/l3I4D/L/J/yuFraiIZ9UCAxCo++YRkbhctAn72M+DOO4FrHIUWXcEpsxPLoGJqWLzQUeQA942Dm4PAI/lkWC8v1G/j1Fw4Ftwai24i7Xa1UrHENxi6MbfVAFWpfapTJYOdJvatnCTgk5F5UPexeDSJjzjRHX9UAgQjGb1mjRMmX9+uAjfvXyrrtOLNTCeSyY73uY8X8Vwj3ViuxD5YGefOAf/+77SulJRQ8G7QIODznyenauZMatvhwYMHDx66NGI6WIZh3AzgjwB6ABhqGMZkAF8UQvyfNh77BgCHhBBHIsdZBeAeALKDdQ+AvwghBIBthmH0MQwjXwhR2cZjO+O//xu3PPwwKf/16UPOVHExsHAhyasnEYkY67LhApiZjVDYnWGlE6xwYwhtqyDnKgwy+H9UEk1DS0b9htt9xLp2sWqleJtEDOW2GqBuvy/fK7+P6Ih22cZkZB50+xjXz5nuKPee8imGsHp9f9cJ5bbbXMeXgjqleJ3leLdvDyEPt9fISZGTn8URF66A7NWnP42bn3+efh47FnjkEaL/TZtGXcA9ePDgwcMVA4N8F4cNDON9APeBlASnRD4rF0IUtOnAhnEfgEVCiH+M/P5ZADcKIb4qbfMKgF8IId6O/L4RwHeFEB9q9vcwgIcBIC8vr2jVqlUJj63HRx+h39q1uDhnDi5cdx2EC+W/9kR1A1BVZ63hASjKO7IP1QDFQkMLqd1lp9P/vD8DQF4PIFfDTGloAY7U0HZ56XWoaumBAcq2DS3A0QuUdTEMYITL8ajHaes+gOjr1CsTGNbbeZseGUBedmLHY9TV1aFHjx6J70AB3yu/QcZqdrrz+OR76/Y8EvkOf+/oBSAs6NkZ2BO4Ruo+oF7fgRl16Jbdo03XN5lI9FmTv4eIqEhOVnKfm3jvSTzby/fF6Z1vC9xeWzdjSdY7NXv27FIhxDQ32yZzTQGAQatXI3ThAi7NnYuGYcM8OXUFyZ43ryR418Ye3rVxhnd97JHqdcWV5yCEOGlYF4NkaDrpVpcon8HFNvShEH/A/9/e/Qfbcdb3Hf98r2SNsGViEcviYgF2UwwSEFNf2QiozRUQCJ5pqRFKXdMkZQBDYruhNTNQcOowdKZkGpdMizrgOozpH7aKK7mQ4uBAyMVQEMgXsLEtnDjGMUK32CYyWPYo1r332z/2LHfv8dk9e87ZH8/uvl8zmnPPuXvPefbZ1dnnu8/zfB/peknavn27z87Ojl+y2VnNnXOOJnqPEs0vSNf0ZaEzRcP1rnzd+O/3izvIF6bfLY7n27xv85w+8ejswG2LmJtS1Ht8aF80bE2K5jntveCZw9eu2R9ts+zSlCafRzQ3N1f4udM/by6tJ2t+QXrPiD0T/X8zaC7YsL/P6kVM1u/7p+f0iR/PBjNPa89B6bpDUeC6xqSrz4rmQOYxvxANQb3lvqj3eNKeoDLOmzSj/J/P815Zxz/PXLZhZamybmKFXlOk6LpSw340BXWTjrpJR91ko37SlV03eQKsH/WGCbqZrZP0ryUdGvI3eRyW9PzE8y2SjoyxTeckUxgnG3eDki2M8n55AprLXh4NGXvwu+mNsiLmbxT1Hru3RUFhnAa9f5hSvO9/fED62sPR8MdB84jqlpxLtrQU7dO+Q6sb9PHaWvEwzrxD11YlE1gavGBxlqxjlazfrz8cvTZuAo4yTDJvbWZ6JaNk01K6F5XIoog1s/rLIg2fDwoAQMjyBFjvVZTJ70xFAc+fS5p0/pUkHZT0IjM7W9KPJV0q6bK+bT4v6cre/KxXSvpZ6fOvGiJuuOzaWky2r1ECmpnpaB2sJjR+dm1dSfWc1oCemY6SOdzRCwCWfbJ1pIrWP5dMin5OBirJBCTLinri8gYMeRcsHtfMtPS+HVGyjGSSjhBSu08aaISU0nxURdzEKGouWlyWQcsEHD2enbUUAIDQ5AmwXuzub0++YGavkfR/J/lgd180sysl3a4oTfun3f1eM3tv7/eflHSbohTtDyhK0/6OST6zjSZpJIXQwC1b3gb00eNRUBIHJ0ePV1jIIfrX3ZrqDZztX5vrxFJUfpP08s3Sta/Nn7Cjf8Hi/oB00nMl/oy451MqP8nCKGUb97OL6glqqqIDzFW9qYsrvalXT7d8HSwAQKvkCbD+q6Tzcrw2Mne/TVEQlXztk4mfXdIVk35OG03a4K0ii1go8jSgd2yJ5l5l9XQVZdRj19+IHTRHaseW3pz5Xkr3Q4+N9nnJOurPGljUuZLs+dxzsNwsfNLKHCkp/zpv4yiiJ6ipig4wN66PzuMpRTcS4t7UVq6DBQBordQAy8xeJenVkjaZ2b9N/OrZinqcUJNJGrxxY/vHT5TfwG2Sqnoixl0QNk/ZkglBF3vHVJIu3bfyeXt3jT6nqoyU5IN6PgYFguPeSJhfiPY7TnByy3359r1oXeklLmLf5hei3tPl5Shr+bv+kXTjXdG515Z1sAAA3ZDVg7VO0dpXayWdmnj954rStqMmwxq8aY26/kx0a3pj4po2d6RoyfrKm0FuXOMGK2mN2GTAnAywpnprfu07tBJkPL0UPQ9hntGgxAb9gWf/a6NkN4zrOVZHYo2m9BKHEgSuGubq0bIKrVoHCwDQGakBlrt/VdJXzexGd//bCsuEIbIavFmNulWN+2Xp0pdJZ55af8OqTlU3gosMVvoD5rVrpMXlaGjVR2ej/dhXQL7Psnr3kkHjoCGD0vjZDeN6joPLrLouK8AoezHiPIbtW0hB4KD/G/E5MjdXT5kAABhHnjlYN5jZbnd/XJLMbKOkve7+pnKLhjRZDd6sRl1/A6bMeSlNUUcjOE6nP2n95wmYd22NhsedSBzzcZQ9zygt8Bw3u+HMdDQkcNgcrDIDjLozDObZtxCCwFh/spU4yO76dxQAoHnyBFinx8GVJLn7UTM7o8QyIYe0Bm9Wo66NGc8m7X2oshHc3+AdN9iJDQuY47r5yGtHWzi4Dmnn5rDshsPeM+9QwjICjLr/v+XZt7qDwH5x+frnDQIA0CR5AqxlM3uBuz8sSWb2Qq1ekgcBGdaoa1PGsyJ6HwbNBSprkdOiG/Nx2eNemvsfy57TFPpxH3RuZmU3LELZAUZyeG7yeRXy7FvdQeAgg+YNvnGq3jIBADCKPAHWhyV93cy+2nt+kaTLyysSJlVEEDUso1sIyl7ktOigpKzG/L5DqxcYXrc26s0at26KmJNUxnuUcXOg7ACjzjlOefct9Jsujz6p1WmWAAAI3NAAy92/aGbnSdqhaA3Tf+Pujw35MzTYoEahtPq1j59Tbxmlchc5LWM+ShmN+WTmNSl6jLPnjVM3RQQEobzHsPfvD97ybjuKUc6pp04U33saevAkPbN+X7Zp9e+/8pB0yUtqKRoAAGPJWgfrJe7+g15wJUlHeo8v6A0Z/E75xUMdBjUKpdWvPXmizhJGig5YqpiPUnSDNy6zxz1YtjIfa9fW0eumiCBzkveoYp22UYK3/m1HSRUv5T+n5hekHz4uXXeoOUM6izDoWBw9vnqbpeUwvm8AAMgrqwfraknvlnTdgN+5pNeVUiLUblhGt5PWSKecVGcJVxQZsFQ5H6so/ZnX+hv/dax5lfUeWb00edZpK2Lo4SgBYHLbUVPFS/lvAhw4HL1vCNn8qjToWOzYIq3rS7EfyvcNAAB5ZK2D9e7e487qitNtoSz4OSyj244t0hP311e+MlU1H6tIZQaZ485pG3j+fF/6u6PSdfdF88T663RY2vmijskoQWRy21FTxSfrY9h2O7ZIBx6S1lgY2fyqkrb2VX+K/bZ+3wAA2ilriOBbs/7Q3fcXX5zuCq1BPyyj21zBDZ5QgstYSOsDSaPVT2h1KUVl+v056X2bowDl6QF1OiztfJFJTfIGkf09hKOmih+lTD85Tbr6rLCOW9nSjkX/90/R3zcAAJQpa4jgP+k9niHp1ZK+0nu+U9KcJAKsAoXWoK9SaMGlFNb6QHnqJw6q+oOAUeuyrAQVBw5HvT+xKXtmnQ4LfIo8JqP0+pWdKj528knSFecX+56xEIPuWBMScQAAMIqsIYLvkCQz+z+Strn7Qu/5tKQ91RSvO0Jq0FctxOAypPWBhtVPMqCRpGXvLVQ3Rl2WleRix5ZoWKAkrZ2SPjo7+H2zGtshHJMmBQNFBd0AAGA0edbBOisOrnp+IimAJN3tEkLjsS6hBpehNKaH1c+qgCZhzdTodVlWkov4/H7wu9JnLxy/XkM5JqFLBt3jzh0b9J5d/H4CAGBUeQKsOTO7XdLNim6MXyrpL0stVUdV2Xisq7E06HNDDS5DaVDmHTq3vNjruVK0YN3ubaOXu8wkFzPTUbKCUI5vXao4r5JB95SkqSnJfPygOcRhvAAAhCrPQsNXmtklki7qvXS9u99abrFQproaS1mfW0ZwOUlDNrQGZZ6hc/sOSbfcJy0uRQ3q/gVbi/isfml1TE/TYFWdV/29iKOu39UvxGG8AACEKk8PliR9R9IT7v5lMzvZzE519yfKLBjKk2dOTxl32KtspE3akG1agzIOaF62KcrWt7wczbt58enp5c5znLO2KSpYCKWnsApVnVdF9wqHOowXAIAQDQ2wzOzdki6X9BxJvyLpTEmflPT6couGsmQ1lvoXe9297Znpssv43KJN2pBtaoPy6PFowdplRfuftt9pwVEy2JGyA6gigoVRgrQ2BGJVnldF9iKGOowXAIAQ5enBukLSBZK+JUnu/tdmdkappUKpshpLyUbz0lK0OOy+Q8UMZaqykTZpQ7apDcq8+z0oOJJWBzu7tmYHUEUEC1lB2ijBXlM09bySqhn22YYgGgCAPAHW37v702YmSTKztVqZS4+GSmss9SdMcGX3hBT1uaPI0wgrKllD0xp5efd7UHDUH+xI2QFUEXWcFqT192wNC/aaJM951cVAI7R5jwAAjCtPgPVVM/uQpGeZ2a9J+l1Jf1pusVCX/oQJS8thDZEbpRHWxACpCHn2Oy04SgY7u7ZG/7Ia+pPWcVo5Rg322qSrgUbT5j0CAJAmT4D1AUnvkvR9Se+RdJukG8osFOoVN5qHNa6HKeMuPI2w4vQHR1np1assh/TMnq08wV5bdPUc7z/mG9dLew62O5gGALRTZoBlZlOS7nb3l0n679UUCaGYpHci7134UYOwpiafaIpQev3qCPZCGZbX1XM8ecw3ro+yYMbfHx9naXsAQINkBljuvmxmd5nZC9z94aoKhebLcxd+nKFQTU4SgHRpC1BXdXzrHpY3vyA9+lT02OVzPD7mew6u/v548kTdJQMAIL88QwSnJd1rZt+W9GT8orv/09JKhcbLcxd+3KFQofSyjKKK3pFQemBGVXdwI9U7LC/e/6s2SdfsX9n/Jh3DovV/f5xyUt0lAgAgvzwB1kdKLwVaJW7oX3tRtC5TWoO/K0Oh8gQQkwZHIQQp4wphzlGd52K8/0Vn7Gyy/l68J+6vu0QAAOSXGmCZ2XpJ75X0DxUluPgTd1+sqmBoplGz/HVhKNSwAKKI4CiEIGVcIQTadZ6L8f6b2n2jYVTJXrw5AiwAQINk9WB9RtIJSV+T9GZJ2yT9XhWFQnON2tDvwlCoYQFEEcFRCEHKuEIJtOs6F+P9f/C70k0Xtv//AwAAbZcVYG1z95dLkpn9iaRvV1MkNFmTG/plGRZAFFFnwz4j9PlZXQi0s8xMR8PgulwHAAC0RVaA9Yu8Te6+aGYVFAdNF0pvRGiyAohx66w/aEr7jCbPzypb6IEnAABonqwA61wz+3nvZ5P0rN5zk+Tu/uzSS4fCVNmQ7HpvxDiy6mzQsRslaAphfla8D2cHlG6bwDNMBL0AgKZLDbDcfU2VBUF5aEg2V9qxGxQ0SYMbpnUP20zuw9XTK2s91e3AYenpRWlZkjcsMUhb8V0FAGiDPGna0XAh9GA0Vd1309OOXX/QtHF9esO07mGbyX1wD+f827g+Cq4kadmj56gX31UAgDYgwOqAunswmiqEu+lpx64/aBrWMK1z2GZyH8zCOf+OHpemtBJk3fNose9fd3DeRHxXAQDagACrA+ruwWiqEO6mZx27/qAp1IZpch/Ofjyc82/HFmntGunppej5LfdJu7YWU74QgvMm4rsKANAGBFgdQeKJ0YVyNz3PsctqmIbQkxLvw9xcPZ8/yMy0tHubdNP3JZe0tFxcEB1CcN7nf8k4AAAYHElEQVRUfFcBAJqOAAtIUebd9DKCnkENU3pSsu3aKu07tFI/RQXRoQTnAACgegRYQIYy7qYPC3qKDL7SelJC6NUKQVlB9KD3TavzEFPYAwCA8RFgARXLGj5WdI9T3JPivQQTG9fTq9WvrCFpyfdNq/NQU9gDAIDxTdVdAKBr4qBnjT1z+Fgy+DqRWN9qXDPT0rUXSVMmLS9LH7ljZUhcUZ+B4dKO66AU9lnmF6Q9B6PHNmnrfgEAuokeLCCnoobVZQ1LK2PuztHjUeN9WVFjXmJ+UNUGrVu252D0mDeFfVt7Htu6XwCA7iLAQuHaOL+n6EZg2rC0MuYE9Tfud22N/rXtGIUseVw3ro96EuNz6dqLoiB4WAr7tmYmbOt+AQC6iwALhWrr3egqG4GjzgkaFtCmBW1tOC5NEh/XPQdXn0tHj0tXnD88hX1TMxMOOz+H7df8gvToU8xPAwA0BwEWCpU3EGlaL1eojdu8AS1rC4Vj3HMp9LXOBslzfg7br8v2S1dtkq7Z354bNgCAdiPAQqHyNB6b2MtV5ppYk2B4VfNMci41ba2zvOdn2g2A+O9dK8lBQtk3AADSEGChUHkaj00NCkLsBcob0DZlseSuKPJcCvn/06Q9v/Hfm8LqOQYAIEstAZaZPUfS/5R0lqSHJP2Gux8dsN1Dkp6QtCRp0d23V1dKjGtY4zHU4XZNNCygLat3I+Rek64J+f/TpD2/8d8/+F3ppgs5xwAAzVBXD9YHJf2Fu3/MzD7Ye/6BlG13uvtj1RUNaZI9Fnm3qypTXpdlBbRl9W6E3GvSNaH/f5q0t25mWnri/vD2CwCANHUFWG+RNNv7+TOS5pQeYCEA/T0WHz8n33YkXahXWb0bIfeadBH/nwAACIe5e/Ufava4u5+WeH7U3TcO2O6Hko4qmuP8KXe/PuM9L5d0uSRt3rx5Zu/evROV8dixY9qwYcNE79Emjz4l/eRYdCBM0pnrj2njs59ZP/3bbd4gbTq54sLWLLRz56kT0pMnpFNOkk4+qd73Da1uQkLdpCuqbnbu3Dmfd6h50dcUiWOchbpJR92ko26yUT/pyr6ulBZgmdmXJT13wK8+LOkzOQOs57n7ETM7Q9KXJF3l7ncM++zt27f7nXfeOUHppbm5Oc3Ozk70Hm3yzB6sOV38a7NDt+vi3BzOnXTUTTrqJl1RdWNmuQOspCKuKRLHOAt1k466SUfdZKN+0pV9XSltiKC7vyGjMD8xs2l3XzCzaUmPpLzHkd7jI2Z2q6QLJA0NsFC8/nkeT9yfb7uuBVcAAADotqmaPvfzkn679/NvS/pc/wZmdoqZnRr/LOmNku6prIQtN78g7TkYPeY1My1dcf7woCnvdmUbZx8BAACASdSV5OJjkj5rZu+U9LCk3VI0JFDSDe5+saTNkm41s7icN7n7F2sqb6t0YRhfF/YxiTWpAAAAwlBLgOXuP5X0+gGvH5F0ce/nByWdW3HROqELKbYPHJaeXpSWJfliO/cxlieYJAADAACoRl09WKhR2Sm2R2nMl9Xw37g+Cq6k6HHj+uLeO8v8QpRJcX6hukAmGTAvL0r7Dq3+7K715qEd8q67BwBAaAiwOqjMRBSjNObLbPgfPR5NMFyWNGXR87LF+3PVJuma/dUFMju2SGumpKWlKD3+LfdJu7aufHYXeizRLnnX3QMAIER1JblAzcpKRJFszJ/oNeaL2HZUO7ZI69ZKa0xaV9FCuPH+uIrfnywz09LubdG6Y5K0tLz6s+MeyzU2vMcymRgklCQhoZQD1en/bnjyRN0lAgAgP3qwUKhRhh+WOVSxynTx8VCmjeuj/TCVM/Qyy66t0dDAEwPqMm9dJHsN1vRuvSwt1zOsMFmnH7mD4Y1d0//dcEqBC2QDAFA2AiwUapTApuwgaGa6/MZ4/1Cmay+STvqRdNOF1QYCw+oyT12smsu1FL3mUuXDCpN1aiYtL/fm0zG8sTPyrrsHAECICLBQuFECmyqCoDL1z286elx66cn17NOkdZnsNejvwRq3N26cJCbJOp1yaWpKMq++VxD1Sp7PcwRYAIAGIcACJjBomGMId9vHCWz6ew2kyXoXByUxyaO/Tq+9KApcSTEPAACagACrpVj3qBqDhuYVdbd93GM4SXbG/l6wSc6dQdkLX5pR5uS+VjV/DmGLz4uzSXIBAGgQAqwWYt2japUxzHGSYxhKWva8vXtp+8o5223J8+Lq6WrXlgMAYBKkaW+hMtOfoxqTHMNR0rKXKe6JuvpV2QEi5ysGSZ4X7pwXAIDmoAerhSZNf87wwvpNcgxDGmKXpyeqzHT9aK7keWHGeQEAaA4CrBaapIHN8MIwDEo4sedg/uPZpCF2IQWECEfyvDj7cc4LAEBzEGC11LgN7FDm72DlGHYh6G1SQIjqxOfF3FzdJQEAID/mYGGVUObvYAVzlAAAAJqDHiyswnCt8IQ2Rymeo7dxPetTAQAA9CPAwjMwXCssIQW98XDFpxelZUVd4OvWVjNsMU/ylaoStJAIBgAApCHAAhqgqKB30sAgHq643Hu+rJVhi2UHNMPmoVU1V60Lc+IAAMD4mIPVUvMLUda5+YV2fRbGFwcG130zehzneMXDFeMvjqkR5urNL0iPPjXe5+47JP39YvY8tDLmqg06t5kTBwAAstCD1UJV3mHnbn5zFJEhMjlccZQ5WPF5ctUm6Zr9o50n8wvSLfdJ3nu+ZmpwQFf0XLW0czu0OXEAACAsBFgtVGWqddK6N8eOLVFwsryUHqTkMc5wxfg8cY0+pPDAYWmpNybRJO3eNvhvi56rlnZuhzQnDgAAhIcAq4WqvMPO3XzkEZ8nptHPk/5zbNfW9G2LTNCSdW6TCAYAAKQhwGqhKu+wcze/euMmqoh7glzRY5W9jfF58uB3pZsuHO1z6zrHOLcBAMA4CLBaqso77NzNr84kc97q7m2cmZaeuH+8c6Wuc4xzGwAAjIoAC2iQSea8daFHhvWpAABA3QiwgAaZtBeqzT0yZLSsBkEsAADZCLCABulCL9S4DhyWnl6MFj/2RTJaloEgFgCA4QiwgIZpcy/UJDauj4IrKXrcuL7O0rQTyzIAADDcVN0FAIAiHD2+8oU2ZdFzFCseorrGWJYBAIA09GAByNSUOTc7tkjr1q4MX6PxXzyGqAIAMBwBFoBUTZpzQ+O/GgxRBQAgGwEWgFRNm3ND4x8AANSNAAtAqroXJwYAAGgaAiygZYqcM8WwOwAAgNEQYAEtUsacqSqH3TUloQYAAEAaAiygRZo2ZyqpSQk1AAAA0rAOFtAiTV6nKBkcnugFhwAAAE1DDxbQIk2eM0VCDQAA0AYEWEDLNDVVeZODQwAAgBgBFgBJYSSYaGpwCAAAECPAAjBRgokQAjMAAIBQEGABGDv7IJn/AAAAViOLIICxsw+S+Q8AAGA1erAAjJ1ggsx/AAAAqxFgAZA0XoIJMv8BAACsRoAFYCJk/gMAAFjBHCwAAAAAKAgBFhC4+QVpz8HoEQAAAGGrJcAys91mdq+ZLZvZ9oztft3M7jezB8zsg1WWEQhBnAb9um9GjwRZAAAAYaurB+seSW+VdEfaBma2RtIeSW+WtE3SvzCzbdUUDwgDadABAACapZYAy90Pufv9Qza7QNID7v6guz8taa+kt5RfOiAc465P1SUMoQQAACEJOYvgmZJ+lHh+WNIrayoLUAvSoGeLh1Ce6K3DddNbqSMAAFAvc/dy3tjsy5KeO+BXH3b3z/W2mZP0fne/c8Df75b0Jnd/V+/5b0q6wN2vSvm8yyVdLkmbN2+e2bt370TlP3bsmDZs2DDRe7QZ9ZOOuklXdN08+pT0k2OSSzJJmzdIm04u7O0rxXmTrqi62blz57y7p877TSr6miJxjLNQN+mom3TUTTbqJ13Z15XSerDc/Q0TvsVhSc9PPN8i6UjG510v6XpJ2r59u8/Ozk704XNzc5r0PdqsyfUzv1Buj1CT66ZsRdfN/IJ0TbIH68Lm9mBx3qSro26KvqZIHOMs1E066iYddZON+klXdt2EPETwoKQXmdnZkn4s6VJJl9VbJDQdQ8rahSGUAAAgNHWlab/EzA5LepWkL5jZ7b3Xn2dmt0mSuy9KulLS7ZIOSfqsu99bR3nRHmTla5+ZaemK8wmuAABAGGrpwXL3WyXdOuD1I5IuTjy/TdJtFRYNLRdn5dMSWfkAAABQvJCHCAKFY0gZAAAAykSA1SBlJ2foiplp6g8AAADlIMBqCJIzAAAAAOGrJckFRkdyBgAAACB8BFgNESdnWGMkZ0D45hekPQejRwAAgC5hiGBDkJwBTcFwVgAA0GUEWA1CcgY0QXI4q3rDWTlvAQBAVxBgASgUa40BAIAuI8ACUCiGswIAgC4jwAJK1sX1yxjOCgAAuooACyhREQkfuhigAQAANBUBFmrThcBh0oQPZOQDAABoFgIs1KIrgcOkCR/IyAcAANAsBFioRVcCh0kTPpCRDwAAoFkIsFCLLgUOkyR8ICMfAABAsxBgoRYEDvmRkQ8AAKA5CLBQGwIHAAAAtM1U3QUAkN/8grTnYPQIAACA8NCDBTREVzIvAgAANBk9WEBDJDMvnuhlXgQAAEBY6MECGqJLmRcBAACaigALaIg48+K+Q3WXBAAAAGkYIgg0zL5D0t57ovlYJLsAAAAICwEW0CDMwwIAAAgbQwSBBmEeFgAAQNgIsIAGiedhHTgcBVekaQcAAAgLARbQMDPTBFYAAAChYg4WAAAAABSEAAsAAAAACkKABQAAAAAFIcACAAAAgIIQYAEAAABAQQiwAAAAAKAgBFgAAAAAUBACLAAAAAAoCAEWAAAAABSEAAsAAAAACkKABQAAAAAFIcACAAAAgIIQYLXM/IK052D0CAAAAKBaa+suAIozvyBdtl86sSSdtEa66a3SzHTdpQIAAAC6gx6sFjlwOAquljx6PHC47hIBAAAA3UIPVovs2BL1XKnXg7VjS90lAgAAALqFAKtFZqajYYEHDkfBFcMDMcj8AucIAABAWQiwWmZmmkYz0jFPDwAAoFzMwQI6hHl6AAAA5aIHC+gQ5ukBAACUiwAL6BDm6QEAAJSrlgDLzHZL+gNJWyVd4O53pmz3kKQnJC1JWnT37VWVEWgr5ukBAACUp64erHskvVXSp3Jsu9PdHyu5PAAAAAAwsVoCLHc/JElmVsfHAwAAAEApzN3r+3CzOUnvzxgi+ENJRyW5pE+5+/UZ73W5pMslafPmzTN79+6dqGzHjh3Thg0bJnqPNqN+0lE36aibdNRNuqLqZufOnfN5h5oXfU2ROMZZqJt01E066iYb9ZOu7OtKaT1YZvZlSc8d8KsPu/vncr7Na9z9iJmdIelLZvYDd79j0Ia94Ot6Sdq+fbvPzs6OU+xfmJub06Tv0WbUTzrqJh11k466SVdH3RR9TZE4xlmom3TUTTrqJhv1k67suiktwHL3NxTwHkd6j4+Y2a2SLpA0MMACAAAAgLoFu9CwmZ1iZqfGP0t6o6LkGAAAAAAQpFoCLDO7xMwOS3qVpC+Y2e29159nZrf1Ntss6etmdpekb0v6grt/sY7yAgAAAEAedWURvFXSrQNePyLp4t7PD0o6t+KiAQAAAMDYgh0iCAAAAABNQ4AFAAAAAAUhwAIAAACAghBgAQAAAEBBCLAAAAAAoCDm7nWXoXBm9qikv53wbU6X9FgBxWkr6icddZOOuklH3aQrqm5e6O6bRv2jgq4pEsc4C3WTjrpJR91ko37SlXpdaWWAVQQzu9Pdt9ddjlBRP+mom3TUTTrqJl1b6qYt+1EG6iYddZOOuslG/aQru24YIggAAAAABSHAAgAAAICCEGClu77uAgSO+klH3aSjbtJRN+naUjdt2Y8yUDfpqJt01E026iddqXXDHCwAAAAAKAg9WAAAAABQEAIsAAAAAChI5wMsM/t1M7vfzB4wsw8O+L2Z2X/p/f5uMzuvjnLWIUfdvL1XJ3eb2TfM7Nw6ylmHYXWT2O58M1sys7dVWb465akbM5s1s++Z2b1m9tWqy1inHP+vfsnM/tTM7urVzzvqKGfVzOzTZvaImd2T8vvGfBdzXUnHdSUd15V0XFfScU1JV+t1xd07+0/SGkl/I+kfSFon6S5J2/q2uVjSn0kySTskfavucgdUN6+WtLH385upm4HbfUXSbZLeVne5Q6kbSadJuk/SC3rPz6i73IHVz4ck/WHv502S/k7SurrLXkHdXCTpPEn3pPy+Ed/FXFcmrhuuK1xXxjlvOnld4ZoytH5qu650vQfrAkkPuPuD7v60pL2S3tK3zVsk/Q+PHJB0mplNV13QGgytG3f/hrsf7T09IGlLxWWsS57zRpKukrRP0iNVFq5meermMkn73f1hSXJ36mc1l3SqmZmkDYouhovVFrN67n6Hon1N05TvYq4r6biupOO6ko7rSjquKRnqvK50PcA6U9KPEs8P914bdZs2GnW/36noLkAXDK0bMztT0iWSPllhuUKQ57w5R9JGM5szs3kz+63KSle/PPXzCUlbJR2R9H1Jv+fuy9UUL2hN+S7mupKO60o6rivpuK6k45oymdK+i9cW8SYNZgNe689bn2ebNsq932a2U9GF8B+XWqJw5KmbP5b0AXdfim4adUaeulkraUbS6yU9S9I3zeyAu/9V2YULQJ76eZOk70l6naRfkfQlM/uau/+87MIFrinfxVxX0nFdScd1JR3XlXRcUyZT2ndx1wOsw5Ken3i+RVGEP+o2bZRrv83sVyXdIOnN7v7TispWtzx1s13S3t5F8HRJF5vZorv/72qKWJu8/6cec/cnJT1pZndIOldS2y+EUr76eYekj3k0QPwBM/uhpJdI+nY1RQxWU76Lua6k47qSjutKOq4r6bimTKa07+KuDxE8KOlFZna2ma2TdKmkz/dt83lJv9XLNLJD0s/cfaHqgtZgaN2Y2Qsk7Zf0mx24S5Q0tG7c/Wx3P8vdz5L0vyT9bgcuglK+/1Ofk3Shma01s5MlvVLSoYrLWZc89fOworuwMrPNkl4s6cFKSxmmpnwXc11Jx3UlHdeVdFxX0nFNmUxp38Wd7sFy90Uzu1LS7YoysXza3e81s/f2fv9JRZl6Lpb0gKSnFN0JaL2cdfPvJf2ypP/Wu6O26O7b6ypzVXLWTSflqRt3P2RmX5R0t6RlSTe4+8AUqm2T89z5qKQbzez7ioYvfMDdH6ut0BUxs5slzUo63cwOS7pW0klSs76Lua6k47qSjutKOq4r6bimZKvzumJRjyEAAAAAYFJdHyIIAAAAAIUhwAIAAACAghBgAQAAAEBBCLAAAAAAoCAEWAAAAABQEAIsoAZm9stm9r3ev/9nZj9OPF9XwPv/gZn9x77XXmFmqeuC9P7m/ZN+NgCgelxXgHB0eh0soC7u/lNJr5CiC5CkY+7+R/HvzWytuy9O8BE3S/ozSf8u8dqlkm6a4D0BAIHiugKEgx4sIBBmdqOZ/Wcz+0tJf9h/58/M7jGzs3o//0sz+3bvzuSnzGxN8r3c/X5Jj5vZKxMv/4akvWb2bjM7aGZ3mdm+3qr3/WWZM7PtvZ9PN7OHej+vMbP/1Pv7u83sPQVXAwCgIFxXgHoQYAFhOUfSG9z96rQNzGyrpH8u6TXu/gpJS5LePmDTmxXdXZSZ7ZD0U3f/a0n73f18dz9X0iFJ7xyhfO+U9DN3P1/S+ZLebWZnj/D3AIBqcV0BKsYQQSAst7j70pBtXi9pRtJBM5OkZ0l6ZMB2eyV9w8yuVnRBvLn3+svM7D9IOk3SBkm3j1C+N0r6VTN7W+/5L0l6kaQfjvAeAIDqcF0BKkaABYTlycTPi1rdy7y+92iSPuPuyXHwz+DuP+oNwXitpF2SXtX71Y2S/pm732Vm/0rS7IA/T372+sTrJukqdx/l4gkAqA/XFaBiDBEEwvWQpPMkyczOkxQPmfgLSW8zszN6v3uOmb0w5T1ulvRxSX/j7od7r50qacHMTtLgISDxZ8/0fn5b4vXbJf1O729lZueY2Skj7hcAoB4PiesKUDoCLCBc+yQ9x8y+J+l3JP2VJLn7fZKukfTnZna3pC9Jmk55j1skvVTRsI7Y70v6Vu/vfpDyd3+k6IL3DUmnJ16/QdJ9kr5jZvdI+pToCQeApuC6AlTA3L3uMgAAAABAK9CDBQAAAAAFIcACAAAAgIIQYAEAAABAQQiwAAAAAKAgBFgAAAAAUBACLAAAAAAoCAEWAAAAABTk/wNN84OxFc2+kwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 864x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12,6), sharex=True, sharey=True)\n",
    "\n",
    "\n",
    "y_test_norm = []\n",
    "y_naive_norm = []\n",
    "y_phys_norm = []\n",
    "\n",
    "for m_tot, y_t, y_p, y_c in zip(x_test.mtotal, y_test, y_pred_naive, y_pred_phys):\n",
    "    y_test_norm.append( y_t / m_tot )\n",
    "    y_naive_norm.append( y_p / m_tot )\n",
    "    y_phys_norm.append( y_c / m_tot )\n",
    "    \n",
    "\n",
    "ax = axes[0]\n",
    "\n",
    "ax.set_title('Naive (single-target)')\n",
    "ax.set_xlabel('True Value')\n",
    "ax.set_ylabel('Predicted Value')\n",
    "\n",
    "ax.grid(True)\n",
    "\n",
    "ax.scatter(y_test_norm, y_naive_norm, marker='.', color='dodgerblue')\n",
    "\n",
    "ax.plot([0,1], [0,1], color='red')\n",
    "\n",
    "\n",
    "ax = axes[1]\n",
    "\n",
    "ax.set_title('Physics-guided (single-target)')\n",
    "ax.set_xlabel('True Value')\n",
    "\n",
    "ax.grid(True)\n",
    "\n",
    "ax.scatter(y_test_norm, y_phys_norm, marker='.', color='dodgerblue')\n",
    "\n",
    "ax.plot([0,1], [0,1], color='red')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig, ax = plt.subplots(1, 1, figsize=(8,8))\n",
    "\n",
    "#ax.set_title('Physics-guided single-target regressor')\n",
    "#ax.set_xlabel('True Values')\n",
    "#ax.set_ylabel('Predicted Values')\n",
    "\n",
    "#plt.plot(y_test, y_pred,ls=\"None\",marker='.')\n",
    "#plt.plot(x_test['mtotal'].values, y_pred, ls=\"None\",marker='.')\n",
    "#plt.plot(y_test, y_pred_naive,ls=\"None\",marker='.')\n",
    "#plt.plot(x_test['mtotal'].values, y_pred_naive, ls=\"None\",marker='.')\n",
    "\n",
    "#plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conservation laws with multi-target regressors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to predict all masses at the same time, and ensure consistency with the input.\n",
    "\n",
    "We want to set up a model which will predict the three mass quantities together, and guarantee conservation of mass.\n",
    "\n",
    "$$ \\mathcal{M}(\\vec{x}) = \\vec{m}, \\quad \\vec{m} = (m_{lr}, m_{slr}, m_{debris}) $$\n",
    "\n",
    "under the constraint that\n",
    "\n",
    "$$ m_{tot} = m_{lr} + m_{slr} + m_{debris} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "\n",
    "Note that we don't scale the features or targets in this case. We could do this, but it would require a bit of extra work within the loss function to apply the inverse scalers to the masses in order to compute the proper sum of the masses. In this case, we have decided to operate on the unscaled data in order to keep things simple and readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = ['lr_mass','slr_mass','debris_mass']\n",
    "\n",
    "features = ['mtotal', 'gamma', 'b_inf', 'v_inf',\n",
    "            'targ_core_fraction', 'targ_omega', 'targ_theta', 'targ_phi',\n",
    "            'proj_core_fraction', 'proj_omega', 'proj_theta', 'proj_phi'\n",
    "           ] + targets\n",
    "\n",
    "x_train = pd.read_csv('../datasets/train.csv', usecols=features)\n",
    "x_test  = pd.read_csv('../datasets/test.csv', usecols=features)\n",
    "\n",
    "x_train = x_train[x_train['lr_mass'] > 0]\n",
    "x_test  = x_test[x_test['lr_mass'] > 0]\n",
    "\n",
    "y_train = x_train[targets].copy()\n",
    "y_test  = x_test[targets].copy()\n",
    "\n",
    "x_train = x_train.drop(targets, axis=1)\n",
    "x_test  = x_test.drop(targets, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up naive multi-target regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting\n",
      "Train on 9085 samples, validate on 479 samples\n",
      "Epoch 1/500\n",
      "9085/9085 [==============================] - 0s 51us/sample - loss: 64.0107 - mse: 64.0106 - val_loss: 1.6792 - val_mse: 1.6792\n",
      "Epoch 2/500\n",
      "2112/9085 [=====>........................] - ETA: 0s - loss: 1.4642 - mse: 1.4642"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seriousmaria/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9085/9085 [==============================] - 0s 25us/sample - loss: 1.0648 - mse: 1.0648 - val_loss: 0.6891 - val_mse: 0.6891\n",
      "Epoch 3/500\n",
      "9085/9085 [==============================] - 0s 32us/sample - loss: 0.5538 - mse: 0.5538 - val_loss: 0.4544 - val_mse: 0.4544\n",
      "Epoch 4/500\n",
      "9085/9085 [==============================] - 1s 61us/sample - loss: 0.4010 - mse: 0.4010 - val_loss: 0.3518 - val_mse: 0.3518\n",
      "Epoch 5/500\n",
      "9085/9085 [==============================] - 0s 31us/sample - loss: 0.3255 - mse: 0.3255 - val_loss: 0.2932 - val_mse: 0.2932\n",
      "Epoch 6/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.2765 - mse: 0.2765 - val_loss: 0.2465 - val_mse: 0.2465\n",
      "Epoch 7/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.2452 - mse: 0.2452 - val_loss: 0.2274 - val_mse: 0.2274\n",
      "Epoch 8/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.2224 - mse: 0.2224 - val_loss: 0.2084 - val_mse: 0.2084\n",
      "Epoch 9/500\n",
      "9085/9085 [==============================] - 0s 21us/sample - loss: 0.2026 - mse: 0.2026 - val_loss: 0.1920 - val_mse: 0.1920\n",
      "Epoch 10/500\n",
      "9085/9085 [==============================] - 0s 21us/sample - loss: 0.1860 - mse: 0.1860 - val_loss: 0.1817 - val_mse: 0.1817\n",
      "Epoch 11/500\n",
      "9085/9085 [==============================] - 0s 20us/sample - loss: 0.1724 - mse: 0.1724 - val_loss: 0.1626 - val_mse: 0.1626\n",
      "Epoch 12/500\n",
      "9085/9085 [==============================] - 0s 20us/sample - loss: 0.1595 - mse: 0.1595 - val_loss: 0.1527 - val_mse: 0.1527\n",
      "Epoch 13/500\n",
      "9085/9085 [==============================] - 0s 21us/sample - loss: 0.1535 - mse: 0.1535 - val_loss: 0.1479 - val_mse: 0.1479\n",
      "Epoch 14/500\n",
      "9085/9085 [==============================] - 0s 21us/sample - loss: 0.1430 - mse: 0.1430 - val_loss: 0.1421 - val_mse: 0.1421\n",
      "Epoch 15/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.1332 - mse: 0.1332 - val_loss: 0.1319 - val_mse: 0.1319\n",
      "Epoch 16/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.1277 - mse: 0.1277 - val_loss: 0.1362 - val_mse: 0.1362\n",
      "Epoch 17/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.1216 - mse: 0.1216 - val_loss: 0.1157 - val_mse: 0.1157\n",
      "Epoch 18/500\n",
      "9085/9085 [==============================] - 0s 25us/sample - loss: 0.1158 - mse: 0.1158 - val_loss: 0.1228 - val_mse: 0.1228\n",
      "Epoch 19/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.1120 - mse: 0.1120 - val_loss: 0.1114 - val_mse: 0.1114\n",
      "Epoch 20/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.1071 - mse: 0.1071 - val_loss: 0.1183 - val_mse: 0.1183\n",
      "Epoch 21/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0997 - mse: 0.0997 - val_loss: 0.1014 - val_mse: 0.1014\n",
      "Epoch 22/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0961 - mse: 0.0961 - val_loss: 0.1062 - val_mse: 0.1062\n",
      "Epoch 23/500\n",
      "9085/9085 [==============================] - 0s 34us/sample - loss: 0.0916 - mse: 0.0916 - val_loss: 0.0970 - val_mse: 0.0970\n",
      "Epoch 24/500\n",
      "9085/9085 [==============================] - 0s 27us/sample - loss: 0.0850 - mse: 0.0850 - val_loss: 0.0968 - val_mse: 0.0968\n",
      "Epoch 25/500\n",
      "9085/9085 [==============================] - 0s 29us/sample - loss: 0.0815 - mse: 0.0815 - val_loss: 0.0897 - val_mse: 0.0897\n",
      "Epoch 26/500\n",
      "9085/9085 [==============================] - 0s 30us/sample - loss: 0.0791 - mse: 0.0791 - val_loss: 0.0837 - val_mse: 0.0837\n",
      "Epoch 27/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0745 - mse: 0.0745 - val_loss: 0.0729 - val_mse: 0.0729\n",
      "Epoch 28/500\n",
      "9085/9085 [==============================] - 0s 29us/sample - loss: 0.0726 - mse: 0.0726 - val_loss: 0.0849 - val_mse: 0.0849\n",
      "Epoch 29/500\n",
      "9085/9085 [==============================] - 0s 26us/sample - loss: 0.0706 - mse: 0.0706 - val_loss: 0.0688 - val_mse: 0.0688\n",
      "Epoch 30/500\n",
      "9085/9085 [==============================] - 0s 26us/sample - loss: 0.0689 - mse: 0.0689 - val_loss: 0.0694 - val_mse: 0.0694\n",
      "Epoch 31/500\n",
      "9085/9085 [==============================] - 0s 27us/sample - loss: 0.0675 - mse: 0.0675 - val_loss: 0.0734 - val_mse: 0.0734\n",
      "Epoch 32/500\n",
      "9085/9085 [==============================] - 0s 25us/sample - loss: 0.0682 - mse: 0.0682 - val_loss: 0.0876 - val_mse: 0.0876\n",
      "Epoch 33/500\n",
      "9085/9085 [==============================] - 0s 28us/sample - loss: 0.0662 - mse: 0.0662 - val_loss: 0.0669 - val_mse: 0.0669\n",
      "Epoch 34/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0629 - mse: 0.0629 - val_loss: 0.0786 - val_mse: 0.0786\n",
      "Epoch 35/500\n",
      "9085/9085 [==============================] - 0s 29us/sample - loss: 0.0666 - mse: 0.0666 - val_loss: 0.0744 - val_mse: 0.0744\n",
      "Epoch 36/500\n",
      "9085/9085 [==============================] - 0s 25us/sample - loss: 0.0630 - mse: 0.0630 - val_loss: 0.0672 - val_mse: 0.0672\n",
      "Epoch 37/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0625 - mse: 0.0625 - val_loss: 0.0646 - val_mse: 0.0646\n",
      "Epoch 38/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0618 - mse: 0.0618 - val_loss: 0.0698 - val_mse: 0.0698\n",
      "Epoch 39/500\n",
      "9085/9085 [==============================] - 0s 30us/sample - loss: 0.0607 - mse: 0.0607 - val_loss: 0.0632 - val_mse: 0.0632\n",
      "Epoch 40/500\n",
      "9085/9085 [==============================] - 0s 32us/sample - loss: 0.0590 - mse: 0.0590 - val_loss: 0.0660 - val_mse: 0.0660\n",
      "Epoch 41/500\n",
      "9085/9085 [==============================] - 0s 25us/sample - loss: 0.0593 - mse: 0.0593 - val_loss: 0.0576 - val_mse: 0.0576\n",
      "Epoch 42/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0576 - mse: 0.0576 - val_loss: 0.0567 - val_mse: 0.0567\n",
      "Epoch 43/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0588 - mse: 0.0588 - val_loss: 0.0565 - val_mse: 0.0565\n",
      "Epoch 44/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0600 - mse: 0.0600 - val_loss: 0.0628 - val_mse: 0.0628\n",
      "Epoch 45/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0577 - mse: 0.0577 - val_loss: 0.0547 - val_mse: 0.0547\n",
      "Epoch 46/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0558 - mse: 0.0558 - val_loss: 0.0625 - val_mse: 0.0625\n",
      "Epoch 47/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0579 - mse: 0.0579 - val_loss: 0.0577 - val_mse: 0.0577\n",
      "Epoch 48/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0549 - mse: 0.0549 - val_loss: 0.0718 - val_mse: 0.0718\n",
      "Epoch 49/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0561 - mse: 0.0561 - val_loss: 0.0521 - val_mse: 0.0521\n",
      "Epoch 50/500\n",
      "9085/9085 [==============================] - 0s 21us/sample - loss: 0.0559 - mse: 0.0559 - val_loss: 0.0563 - val_mse: 0.0563\n",
      "Epoch 51/500\n",
      "9085/9085 [==============================] - 0s 21us/sample - loss: 0.0536 - mse: 0.0536 - val_loss: 0.0644 - val_mse: 0.0644\n",
      "Epoch 52/500\n",
      "9085/9085 [==============================] - 0s 21us/sample - loss: 0.0543 - mse: 0.0543 - val_loss: 0.0512 - val_mse: 0.0512\n",
      "Epoch 53/500\n",
      "9085/9085 [==============================] - 0s 25us/sample - loss: 0.0542 - mse: 0.0542 - val_loss: 0.0575 - val_mse: 0.0575\n",
      "Epoch 54/500\n",
      "9085/9085 [==============================] - 0s 26us/sample - loss: 0.0515 - mse: 0.0515 - val_loss: 0.0505 - val_mse: 0.0505\n",
      "Epoch 55/500\n",
      "9085/9085 [==============================] - 0s 26us/sample - loss: 0.0510 - mse: 0.0510 - val_loss: 0.0546 - val_mse: 0.0546\n",
      "Epoch 56/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0500 - mse: 0.0500 - val_loss: 0.0480 - val_mse: 0.0480\n",
      "Epoch 57/500\n",
      "9085/9085 [==============================] - 0s 26us/sample - loss: 0.0509 - mse: 0.0509 - val_loss: 0.0457 - val_mse: 0.0457\n",
      "Epoch 58/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0485 - mse: 0.0485 - val_loss: 0.0479 - val_mse: 0.0479\n",
      "Epoch 59/500\n",
      "9085/9085 [==============================] - 0s 21us/sample - loss: 0.0489 - mse: 0.0489 - val_loss: 0.0459 - val_mse: 0.0459\n",
      "Epoch 60/500\n",
      "9085/9085 [==============================] - 0s 21us/sample - loss: 0.0472 - mse: 0.0472 - val_loss: 0.0614 - val_mse: 0.0614\n",
      "Epoch 61/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0444 - mse: 0.0444 - val_loss: 0.0413 - val_mse: 0.0413\n",
      "Epoch 62/500\n",
      "9085/9085 [==============================] - 0s 27us/sample - loss: 0.0438 - mse: 0.0438 - val_loss: 0.0441 - val_mse: 0.0441\n",
      "Epoch 63/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0415 - mse: 0.0415 - val_loss: 0.0447 - val_mse: 0.0447\n",
      "Epoch 64/500\n",
      "9085/9085 [==============================] - 0s 26us/sample - loss: 0.0422 - mse: 0.0422 - val_loss: 0.0450 - val_mse: 0.0450\n",
      "Epoch 65/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0408 - mse: 0.0408 - val_loss: 0.0376 - val_mse: 0.0376\n",
      "Epoch 66/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0390 - mse: 0.0390 - val_loss: 0.0503 - val_mse: 0.0503\n",
      "Epoch 67/500\n",
      "9085/9085 [==============================] - 0s 26us/sample - loss: 0.0384 - mse: 0.0384 - val_loss: 0.0400 - val_mse: 0.0400\n",
      "Epoch 68/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0387 - mse: 0.0387 - val_loss: 0.0362 - val_mse: 0.0362\n",
      "Epoch 69/500\n",
      "9085/9085 [==============================] - 0s 39us/sample - loss: 0.0381 - mse: 0.0381 - val_loss: 0.0367 - val_mse: 0.0367\n",
      "Epoch 70/500\n",
      "9085/9085 [==============================] - 1s 55us/sample - loss: 0.0368 - mse: 0.0368 - val_loss: 0.0401 - val_mse: 0.0401\n",
      "Epoch 71/500\n",
      "9085/9085 [==============================] - 0s 33us/sample - loss: 0.0350 - mse: 0.0350 - val_loss: 0.0325 - val_mse: 0.0325\n",
      "Epoch 72/500\n",
      "9085/9085 [==============================] - 0s 26us/sample - loss: 0.0345 - mse: 0.0345 - val_loss: 0.0331 - val_mse: 0.0331\n",
      "Epoch 73/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0328 - mse: 0.0328 - val_loss: 0.0308 - val_mse: 0.0308\n",
      "Epoch 74/500\n",
      "9085/9085 [==============================] - 0s 20us/sample - loss: 0.0306 - mse: 0.0306 - val_loss: 0.0387 - val_mse: 0.0387\n",
      "Epoch 75/500\n",
      "9085/9085 [==============================] - 0s 20us/sample - loss: 0.0299 - mse: 0.0299 - val_loss: 0.0290 - val_mse: 0.0290\n",
      "Epoch 76/500\n",
      "9085/9085 [==============================] - 0s 20us/sample - loss: 0.0292 - mse: 0.0292 - val_loss: 0.0290 - val_mse: 0.0290\n",
      "Epoch 77/500\n",
      "9085/9085 [==============================] - 0s 21us/sample - loss: 0.0287 - mse: 0.0287 - val_loss: 0.0248 - val_mse: 0.0248\n",
      "Epoch 78/500\n",
      "9085/9085 [==============================] - 0s 20us/sample - loss: 0.0274 - mse: 0.0274 - val_loss: 0.0265 - val_mse: 0.0265\n",
      "Epoch 79/500\n",
      "9085/9085 [==============================] - 0s 21us/sample - loss: 0.0263 - mse: 0.0263 - val_loss: 0.0233 - val_mse: 0.0233\n",
      "Epoch 80/500\n",
      "9085/9085 [==============================] - 0s 20us/sample - loss: 0.0257 - mse: 0.0257 - val_loss: 0.0221 - val_mse: 0.0221\n",
      "Epoch 81/500\n",
      "9085/9085 [==============================] - 0s 20us/sample - loss: 0.0250 - mse: 0.0250 - val_loss: 0.0218 - val_mse: 0.0218\n",
      "Epoch 82/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0262 - mse: 0.0262 - val_loss: 0.0212 - val_mse: 0.0212\n",
      "Epoch 83/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0244 - mse: 0.0244 - val_loss: 0.0219 - val_mse: 0.0219\n",
      "Epoch 84/500\n",
      "9085/9085 [==============================] - 0s 25us/sample - loss: 0.0246 - mse: 0.0246 - val_loss: 0.0211 - val_mse: 0.0211\n",
      "Epoch 85/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0229 - mse: 0.0229 - val_loss: 0.0220 - val_mse: 0.0220\n",
      "Epoch 86/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0228 - mse: 0.0228 - val_loss: 0.0192 - val_mse: 0.0192\n",
      "Epoch 87/500\n",
      "9085/9085 [==============================] - 0s 20us/sample - loss: 0.0232 - mse: 0.0232 - val_loss: 0.0277 - val_mse: 0.0277\n",
      "Epoch 88/500\n",
      "9085/9085 [==============================] - 0s 20us/sample - loss: 0.0242 - mse: 0.0242 - val_loss: 0.0203 - val_mse: 0.0203\n",
      "Epoch 89/500\n",
      "9085/9085 [==============================] - 0s 21us/sample - loss: 0.0207 - mse: 0.0207 - val_loss: 0.0187 - val_mse: 0.0187\n",
      "Epoch 90/500\n",
      "9085/9085 [==============================] - 0s 20us/sample - loss: 0.0216 - mse: 0.0216 - val_loss: 0.0218 - val_mse: 0.0218\n",
      "Epoch 91/500\n",
      "9085/9085 [==============================] - 0s 20us/sample - loss: 0.0209 - mse: 0.0209 - val_loss: 0.0183 - val_mse: 0.0183\n",
      "Epoch 92/500\n",
      "9085/9085 [==============================] - 0s 20us/sample - loss: 0.0221 - mse: 0.0221 - val_loss: 0.0182 - val_mse: 0.0182\n",
      "Epoch 93/500\n",
      "9085/9085 [==============================] - 0s 20us/sample - loss: 0.0211 - mse: 0.0211 - val_loss: 0.0216 - val_mse: 0.0216\n",
      "Epoch 94/500\n",
      "9085/9085 [==============================] - 0s 21us/sample - loss: 0.0199 - mse: 0.0199 - val_loss: 0.0191 - val_mse: 0.0191\n",
      "Epoch 95/500\n",
      "9085/9085 [==============================] - 0s 20us/sample - loss: 0.0199 - mse: 0.0199 - val_loss: 0.0169 - val_mse: 0.0169\n",
      "Epoch 96/500\n",
      "9085/9085 [==============================] - 0s 21us/sample - loss: 0.0200 - mse: 0.0200 - val_loss: 0.0192 - val_mse: 0.0192\n",
      "Epoch 97/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0195 - mse: 0.0195 - val_loss: 0.0154 - val_mse: 0.0154\n",
      "Epoch 98/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0186 - mse: 0.0186 - val_loss: 0.0164 - val_mse: 0.0164\n",
      "Epoch 99/500\n",
      "9085/9085 [==============================] - 0s 20us/sample - loss: 0.0181 - mse: 0.0181 - val_loss: 0.0161 - val_mse: 0.0161\n",
      "Epoch 100/500\n",
      "9085/9085 [==============================] - 0s 20us/sample - loss: 0.0189 - mse: 0.0189 - val_loss: 0.0189 - val_mse: 0.0189\n",
      "Epoch 101/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0186 - mse: 0.0186 - val_loss: 0.0146 - val_mse: 0.0146\n",
      "Epoch 102/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0174 - mse: 0.0174 - val_loss: 0.0141 - val_mse: 0.0141\n",
      "Epoch 103/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0168 - mse: 0.0168 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 104/500\n",
      "9085/9085 [==============================] - 0s 26us/sample - loss: 0.0167 - mse: 0.0167 - val_loss: 0.0160 - val_mse: 0.0160\n",
      "Epoch 105/500\n",
      "9085/9085 [==============================] - 0s 30us/sample - loss: 0.0151 - mse: 0.0151 - val_loss: 0.0148 - val_mse: 0.0148\n",
      "Epoch 106/500\n",
      "9085/9085 [==============================] - 0s 31us/sample - loss: 0.0164 - mse: 0.0164 - val_loss: 0.0166 - val_mse: 0.0166\n",
      "Epoch 107/500\n",
      "9085/9085 [==============================] - 0s 35us/sample - loss: 0.0158 - mse: 0.0158 - val_loss: 0.0139 - val_mse: 0.0139\n",
      "Epoch 108/500\n",
      "9085/9085 [==============================] - 0s 36us/sample - loss: 0.0151 - mse: 0.0151 - val_loss: 0.0164 - val_mse: 0.0164\n",
      "Epoch 109/500\n",
      "9085/9085 [==============================] - 0s 26us/sample - loss: 0.0160 - mse: 0.0160 - val_loss: 0.0129 - val_mse: 0.0129\n",
      "Epoch 110/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0148 - mse: 0.0148 - val_loss: 0.0187 - val_mse: 0.0187\n",
      "Epoch 111/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0155 - mse: 0.0155 - val_loss: 0.0126 - val_mse: 0.0126\n",
      "Epoch 112/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0152 - mse: 0.0152 - val_loss: 0.0129 - val_mse: 0.0129\n",
      "Epoch 113/500\n",
      "9085/9085 [==============================] - 0s 28us/sample - loss: 0.0134 - mse: 0.0134 - val_loss: 0.0121 - val_mse: 0.0121\n",
      "Epoch 114/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0146 - mse: 0.0146 - val_loss: 0.0129 - val_mse: 0.0129\n",
      "Epoch 115/500\n",
      "9085/9085 [==============================] - 0s 25us/sample - loss: 0.0138 - mse: 0.0138 - val_loss: 0.0100 - val_mse: 0.0100\n",
      "Epoch 116/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0136 - mse: 0.0136 - val_loss: 0.0114 - val_mse: 0.0114\n",
      "Epoch 117/500\n",
      "9085/9085 [==============================] - 0s 20us/sample - loss: 0.0125 - mse: 0.0125 - val_loss: 0.0165 - val_mse: 0.0165\n",
      "Epoch 118/500\n",
      "9085/9085 [==============================] - 0s 20us/sample - loss: 0.0128 - mse: 0.0128 - val_loss: 0.0108 - val_mse: 0.0108\n",
      "Epoch 119/500\n",
      "9085/9085 [==============================] - 0s 20us/sample - loss: 0.0134 - mse: 0.0134 - val_loss: 0.0134 - val_mse: 0.0134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/500\n",
      "9085/9085 [==============================] - 0s 21us/sample - loss: 0.0124 - mse: 0.0124 - val_loss: 0.0155 - val_mse: 0.0155\n",
      "Epoch 121/500\n",
      "9085/9085 [==============================] - 0s 21us/sample - loss: 0.0121 - mse: 0.0121 - val_loss: 0.0120 - val_mse: 0.0120\n",
      "Epoch 122/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0127 - mse: 0.0127 - val_loss: 0.0113 - val_mse: 0.0113\n",
      "Epoch 123/500\n",
      "9085/9085 [==============================] - 0s 21us/sample - loss: 0.0130 - mse: 0.0130 - val_loss: 0.0083 - val_mse: 0.0083\n",
      "Epoch 124/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0119 - mse: 0.0119 - val_loss: 0.0121 - val_mse: 0.0121\n",
      "Epoch 125/500\n",
      "9085/9085 [==============================] - 0s 21us/sample - loss: 0.0112 - mse: 0.0112 - val_loss: 0.0104 - val_mse: 0.0104\n",
      "Epoch 126/500\n",
      "9085/9085 [==============================] - 0s 20us/sample - loss: 0.0114 - mse: 0.0114 - val_loss: 0.0082 - val_mse: 0.0082\n",
      "Epoch 127/500\n",
      "9085/9085 [==============================] - 0s 21us/sample - loss: 0.0111 - mse: 0.0111 - val_loss: 0.0082 - val_mse: 0.0082\n",
      "Epoch 128/500\n",
      "9085/9085 [==============================] - 0s 21us/sample - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0110 - val_mse: 0.0110\n",
      "Epoch 129/500\n",
      "9085/9085 [==============================] - 0s 21us/sample - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0099 - val_mse: 0.0099\n",
      "Epoch 130/500\n",
      "9085/9085 [==============================] - 0s 21us/sample - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0089 - val_mse: 0.0089\n",
      "Epoch 131/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0096 - val_mse: 0.0096\n",
      "Epoch 132/500\n",
      "9085/9085 [==============================] - 0s 25us/sample - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0146 - val_mse: 0.0146\n",
      "Epoch 133/500\n",
      "9085/9085 [==============================] - 0s 21us/sample - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0092 - val_mse: 0.0092\n",
      "Epoch 134/500\n",
      "9085/9085 [==============================] - 0s 21us/sample - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0068 - val_mse: 0.0068\n",
      "Epoch 135/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0077 - val_mse: 0.0077\n",
      "Epoch 136/500\n",
      "9085/9085 [==============================] - 0s 21us/sample - loss: 0.0090 - mse: 0.0090 - val_loss: 0.0081 - val_mse: 0.0081\n",
      "Epoch 137/500\n",
      "9085/9085 [==============================] - 0s 21us/sample - loss: 0.0083 - mse: 0.0083 - val_loss: 0.0079 - val_mse: 0.0079\n",
      "Epoch 138/500\n",
      "9085/9085 [==============================] - 0s 21us/sample - loss: 0.0090 - mse: 0.0090 - val_loss: 0.0076 - val_mse: 0.0076\n",
      "Epoch 139/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0086 - mse: 0.0086 - val_loss: 0.0065 - val_mse: 0.0065\n",
      "Epoch 140/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0092 - mse: 0.0092 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 141/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0114 - val_mse: 0.0114\n",
      "Epoch 142/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0092 - mse: 0.0092 - val_loss: 0.0064 - val_mse: 0.0064\n",
      "Epoch 143/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0081 - mse: 0.0081 - val_loss: 0.0061 - val_mse: 0.0061\n",
      "Epoch 144/500\n",
      "9085/9085 [==============================] - 0s 30us/sample - loss: 0.0091 - mse: 0.0091 - val_loss: 0.0101 - val_mse: 0.0101\n",
      "Epoch 145/500\n",
      "9085/9085 [==============================] - 0s 31us/sample - loss: 0.0094 - mse: 0.0094 - val_loss: 0.0068 - val_mse: 0.0068\n",
      "Epoch 146/500\n",
      "9085/9085 [==============================] - 0s 44us/sample - loss: 0.0089 - mse: 0.0089 - val_loss: 0.0075 - val_mse: 0.0075\n",
      "Epoch 147/500\n",
      "9085/9085 [==============================] - 0s 26us/sample - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0178 - val_mse: 0.0178\n",
      "Epoch 148/500\n",
      "9085/9085 [==============================] - 0s 29us/sample - loss: 0.0079 - mse: 0.0079 - val_loss: 0.0071 - val_mse: 0.0071\n",
      "Epoch 149/500\n",
      "9085/9085 [==============================] - 0s 37us/sample - loss: 0.0086 - mse: 0.0086 - val_loss: 0.0078 - val_mse: 0.0078\n",
      "Epoch 150/500\n",
      "9085/9085 [==============================] - 1s 81us/sample - loss: 0.0087 - mse: 0.0087 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 151/500\n",
      "9085/9085 [==============================] - 0s 41us/sample - loss: 0.0082 - mse: 0.0082 - val_loss: 0.0058 - val_mse: 0.0058\n",
      "Epoch 152/500\n",
      "9085/9085 [==============================] - 0s 21us/sample - loss: 0.0077 - mse: 0.0077 - val_loss: 0.0060 - val_mse: 0.0060\n",
      "Epoch 153/500\n",
      "9085/9085 [==============================] - 0s 21us/sample - loss: 0.0094 - mse: 0.0094 - val_loss: 0.0107 - val_mse: 0.0107\n",
      "Epoch 154/500\n",
      "9085/9085 [==============================] - 0s 20us/sample - loss: 0.0079 - mse: 0.0079 - val_loss: 0.0096 - val_mse: 0.0096\n",
      "Epoch 155/500\n",
      "9085/9085 [==============================] - 0s 21us/sample - loss: 0.0074 - mse: 0.0074 - val_loss: 0.0058 - val_mse: 0.0058\n",
      "Epoch 156/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0077 - mse: 0.0077 - val_loss: 0.0053 - val_mse: 0.0053\n",
      "Epoch 157/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0081 - mse: 0.0081 - val_loss: 0.0057 - val_mse: 0.0057\n",
      "Epoch 158/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0076 - mse: 0.0076 - val_loss: 0.0066 - val_mse: 0.0066\n",
      "Epoch 159/500\n",
      "9085/9085 [==============================] - 0s 21us/sample - loss: 0.0073 - mse: 0.0073 - val_loss: 0.0098 - val_mse: 0.0098\n",
      "Epoch 160/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0087 - mse: 0.0087 - val_loss: 0.0066 - val_mse: 0.0066\n",
      "Epoch 161/500\n",
      "9085/9085 [==============================] - 0s 21us/sample - loss: 0.0070 - mse: 0.0070 - val_loss: 0.0049 - val_mse: 0.0049\n",
      "Epoch 162/500\n",
      "9085/9085 [==============================] - 0s 21us/sample - loss: 0.0073 - mse: 0.0073 - val_loss: 0.0048 - val_mse: 0.0048\n",
      "Epoch 163/500\n",
      "9085/9085 [==============================] - 0s 20us/sample - loss: 0.0083 - mse: 0.0083 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 164/500\n",
      "9085/9085 [==============================] - 0s 21us/sample - loss: 0.0075 - mse: 0.0075 - val_loss: 0.0068 - val_mse: 0.0068\n",
      "Epoch 165/500\n",
      "9085/9085 [==============================] - 0s 20us/sample - loss: 0.0074 - mse: 0.0074 - val_loss: 0.0060 - val_mse: 0.0060\n",
      "Epoch 166/500\n",
      "9085/9085 [==============================] - 0s 21us/sample - loss: 0.0083 - mse: 0.0083 - val_loss: 0.0059 - val_mse: 0.0059\n",
      "Epoch 167/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0074 - mse: 0.0074 - val_loss: 0.0058 - val_mse: 0.0058\n",
      "Epoch 168/500\n",
      "9085/9085 [==============================] - 0s 21us/sample - loss: 0.0075 - mse: 0.0075 - val_loss: 0.0060 - val_mse: 0.0060\n",
      "Epoch 169/500\n",
      "9085/9085 [==============================] - 0s 20us/sample - loss: 0.0075 - mse: 0.0075 - val_loss: 0.0084 - val_mse: 0.0084\n",
      "Epoch 170/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0072 - mse: 0.0072 - val_loss: 0.0055 - val_mse: 0.0055\n",
      "Epoch 171/500\n",
      "9085/9085 [==============================] - 0s 25us/sample - loss: 0.0075 - mse: 0.0075 - val_loss: 0.0076 - val_mse: 0.0076\n",
      "Epoch 172/500\n",
      "9085/9085 [==============================] - 0s 36us/sample - loss: 0.0075 - mse: 0.0075 - val_loss: 0.0055 - val_mse: 0.0055\n",
      "Epoch 173/500\n",
      "9085/9085 [==============================] - 0s 37us/sample - loss: 0.0068 - mse: 0.0068 - val_loss: 0.0050 - val_mse: 0.0050\n",
      "Epoch 174/500\n",
      "9085/9085 [==============================] - 0s 26us/sample - loss: 0.0064 - mse: 0.0064 - val_loss: 0.0067 - val_mse: 0.0067\n",
      "Epoch 175/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0069 - mse: 0.0069 - val_loss: 0.0066 - val_mse: 0.0066\n",
      "Epoch 176/500\n",
      "9085/9085 [==============================] - 0s 21us/sample - loss: 0.0065 - mse: 0.0065 - val_loss: 0.0058 - val_mse: 0.0058\n",
      "Epoch 177/500\n",
      "9085/9085 [==============================] - 0s 20us/sample - loss: 0.0081 - mse: 0.0081 - val_loss: 0.0059 - val_mse: 0.0059\n",
      "Epoch 178/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9085/9085 [==============================] - 0s 20us/sample - loss: 0.0075 - mse: 0.0075 - val_loss: 0.0050 - val_mse: 0.0050\n",
      "Epoch 179/500\n",
      "9085/9085 [==============================] - 0s 21us/sample - loss: 0.0072 - mse: 0.0072 - val_loss: 0.0062 - val_mse: 0.0062\n",
      "Epoch 180/500\n",
      "9085/9085 [==============================] - 0s 21us/sample - loss: 0.0069 - mse: 0.0069 - val_loss: 0.0065 - val_mse: 0.0065\n",
      "Epoch 181/500\n",
      "9085/9085 [==============================] - 0s 30us/sample - loss: 0.0074 - mse: 0.0074 - val_loss: 0.0107 - val_mse: 0.0107\n",
      "Epoch 182/500\n",
      "9085/9085 [==============================] - 0s 27us/sample - loss: 0.0070 - mse: 0.0070 - val_loss: 0.0060 - val_mse: 0.0060\n",
      "Epoch 183/500\n",
      "9085/9085 [==============================] - 0s 31us/sample - loss: 0.0070 - mse: 0.0070 - val_loss: 0.0063 - val_mse: 0.0063\n",
      "Epoch 184/500\n",
      "9085/9085 [==============================] - 0s 31us/sample - loss: 0.0067 - mse: 0.0067 - val_loss: 0.0117 - val_mse: 0.0117\n",
      "Epoch 185/500\n",
      "9085/9085 [==============================] - 0s 36us/sample - loss: 0.0075 - mse: 0.0075 - val_loss: 0.0071 - val_mse: 0.0071\n",
      "Epoch 186/500\n",
      "9085/9085 [==============================] - 1s 57us/sample - loss: 0.0069 - mse: 0.0069 - val_loss: 0.0112 - val_mse: 0.0112\n",
      "Epoch 187/500\n",
      "9085/9085 [==============================] - 0s 29us/sample - loss: 0.0070 - mse: 0.0070 - val_loss: 0.0062 - val_mse: 0.0062\n",
      "Epoch 188/500\n",
      "9085/9085 [==============================] - 0s 27us/sample - loss: 0.0061 - mse: 0.0061 - val_loss: 0.0047 - val_mse: 0.0047\n",
      "Epoch 189/500\n",
      "9085/9085 [==============================] - 0s 26us/sample - loss: 0.0083 - mse: 0.0083 - val_loss: 0.0079 - val_mse: 0.0079\n",
      "Epoch 190/500\n",
      "9085/9085 [==============================] - 0s 30us/sample - loss: 0.0068 - mse: 0.0068 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 191/500\n",
      "9085/9085 [==============================] - 0s 26us/sample - loss: 0.0070 - mse: 0.0070 - val_loss: 0.0057 - val_mse: 0.0057\n",
      "Epoch 192/500\n",
      "9085/9085 [==============================] - 0s 27us/sample - loss: 0.0072 - mse: 0.0072 - val_loss: 0.0057 - val_mse: 0.0057\n",
      "Epoch 193/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0059 - mse: 0.0059 - val_loss: 0.0043 - val_mse: 0.0043\n",
      "Epoch 194/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0076 - mse: 0.0076 - val_loss: 0.0046 - val_mse: 0.0046\n",
      "Epoch 195/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0071 - mse: 0.0071 - val_loss: 0.0044 - val_mse: 0.0044\n",
      "Epoch 196/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0066 - mse: 0.0066 - val_loss: 0.0063 - val_mse: 0.0063\n",
      "Epoch 197/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0066 - mse: 0.0066 - val_loss: 0.0058 - val_mse: 0.0058\n",
      "Epoch 198/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0067 - mse: 0.0067 - val_loss: 0.0045 - val_mse: 0.0045\n",
      "Epoch 199/500\n",
      "9085/9085 [==============================] - 0s 28us/sample - loss: 0.0065 - mse: 0.0065 - val_loss: 0.0049 - val_mse: 0.0049\n",
      "Epoch 200/500\n",
      "9085/9085 [==============================] - 0s 30us/sample - loss: 0.0067 - mse: 0.0067 - val_loss: 0.0039 - val_mse: 0.0039\n",
      "Epoch 201/500\n",
      "9085/9085 [==============================] - 0s 27us/sample - loss: 0.0065 - mse: 0.0065 - val_loss: 0.0057 - val_mse: 0.0057\n",
      "Epoch 202/500\n",
      "9085/9085 [==============================] - 0s 26us/sample - loss: 0.0062 - mse: 0.0062 - val_loss: 0.0071 - val_mse: 0.0071\n",
      "Epoch 203/500\n",
      "9085/9085 [==============================] - 0s 28us/sample - loss: 0.0069 - mse: 0.0069 - val_loss: 0.0048 - val_mse: 0.0048\n",
      "Epoch 204/500\n",
      "9085/9085 [==============================] - 0s 28us/sample - loss: 0.0057 - mse: 0.0057 - val_loss: 0.0045 - val_mse: 0.0045\n",
      "Epoch 205/500\n",
      "9085/9085 [==============================] - 0s 25us/sample - loss: 0.0061 - mse: 0.0061 - val_loss: 0.0049 - val_mse: 0.0049\n",
      "Epoch 206/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0079 - mse: 0.0079 - val_loss: 0.0041 - val_mse: 0.0041\n",
      "Epoch 207/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0066 - mse: 0.0066 - val_loss: 0.0048 - val_mse: 0.0048\n",
      "Epoch 208/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0056 - mse: 0.0056 - val_loss: 0.0062 - val_mse: 0.0062\n",
      "Epoch 209/500\n",
      "9085/9085 [==============================] - 0s 25us/sample - loss: 0.0062 - mse: 0.0062 - val_loss: 0.0058 - val_mse: 0.0058\n",
      "Epoch 210/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0065 - mse: 0.0065 - val_loss: 0.0061 - val_mse: 0.0061\n",
      "Epoch 211/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0063 - mse: 0.0063 - val_loss: 0.0108 - val_mse: 0.0108\n",
      "Epoch 212/500\n",
      "9085/9085 [==============================] - 0s 32us/sample - loss: 0.0067 - mse: 0.0067 - val_loss: 0.0087 - val_mse: 0.0087\n",
      "Epoch 213/500\n",
      "9085/9085 [==============================] - 0s 32us/sample - loss: 0.0060 - mse: 0.0060 - val_loss: 0.0117 - val_mse: 0.0117\n",
      "Epoch 214/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0061 - mse: 0.0061 - val_loss: 0.0081 - val_mse: 0.0081\n",
      "Epoch 215/500\n",
      "9085/9085 [==============================] - 0s 25us/sample - loss: 0.0065 - mse: 0.0065 - val_loss: 0.0055 - val_mse: 0.0055\n",
      "Epoch 216/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0062 - mse: 0.0062 - val_loss: 0.0048 - val_mse: 0.0048\n",
      "Epoch 217/500\n",
      "9085/9085 [==============================] - 0s 28us/sample - loss: 0.0063 - mse: 0.0063 - val_loss: 0.0071 - val_mse: 0.0071\n",
      "Epoch 218/500\n",
      "9085/9085 [==============================] - 0s 30us/sample - loss: 0.0073 - mse: 0.0073 - val_loss: 0.0047 - val_mse: 0.0047\n",
      "Epoch 219/500\n",
      "9085/9085 [==============================] - 0s 42us/sample - loss: 0.0058 - mse: 0.0058 - val_loss: 0.0040 - val_mse: 0.0040\n",
      "Epoch 220/500\n",
      "9085/9085 [==============================] - 0s 31us/sample - loss: 0.0064 - mse: 0.0064 - val_loss: 0.0039 - val_mse: 0.0039\n",
      "Epoch 221/500\n",
      "9085/9085 [==============================] - 0s 28us/sample - loss: 0.0060 - mse: 0.0060 - val_loss: 0.0067 - val_mse: 0.0067\n",
      "Epoch 222/500\n",
      "9085/9085 [==============================] - 0s 27us/sample - loss: 0.0061 - mse: 0.0061 - val_loss: 0.0055 - val_mse: 0.0055\n",
      "Epoch 223/500\n",
      "9085/9085 [==============================] - 0s 33us/sample - loss: 0.0054 - mse: 0.0054 - val_loss: 0.0036 - val_mse: 0.0036\n",
      "Epoch 224/500\n",
      "9085/9085 [==============================] - 0s 36us/sample - loss: 0.0071 - mse: 0.0071 - val_loss: 0.0059 - val_mse: 0.0059\n",
      "Epoch 225/500\n",
      "9085/9085 [==============================] - 0s 51us/sample - loss: 0.0060 - mse: 0.0060 - val_loss: 0.0056 - val_mse: 0.0056\n",
      "Epoch 226/500\n",
      "9085/9085 [==============================] - 0s 27us/sample - loss: 0.0069 - mse: 0.0069 - val_loss: 0.0055 - val_mse: 0.0055\n",
      "Epoch 227/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0059 - mse: 0.0059 - val_loss: 0.0053 - val_mse: 0.0053\n",
      "Epoch 228/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0061 - mse: 0.0061 - val_loss: 0.0038 - val_mse: 0.0038\n",
      "Epoch 229/500\n",
      "9085/9085 [==============================] - 0s 25us/sample - loss: 0.0060 - mse: 0.0060 - val_loss: 0.0096 - val_mse: 0.0096\n",
      "Epoch 230/500\n",
      "9085/9085 [==============================] - 0s 28us/sample - loss: 0.0067 - mse: 0.0067 - val_loss: 0.0041 - val_mse: 0.0041\n",
      "Epoch 231/500\n",
      "9085/9085 [==============================] - 0s 28us/sample - loss: 0.0058 - mse: 0.0058 - val_loss: 0.0042 - val_mse: 0.0042\n",
      "Epoch 232/500\n",
      "9085/9085 [==============================] - 0s 26us/sample - loss: 0.0056 - mse: 0.0056 - val_loss: 0.0036 - val_mse: 0.0036\n",
      "Epoch 233/500\n",
      "9085/9085 [==============================] - 0s 28us/sample - loss: 0.0053 - mse: 0.0053 - val_loss: 0.0049 - val_mse: 0.0049\n",
      "Epoch 234/500\n",
      "9085/9085 [==============================] - 0s 27us/sample - loss: 0.0062 - mse: 0.0062 - val_loss: 0.0059 - val_mse: 0.0059\n",
      "Epoch 235/500\n",
      "9085/9085 [==============================] - 0s 25us/sample - loss: 0.0059 - mse: 0.0059 - val_loss: 0.0044 - val_mse: 0.0044\n",
      "Epoch 236/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0054 - mse: 0.0054 - val_loss: 0.0096 - val_mse: 0.0096\n",
      "Epoch 237/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0054 - mse: 0.0054 - val_loss: 0.0055 - val_mse: 0.0055\n",
      "Epoch 238/500\n",
      "9085/9085 [==============================] - 0s 27us/sample - loss: 0.0059 - mse: 0.0059 - val_loss: 0.0037 - val_mse: 0.0037\n",
      "Epoch 239/500\n",
      "9085/9085 [==============================] - 0s 26us/sample - loss: 0.0057 - mse: 0.0057 - val_loss: 0.0036 - val_mse: 0.0036\n",
      "Epoch 240/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0057 - mse: 0.0057 - val_loss: 0.0037 - val_mse: 0.0037\n",
      "Epoch 241/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0051 - mse: 0.0051 - val_loss: 0.0048 - val_mse: 0.0048\n",
      "Epoch 242/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0067 - mse: 0.0067 - val_loss: 0.0054 - val_mse: 0.0054\n",
      "Epoch 243/500\n",
      "9085/9085 [==============================] - 0s 26us/sample - loss: 0.0059 - mse: 0.0059 - val_loss: 0.0049 - val_mse: 0.0049\n",
      "Epoch 244/500\n",
      "9085/9085 [==============================] - 0s 26us/sample - loss: 0.0058 - mse: 0.0058 - val_loss: 0.0046 - val_mse: 0.0046\n",
      "Epoch 245/500\n",
      "9085/9085 [==============================] - 0s 27us/sample - loss: 0.0056 - mse: 0.0056 - val_loss: 0.0037 - val_mse: 0.0037\n",
      "Epoch 246/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0052 - mse: 0.0052 - val_loss: 0.0063 - val_mse: 0.0063\n",
      "Epoch 247/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0057 - mse: 0.0057 - val_loss: 0.0054 - val_mse: 0.0054\n",
      "Epoch 248/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0053 - mse: 0.0053 - val_loss: 0.0041 - val_mse: 0.0041\n",
      "Epoch 249/500\n",
      "9085/9085 [==============================] - 0s 21us/sample - loss: 0.0053 - mse: 0.0053 - val_loss: 0.0045 - val_mse: 0.0045\n",
      "Epoch 250/500\n",
      "9085/9085 [==============================] - 0s 21us/sample - loss: 0.0062 - mse: 0.0062 - val_loss: 0.0065 - val_mse: 0.0065\n",
      "Epoch 251/500\n",
      "9085/9085 [==============================] - 0s 20us/sample - loss: 0.0058 - mse: 0.0058 - val_loss: 0.0060 - val_mse: 0.0060\n",
      "Epoch 252/500\n",
      "9085/9085 [==============================] - 0s 20us/sample - loss: 0.0058 - mse: 0.0058 - val_loss: 0.0090 - val_mse: 0.0090\n",
      "Epoch 253/500\n",
      "9085/9085 [==============================] - 0s 28us/sample - loss: 0.0055 - mse: 0.0055 - val_loss: 0.0050 - val_mse: 0.0050\n",
      "Epoch 254/500\n",
      "9085/9085 [==============================] - 0s 25us/sample - loss: 0.0057 - mse: 0.0057 - val_loss: 0.0054 - val_mse: 0.0054\n",
      "Epoch 255/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0058 - mse: 0.0058 - val_loss: 0.0041 - val_mse: 0.0041\n",
      "Epoch 256/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0054 - mse: 0.0054 - val_loss: 0.0039 - val_mse: 0.0039\n",
      "Epoch 257/500\n",
      "9085/9085 [==============================] - 0s 31us/sample - loss: 0.0050 - mse: 0.0050 - val_loss: 0.0056 - val_mse: 0.0056\n",
      "Epoch 258/500\n",
      "9085/9085 [==============================] - 0s 26us/sample - loss: 0.0052 - mse: 0.0052 - val_loss: 0.0056 - val_mse: 0.0056\n",
      "Epoch 259/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0054 - mse: 0.0054 - val_loss: 0.0058 - val_mse: 0.0058\n",
      "Epoch 260/500\n",
      "9085/9085 [==============================] - 0s 25us/sample - loss: 0.0059 - mse: 0.0059 - val_loss: 0.0042 - val_mse: 0.0042\n",
      "Epoch 261/500\n",
      "9085/9085 [==============================] - 0s 40us/sample - loss: 0.0052 - mse: 0.0052 - val_loss: 0.0037 - val_mse: 0.0037\n",
      "Epoch 262/500\n",
      "9085/9085 [==============================] - 0s 44us/sample - loss: 0.0054 - mse: 0.0054 - val_loss: 0.0041 - val_mse: 0.0041\n",
      "Epoch 263/500\n",
      "9085/9085 [==============================] - 0s 53us/sample - loss: 0.0060 - mse: 0.0060 - val_loss: 0.0079 - val_mse: 0.0079\n",
      "Epoch 264/500\n",
      "9085/9085 [==============================] - 0s 42us/sample - loss: 0.0056 - mse: 0.0056 - val_loss: 0.0045 - val_mse: 0.0045\n",
      "Epoch 265/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0059 - mse: 0.0059 - val_loss: 0.0044 - val_mse: 0.0044\n",
      "Epoch 266/500\n",
      "9085/9085 [==============================] - 0s 20us/sample - loss: 0.0052 - mse: 0.0052 - val_loss: 0.0051 - val_mse: 0.0051\n",
      "Epoch 267/500\n",
      "9085/9085 [==============================] - 0s 21us/sample - loss: 0.0051 - mse: 0.0051 - val_loss: 0.0037 - val_mse: 0.0037\n",
      "Epoch 268/500\n",
      "9085/9085 [==============================] - 0s 21us/sample - loss: 0.0061 - mse: 0.0061 - val_loss: 0.0054 - val_mse: 0.0054\n",
      "Epoch 269/500\n",
      "9085/9085 [==============================] - 0s 20us/sample - loss: 0.0052 - mse: 0.0052 - val_loss: 0.0034 - val_mse: 0.0034\n",
      "Epoch 270/500\n",
      "9085/9085 [==============================] - 0s 20us/sample - loss: 0.0055 - mse: 0.0055 - val_loss: 0.0036 - val_mse: 0.0036\n",
      "Epoch 271/500\n",
      "9085/9085 [==============================] - 0s 21us/sample - loss: 0.0052 - mse: 0.0052 - val_loss: 0.0106 - val_mse: 0.0106\n",
      "Epoch 272/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0058 - mse: 0.0058 - val_loss: 0.0033 - val_mse: 0.0033\n",
      "Epoch 273/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0052 - mse: 0.0052 - val_loss: 0.0054 - val_mse: 0.0054\n",
      "Epoch 274/500\n",
      "9085/9085 [==============================] - 0s 20us/sample - loss: 0.0050 - mse: 0.0050 - val_loss: 0.0047 - val_mse: 0.0047\n",
      "Epoch 275/500\n",
      "9085/9085 [==============================] - 0s 20us/sample - loss: 0.0047 - mse: 0.0047 - val_loss: 0.0046 - val_mse: 0.0046\n",
      "Epoch 276/500\n",
      "9085/9085 [==============================] - 0s 20us/sample - loss: 0.0060 - mse: 0.0060 - val_loss: 0.0051 - val_mse: 0.0051\n",
      "Epoch 277/500\n",
      "9085/9085 [==============================] - 0s 20us/sample - loss: 0.0046 - mse: 0.0046 - val_loss: 0.0048 - val_mse: 0.0048\n",
      "Epoch 278/500\n",
      "9085/9085 [==============================] - 0s 21us/sample - loss: 0.0046 - mse: 0.0046 - val_loss: 0.0079 - val_mse: 0.0079\n",
      "Epoch 279/500\n",
      "9085/9085 [==============================] - 0s 20us/sample - loss: 0.0055 - mse: 0.0055 - val_loss: 0.0072 - val_mse: 0.0072\n",
      "Epoch 280/500\n",
      "9085/9085 [==============================] - 0s 21us/sample - loss: 0.0058 - mse: 0.0058 - val_loss: 0.0037 - val_mse: 0.0037\n",
      "Epoch 281/500\n",
      "9085/9085 [==============================] - 0s 21us/sample - loss: 0.0052 - mse: 0.0052 - val_loss: 0.0040 - val_mse: 0.0040\n",
      "Epoch 282/500\n",
      "9085/9085 [==============================] - 0s 29us/sample - loss: 0.0055 - mse: 0.0055 - val_loss: 0.0033 - val_mse: 0.0033\n",
      "Epoch 283/500\n",
      "9085/9085 [==============================] - 0s 34us/sample - loss: 0.0049 - mse: 0.0049 - val_loss: 0.0060 - val_mse: 0.0060\n",
      "Epoch 284/500\n",
      "9085/9085 [==============================] - 0s 26us/sample - loss: 0.0048 - mse: 0.0048 - val_loss: 0.0038 - val_mse: 0.0038\n",
      "Epoch 285/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0047 - mse: 0.0047 - val_loss: 0.0044 - val_mse: 0.0044\n",
      "Epoch 286/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0053 - mse: 0.0053 - val_loss: 0.0042 - val_mse: 0.0042\n",
      "Epoch 287/500\n",
      "9085/9085 [==============================] - 0s 21us/sample - loss: 0.0049 - mse: 0.0049 - val_loss: 0.0036 - val_mse: 0.0036\n",
      "Epoch 288/500\n",
      "9085/9085 [==============================] - 0s 20us/sample - loss: 0.0054 - mse: 0.0054 - val_loss: 0.0038 - val_mse: 0.0038\n",
      "Epoch 289/500\n",
      "9085/9085 [==============================] - 0s 25us/sample - loss: 0.0053 - mse: 0.0053 - val_loss: 0.0039 - val_mse: 0.0039\n",
      "Epoch 290/500\n",
      "9085/9085 [==============================] - 0s 30us/sample - loss: 0.0053 - mse: 0.0053 - val_loss: 0.0041 - val_mse: 0.0041\n",
      "Epoch 291/500\n",
      "9085/9085 [==============================] - 0s 34us/sample - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0033 - val_mse: 0.0033\n",
      "Epoch 292/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0051 - mse: 0.0051 - val_loss: 0.0058 - val_mse: 0.0058\n",
      "Epoch 293/500\n",
      "9085/9085 [==============================] - 0s 27us/sample - loss: 0.0051 - mse: 0.0051 - val_loss: 0.0060 - val_mse: 0.0060\n",
      "Epoch 294/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9085/9085 [==============================] - 0s 54us/sample - loss: 0.0050 - mse: 0.0050 - val_loss: 0.0033 - val_mse: 0.0033\n",
      "Epoch 295/500\n",
      "9085/9085 [==============================] - 0s 43us/sample - loss: 0.0046 - mse: 0.0046 - val_loss: 0.0034 - val_mse: 0.0034\n",
      "Epoch 296/500\n",
      "9085/9085 [==============================] - 0s 31us/sample - loss: 0.0053 - mse: 0.0053 - val_loss: 0.0046 - val_mse: 0.0046\n",
      "Epoch 297/500\n",
      "9085/9085 [==============================] - 0s 28us/sample - loss: 0.0053 - mse: 0.0053 - val_loss: 0.0049 - val_mse: 0.0049\n",
      "Epoch 298/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0047 - mse: 0.0047 - val_loss: 0.0041 - val_mse: 0.0041\n",
      "Epoch 299/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0052 - mse: 0.0052 - val_loss: 0.0044 - val_mse: 0.0044\n",
      "Epoch 300/500\n",
      "9085/9085 [==============================] - 0s 25us/sample - loss: 0.0052 - mse: 0.0052 - val_loss: 0.0050 - val_mse: 0.0050\n",
      "Epoch 301/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0047 - mse: 0.0047 - val_loss: 0.0044 - val_mse: 0.0044\n",
      "Epoch 302/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0055 - mse: 0.0055 - val_loss: 0.0040 - val_mse: 0.0040\n",
      "Epoch 303/500\n",
      "9085/9085 [==============================] - 0s 21us/sample - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0037 - val_mse: 0.0037\n",
      "Epoch 304/500\n",
      "9085/9085 [==============================] - 0s 21us/sample - loss: 0.0048 - mse: 0.0048 - val_loss: 0.0036 - val_mse: 0.0036\n",
      "Epoch 305/500\n",
      "9085/9085 [==============================] - 0s 21us/sample - loss: 0.0052 - mse: 0.0052 - val_loss: 0.0039 - val_mse: 0.0039\n",
      "Epoch 306/500\n",
      "9085/9085 [==============================] - 0s 21us/sample - loss: 0.0048 - mse: 0.0048 - val_loss: 0.0035 - val_mse: 0.0035\n",
      "Epoch 307/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0049 - mse: 0.0049 - val_loss: 0.0032 - val_mse: 0.0032\n",
      "Epoch 308/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0053 - mse: 0.0053 - val_loss: 0.0068 - val_mse: 0.0068\n",
      "Epoch 309/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0054 - mse: 0.0054 - val_loss: 0.0061 - val_mse: 0.0061\n",
      "Epoch 310/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0051 - mse: 0.0051 - val_loss: 0.0041 - val_mse: 0.0041\n",
      "Epoch 311/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0053 - mse: 0.0053 - val_loss: 0.0034 - val_mse: 0.0034\n",
      "Epoch 312/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0047 - mse: 0.0047 - val_loss: 0.0031 - val_mse: 0.0031\n",
      "Epoch 313/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0048 - mse: 0.0048 - val_loss: 0.0040 - val_mse: 0.0040\n",
      "Epoch 314/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0051 - mse: 0.0051 - val_loss: 0.0050 - val_mse: 0.0050\n",
      "Epoch 315/500\n",
      "9085/9085 [==============================] - 0s 20us/sample - loss: 0.0050 - mse: 0.0050 - val_loss: 0.0033 - val_mse: 0.0033\n",
      "Epoch 316/500\n",
      "9085/9085 [==============================] - 0s 21us/sample - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0044 - val_mse: 0.0044\n",
      "Epoch 317/500\n",
      "9085/9085 [==============================] - 0s 21us/sample - loss: 0.0052 - mse: 0.0052 - val_loss: 0.0056 - val_mse: 0.0056\n",
      "Epoch 318/500\n",
      "9085/9085 [==============================] - 0s 20us/sample - loss: 0.0050 - mse: 0.0050 - val_loss: 0.0060 - val_mse: 0.0060\n",
      "Epoch 319/500\n",
      "9085/9085 [==============================] - 0s 20us/sample - loss: 0.0047 - mse: 0.0047 - val_loss: 0.0053 - val_mse: 0.0053\n",
      "Epoch 320/500\n",
      "9085/9085 [==============================] - 0s 21us/sample - loss: 0.0051 - mse: 0.0051 - val_loss: 0.0041 - val_mse: 0.0041\n",
      "Epoch 321/500\n",
      "9085/9085 [==============================] - 0s 21us/sample - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0043 - val_mse: 0.0043\n",
      "Epoch 322/500\n",
      "9085/9085 [==============================] - 0s 21us/sample - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0035 - val_mse: 0.0035\n",
      "Epoch 323/500\n",
      "9085/9085 [==============================] - 0s 20us/sample - loss: 0.0043 - mse: 0.0043 - val_loss: 0.0085 - val_mse: 0.0085\n",
      "Epoch 324/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0046 - mse: 0.0046 - val_loss: 0.0050 - val_mse: 0.0050\n",
      "Epoch 325/500\n",
      "9085/9085 [==============================] - 0s 26us/sample - loss: 0.0062 - mse: 0.0062 - val_loss: 0.0046 - val_mse: 0.0046\n",
      "Epoch 326/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0049 - mse: 0.0049 - val_loss: 0.0053 - val_mse: 0.0053\n",
      "Epoch 327/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0052 - mse: 0.0052 - val_loss: 0.0047 - val_mse: 0.0047\n",
      "Epoch 328/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0046 - mse: 0.0046 - val_loss: 0.0039 - val_mse: 0.0039\n",
      "Epoch 329/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0048 - mse: 0.0048 - val_loss: 0.0040 - val_mse: 0.0040\n",
      "Epoch 330/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0042 - val_mse: 0.0042\n",
      "Epoch 331/500\n",
      "9085/9085 [==============================] - 0s 28us/sample - loss: 0.0051 - mse: 0.0051 - val_loss: 0.0053 - val_mse: 0.0053\n",
      "Epoch 332/500\n",
      "9085/9085 [==============================] - 0s 31us/sample - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0043 - val_mse: 0.0043\n",
      "Epoch 333/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0048 - val_mse: 0.0048\n",
      "Epoch 334/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0041 - mse: 0.0041 - val_loss: 0.0040 - val_mse: 0.0040\n",
      "Epoch 335/500\n",
      "9085/9085 [==============================] - 0s 25us/sample - loss: 0.0047 - mse: 0.0047 - val_loss: 0.0028 - val_mse: 0.0028\n",
      "Epoch 336/500\n",
      "9085/9085 [==============================] - 0s 25us/sample - loss: 0.0047 - mse: 0.0047 - val_loss: 0.0032 - val_mse: 0.0032\n",
      "Epoch 337/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0043 - mse: 0.0043 - val_loss: 0.0035 - val_mse: 0.0035\n",
      "Epoch 338/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0052 - mse: 0.0052 - val_loss: 0.0047 - val_mse: 0.0047\n",
      "Epoch 339/500\n",
      "9085/9085 [==============================] - 0s 28us/sample - loss: 0.0048 - mse: 0.0048 - val_loss: 0.0040 - val_mse: 0.0040\n",
      "Epoch 340/500\n",
      "9085/9085 [==============================] - 0s 28us/sample - loss: 0.0046 - mse: 0.0046 - val_loss: 0.0033 - val_mse: 0.0033\n",
      "Epoch 341/500\n",
      "9085/9085 [==============================] - 0s 27us/sample - loss: 0.0043 - mse: 0.0043 - val_loss: 0.0032 - val_mse: 0.0032\n",
      "Epoch 342/500\n",
      "9085/9085 [==============================] - 0s 25us/sample - loss: 0.0042 - mse: 0.0042 - val_loss: 0.0027 - val_mse: 0.0027\n",
      "Epoch 343/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0042 - mse: 0.0042 - val_loss: 0.0051 - val_mse: 0.0051\n",
      "Epoch 344/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0050 - mse: 0.0050 - val_loss: 0.0033 - val_mse: 0.0033\n",
      "Epoch 345/500\n",
      "9085/9085 [==============================] - 0s 21us/sample - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0036 - val_mse: 0.0036\n",
      "Epoch 346/500\n",
      "9085/9085 [==============================] - 0s 21us/sample - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0037 - val_mse: 0.0037\n",
      "Epoch 347/500\n",
      "9085/9085 [==============================] - 0s 21us/sample - loss: 0.0041 - mse: 0.0041 - val_loss: 0.0055 - val_mse: 0.0055\n",
      "Epoch 348/500\n",
      "9085/9085 [==============================] - 0s 20us/sample - loss: 0.0046 - mse: 0.0046 - val_loss: 0.0034 - val_mse: 0.0034\n",
      "Epoch 349/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0047 - mse: 0.0047 - val_loss: 0.0043 - val_mse: 0.0043\n",
      "Epoch 350/500\n",
      "9085/9085 [==============================] - 0s 20us/sample - loss: 0.0043 - mse: 0.0043 - val_loss: 0.0032 - val_mse: 0.0032\n",
      "Epoch 351/500\n",
      "9085/9085 [==============================] - 0s 20us/sample - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0034 - val_mse: 0.0034\n",
      "Epoch 352/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9085/9085 [==============================] - 0s 20us/sample - loss: 0.0042 - mse: 0.0042 - val_loss: 0.0061 - val_mse: 0.0061\n",
      "Epoch 353/500\n",
      "9085/9085 [==============================] - 0s 20us/sample - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0032 - val_mse: 0.0032\n",
      "Epoch 354/500\n",
      "9085/9085 [==============================] - 0s 20us/sample - loss: 0.0051 - mse: 0.0051 - val_loss: 0.0039 - val_mse: 0.0039\n",
      "Epoch 355/500\n",
      "9085/9085 [==============================] - 0s 25us/sample - loss: 0.0043 - mse: 0.0043 - val_loss: 0.0043 - val_mse: 0.0043\n",
      "Epoch 356/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0050 - mse: 0.0050 - val_loss: 0.0027 - val_mse: 0.0027\n",
      "Epoch 357/500\n",
      "9085/9085 [==============================] - 0s 21us/sample - loss: 0.0043 - mse: 0.0043 - val_loss: 0.0034 - val_mse: 0.0034\n",
      "Epoch 358/500\n",
      "9085/9085 [==============================] - 0s 20us/sample - loss: 0.0039 - mse: 0.0039 - val_loss: 0.0045 - val_mse: 0.0045\n",
      "Epoch 359/500\n",
      "9085/9085 [==============================] - 0s 20us/sample - loss: 0.0048 - mse: 0.0048 - val_loss: 0.0054 - val_mse: 0.0054\n",
      "Epoch 360/500\n",
      "9085/9085 [==============================] - 0s 21us/sample - loss: 0.0042 - mse: 0.0042 - val_loss: 0.0043 - val_mse: 0.0043\n",
      "Epoch 361/500\n",
      "9085/9085 [==============================] - 0s 20us/sample - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 362/500\n",
      "9085/9085 [==============================] - 0s 25us/sample - loss: 0.0043 - mse: 0.0043 - val_loss: 0.0052 - val_mse: 0.0052\n",
      "Epoch 363/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0043 - mse: 0.0043 - val_loss: 0.0028 - val_mse: 0.0028\n",
      "Epoch 364/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0047 - mse: 0.0047 - val_loss: 0.0028 - val_mse: 0.0028\n",
      "Epoch 365/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0042 - mse: 0.0042 - val_loss: 0.0029 - val_mse: 0.0029\n",
      "Epoch 366/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0042 - mse: 0.0042 - val_loss: 0.0039 - val_mse: 0.0039\n",
      "Epoch 367/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0042 - mse: 0.0042 - val_loss: 0.0083 - val_mse: 0.0083\n",
      "Epoch 368/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0069 - val_mse: 0.0069\n",
      "Epoch 369/500\n",
      "9085/9085 [==============================] - 0s 20us/sample - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0038 - val_mse: 0.0038\n",
      "Epoch 370/500\n",
      "9085/9085 [==============================] - 0s 21us/sample - loss: 0.0041 - mse: 0.0041 - val_loss: 0.0026 - val_mse: 0.0026\n",
      "Epoch 371/500\n",
      "9085/9085 [==============================] - 0s 20us/sample - loss: 0.0042 - mse: 0.0042 - val_loss: 0.0030 - val_mse: 0.0030\n",
      "Epoch 372/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0047 - mse: 0.0047 - val_loss: 0.0037 - val_mse: 0.0037\n",
      "Epoch 373/500\n",
      "9085/9085 [==============================] - 0s 25us/sample - loss: 0.0042 - mse: 0.0042 - val_loss: 0.0046 - val_mse: 0.0046\n",
      "Epoch 374/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0046 - mse: 0.0046 - val_loss: 0.0043 - val_mse: 0.0043\n",
      "Epoch 375/500\n",
      "9085/9085 [==============================] - 0s 28us/sample - loss: 0.0043 - mse: 0.0043 - val_loss: 0.0042 - val_mse: 0.0042\n",
      "Epoch 376/500\n",
      "9085/9085 [==============================] - 0s 26us/sample - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0046 - val_mse: 0.0046\n",
      "Epoch 377/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0041 - mse: 0.0041 - val_loss: 0.0028 - val_mse: 0.0028\n",
      "Epoch 378/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0043 - mse: 0.0043 - val_loss: 0.0036 - val_mse: 0.0036\n",
      "Epoch 379/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0041 - mse: 0.0041 - val_loss: 0.0030 - val_mse: 0.0030\n",
      "Epoch 380/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0029 - val_mse: 0.0029\n",
      "Epoch 381/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0042 - mse: 0.0042 - val_loss: 0.0055 - val_mse: 0.0055\n",
      "Epoch 382/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0033 - val_mse: 0.0033\n",
      "Epoch 383/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0047 - mse: 0.0047 - val_loss: 0.0036 - val_mse: 0.0036\n",
      "Epoch 384/500\n",
      "9085/9085 [==============================] - 0s 25us/sample - loss: 0.0041 - mse: 0.0041 - val_loss: 0.0034 - val_mse: 0.0034\n",
      "Epoch 385/500\n",
      "9085/9085 [==============================] - 0s 20us/sample - loss: 0.0046 - mse: 0.0046 - val_loss: 0.0071 - val_mse: 0.0071\n",
      "Epoch 386/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0041 - mse: 0.0041 - val_loss: 0.0034 - val_mse: 0.0034\n",
      "Epoch 387/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0047 - mse: 0.0047 - val_loss: 0.0036 - val_mse: 0.0036\n",
      "Epoch 388/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0043 - mse: 0.0043 - val_loss: 0.0035 - val_mse: 0.0035\n",
      "Epoch 389/500\n",
      "9085/9085 [==============================] - 0s 21us/sample - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0054 - val_mse: 0.0054\n",
      "Epoch 390/500\n",
      "9085/9085 [==============================] - 0s 20us/sample - loss: 0.0039 - mse: 0.0039 - val_loss: 0.0026 - val_mse: 0.0026\n",
      "Epoch 391/500\n",
      "9085/9085 [==============================] - 0s 20us/sample - loss: 0.0039 - mse: 0.0039 - val_loss: 0.0045 - val_mse: 0.0045\n",
      "Epoch 392/500\n",
      "9085/9085 [==============================] - 0s 21us/sample - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0043 - val_mse: 0.0043\n",
      "Epoch 393/500\n",
      "9085/9085 [==============================] - 0s 20us/sample - loss: 0.0041 - mse: 0.0041 - val_loss: 0.0028 - val_mse: 0.0028\n",
      "Epoch 394/500\n",
      "9085/9085 [==============================] - 0s 21us/sample - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0037 - val_mse: 0.0037\n",
      "Epoch 395/500\n",
      "9085/9085 [==============================] - 0s 20us/sample - loss: 0.0046 - mse: 0.0046 - val_loss: 0.0052 - val_mse: 0.0052\n",
      "Epoch 396/500\n",
      "9085/9085 [==============================] - 0s 21us/sample - loss: 0.0042 - mse: 0.0042 - val_loss: 0.0035 - val_mse: 0.0035\n",
      "Epoch 397/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0039 - mse: 0.0039 - val_loss: 0.0046 - val_mse: 0.0046\n",
      "Epoch 398/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0039 - mse: 0.0039 - val_loss: 0.0031 - val_mse: 0.0031\n",
      "Epoch 399/500\n",
      "9085/9085 [==============================] - 0s 26us/sample - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0047 - val_mse: 0.0047\n",
      "Epoch 400/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0042 - mse: 0.0042 - val_loss: 0.0038 - val_mse: 0.0038\n",
      "Epoch 401/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0043 - mse: 0.0043 - val_loss: 0.0041 - val_mse: 0.0041\n",
      "Epoch 402/500\n",
      "9085/9085 [==============================] - 0s 27us/sample - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0066 - val_mse: 0.0066\n",
      "Epoch 403/500\n",
      "9085/9085 [==============================] - 0s 26us/sample - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0062 - val_mse: 0.0062\n",
      "Epoch 404/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0041 - mse: 0.0041 - val_loss: 0.0040 - val_mse: 0.0040\n",
      "Epoch 405/500\n",
      "9085/9085 [==============================] - 0s 21us/sample - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0026 - val_mse: 0.0026\n",
      "Epoch 406/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0039 - mse: 0.0039 - val_loss: 0.0040 - val_mse: 0.0040\n",
      "Epoch 407/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0041 - mse: 0.0041 - val_loss: 0.0027 - val_mse: 0.0027\n",
      "Epoch 408/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0041 - mse: 0.0041 - val_loss: 0.0047 - val_mse: 0.0047\n",
      "Epoch 409/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0038 - mse: 0.0038 - val_loss: 0.0054 - val_mse: 0.0054\n",
      "Epoch 410/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9085/9085 [==============================] - 0s 21us/sample - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0071 - val_mse: 0.0071\n",
      "Epoch 411/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0041 - mse: 0.0041 - val_loss: 0.0040 - val_mse: 0.0040\n",
      "Epoch 412/500\n",
      "9085/9085 [==============================] - 0s 26us/sample - loss: 0.0048 - mse: 0.0048 - val_loss: 0.0037 - val_mse: 0.0037\n",
      "Epoch 413/500\n",
      "9085/9085 [==============================] - 0s 27us/sample - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0034 - val_mse: 0.0034\n",
      "Epoch 414/500\n",
      "9085/9085 [==============================] - 0s 28us/sample - loss: 0.0041 - mse: 0.0041 - val_loss: 0.0043 - val_mse: 0.0043\n",
      "Epoch 415/500\n",
      "9085/9085 [==============================] - 0s 29us/sample - loss: 0.0042 - mse: 0.0042 - val_loss: 0.0059 - val_mse: 0.0059\n",
      "Epoch 416/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0041 - mse: 0.0041 - val_loss: 0.0049 - val_mse: 0.0049\n",
      "Epoch 417/500\n",
      "9085/9085 [==============================] - 0s 29us/sample - loss: 0.0041 - mse: 0.0041 - val_loss: 0.0030 - val_mse: 0.0030\n",
      "Epoch 418/500\n",
      "9085/9085 [==============================] - 0s 33us/sample - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0034 - val_mse: 0.0034\n",
      "Epoch 419/500\n",
      "9085/9085 [==============================] - 0s 28us/sample - loss: 0.0041 - mse: 0.0041 - val_loss: 0.0070 - val_mse: 0.0070\n",
      "Epoch 420/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0039 - mse: 0.0039 - val_loss: 0.0050 - val_mse: 0.0050\n",
      "Epoch 421/500\n",
      "9085/9085 [==============================] - 0s 26us/sample - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0031 - val_mse: 0.0031\n",
      "Epoch 422/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0070 - val_mse: 0.0070\n",
      "Epoch 423/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0041 - mse: 0.0041 - val_loss: 0.0038 - val_mse: 0.0038\n",
      "Epoch 424/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0039 - mse: 0.0039 - val_loss: 0.0035 - val_mse: 0.0035\n",
      "Epoch 425/500\n",
      "9085/9085 [==============================] - 0s 21us/sample - loss: 0.0041 - mse: 0.0041 - val_loss: 0.0036 - val_mse: 0.0036\n",
      "Epoch 426/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0049 - mse: 0.0049 - val_loss: 0.0061 - val_mse: 0.0061\n",
      "Epoch 427/500\n",
      "9085/9085 [==============================] - 0s 21us/sample - loss: 0.0041 - mse: 0.0041 - val_loss: 0.0037 - val_mse: 0.0037\n",
      "Epoch 428/500\n",
      "9085/9085 [==============================] - 0s 21us/sample - loss: 0.0043 - mse: 0.0043 - val_loss: 0.0045 - val_mse: 0.0045\n",
      "Epoch 429/500\n",
      "9085/9085 [==============================] - 0s 21us/sample - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0035 - val_mse: 0.0035\n",
      "Epoch 430/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0039 - mse: 0.0039 - val_loss: 0.0025 - val_mse: 0.0025\n",
      "Epoch 431/500\n",
      "9085/9085 [==============================] - 0s 34us/sample - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0025 - val_mse: 0.0025\n",
      "Epoch 432/500\n",
      "9085/9085 [==============================] - 0s 25us/sample - loss: 0.0043 - mse: 0.0043 - val_loss: 0.0030 - val_mse: 0.0030\n",
      "Epoch 433/500\n",
      "9085/9085 [==============================] - 0s 25us/sample - loss: 0.0041 - mse: 0.0041 - val_loss: 0.0056 - val_mse: 0.0056\n",
      "Epoch 434/500\n",
      "9085/9085 [==============================] - 0s 28us/sample - loss: 0.0039 - mse: 0.0039 - val_loss: 0.0039 - val_mse: 0.0039\n",
      "Epoch 435/500\n",
      "9085/9085 [==============================] - 0s 33us/sample - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0033 - val_mse: 0.0033\n",
      "Epoch 436/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0042 - mse: 0.0042 - val_loss: 0.0038 - val_mse: 0.0038\n",
      "Epoch 437/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0042 - mse: 0.0042 - val_loss: 0.0043 - val_mse: 0.0043\n",
      "Epoch 438/500\n",
      "9085/9085 [==============================] - 0s 21us/sample - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0052 - val_mse: 0.0052\n",
      "Epoch 439/500\n",
      "9085/9085 [==============================] - 0s 21us/sample - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0026 - val_mse: 0.0026\n",
      "Epoch 440/500\n",
      "9085/9085 [==============================] - 0s 21us/sample - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0040 - val_mse: 0.0040\n",
      "Epoch 441/500\n",
      "9085/9085 [==============================] - 0s 21us/sample - loss: 0.0038 - mse: 0.0038 - val_loss: 0.0043 - val_mse: 0.0043\n",
      "Epoch 442/500\n",
      "9085/9085 [==============================] - 0s 20us/sample - loss: 0.0037 - mse: 0.0037 - val_loss: 0.0038 - val_mse: 0.0038\n",
      "Epoch 443/500\n",
      "9085/9085 [==============================] - 0s 21us/sample - loss: 0.0038 - mse: 0.0038 - val_loss: 0.0031 - val_mse: 0.0031\n",
      "Epoch 444/500\n",
      "9085/9085 [==============================] - 0s 20us/sample - loss: 0.0038 - mse: 0.0038 - val_loss: 0.0070 - val_mse: 0.0070\n",
      "Epoch 445/500\n",
      "9085/9085 [==============================] - 0s 21us/sample - loss: 0.0039 - mse: 0.0039 - val_loss: 0.0042 - val_mse: 0.0042\n",
      "Epoch 446/500\n",
      "9085/9085 [==============================] - 0s 20us/sample - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0031 - val_mse: 0.0031\n",
      "Epoch 447/500\n",
      "9085/9085 [==============================] - 0s 20us/sample - loss: 0.0046 - mse: 0.0046 - val_loss: 0.0039 - val_mse: 0.0039\n",
      "Epoch 448/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0042 - mse: 0.0042 - val_loss: 0.0053 - val_mse: 0.0053\n",
      "Epoch 449/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0041 - mse: 0.0041 - val_loss: 0.0045 - val_mse: 0.0045\n",
      "Epoch 450/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0034 - val_mse: 0.0034\n",
      "Epoch 451/500\n",
      "9085/9085 [==============================] - 0s 20us/sample - loss: 0.0038 - mse: 0.0038 - val_loss: 0.0063 - val_mse: 0.0063\n",
      "Epoch 452/500\n",
      "9085/9085 [==============================] - 0s 20us/sample - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0037 - val_mse: 0.0037\n",
      "Epoch 453/500\n",
      "9085/9085 [==============================] - 0s 20us/sample - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0032 - val_mse: 0.0032\n",
      "Epoch 454/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0036 - mse: 0.0036 - val_loss: 0.0039 - val_mse: 0.0039\n",
      "Epoch 455/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0071 - val_mse: 0.0071\n",
      "Epoch 456/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0045 - val_mse: 0.0045\n",
      "Epoch 457/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0026 - val_mse: 0.0026\n",
      "Epoch 458/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0037 - mse: 0.0037 - val_loss: 0.0029 - val_mse: 0.0029\n",
      "Epoch 459/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0036 - mse: 0.0036 - val_loss: 0.0048 - val_mse: 0.0048\n",
      "Epoch 460/500\n",
      "9085/9085 [==============================] - 0s 25us/sample - loss: 0.0042 - mse: 0.0042 - val_loss: 0.0031 - val_mse: 0.0031\n",
      "Epoch 461/500\n",
      "9085/9085 [==============================] - 0s 25us/sample - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0023 - val_mse: 0.0023\n",
      "Epoch 462/500\n",
      "9085/9085 [==============================] - 0s 30us/sample - loss: 0.0039 - mse: 0.0039 - val_loss: 0.0038 - val_mse: 0.0038\n",
      "Epoch 463/500\n",
      "9085/9085 [==============================] - 0s 25us/sample - loss: 0.0038 - mse: 0.0038 - val_loss: 0.0041 - val_mse: 0.0041\n",
      "Epoch 464/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0042 - mse: 0.0042 - val_loss: 0.0057 - val_mse: 0.0057\n",
      "Epoch 465/500\n",
      "9085/9085 [==============================] - 0s 25us/sample - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0033 - val_mse: 0.0033\n",
      "Epoch 466/500\n",
      "9085/9085 [==============================] - 0s 29us/sample - loss: 0.0038 - mse: 0.0038 - val_loss: 0.0040 - val_mse: 0.0040\n",
      "Epoch 467/500\n",
      "9085/9085 [==============================] - 0s 30us/sample - loss: 0.0043 - mse: 0.0043 - val_loss: 0.0040 - val_mse: 0.0040\n",
      "Epoch 468/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0042 - mse: 0.0042 - val_loss: 0.0033 - val_mse: 0.0033\n",
      "Epoch 469/500\n",
      "9085/9085 [==============================] - 0s 21us/sample - loss: 0.0037 - mse: 0.0037 - val_loss: 0.0035 - val_mse: 0.0035\n",
      "Epoch 470/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0038 - mse: 0.0038 - val_loss: 0.0036 - val_mse: 0.0036\n",
      "Epoch 471/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0046 - val_mse: 0.0046\n",
      "Epoch 472/500\n",
      "9085/9085 [==============================] - 0s 20us/sample - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0042 - val_mse: 0.0042\n",
      "Epoch 473/500\n",
      "9085/9085 [==============================] - 0s 20us/sample - loss: 0.0037 - mse: 0.0037 - val_loss: 0.0046 - val_mse: 0.0046\n",
      "Epoch 474/500\n",
      "9085/9085 [==============================] - 0s 20us/sample - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0034 - val_mse: 0.0034\n",
      "Epoch 475/500\n",
      "9085/9085 [==============================] - 0s 21us/sample - loss: 0.0036 - mse: 0.0036 - val_loss: 0.0034 - val_mse: 0.0034\n",
      "Epoch 476/500\n",
      "9085/9085 [==============================] - 0s 20us/sample - loss: 0.0037 - mse: 0.0037 - val_loss: 0.0038 - val_mse: 0.0038\n",
      "Epoch 477/500\n",
      "9085/9085 [==============================] - 0s 20us/sample - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0033 - val_mse: 0.0033\n",
      "Epoch 478/500\n",
      "9085/9085 [==============================] - 0s 20us/sample - loss: 0.0037 - mse: 0.0037 - val_loss: 0.0031 - val_mse: 0.0031\n",
      "Epoch 479/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0038 - mse: 0.0038 - val_loss: 0.0057 - val_mse: 0.0057\n",
      "Epoch 480/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0038 - mse: 0.0038 - val_loss: 0.0046 - val_mse: 0.0046\n",
      "Epoch 481/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0042 - mse: 0.0042 - val_loss: 0.0028 - val_mse: 0.0028\n",
      "Epoch 482/500\n",
      "9085/9085 [==============================] - 0s 21us/sample - loss: 0.0041 - mse: 0.0041 - val_loss: 0.0040 - val_mse: 0.0040\n",
      "Epoch 483/500\n",
      "9085/9085 [==============================] - 0s 21us/sample - loss: 0.0037 - mse: 0.0037 - val_loss: 0.0040 - val_mse: 0.0040\n",
      "Epoch 484/500\n",
      "9085/9085 [==============================] - 0s 20us/sample - loss: 0.0038 - mse: 0.0038 - val_loss: 0.0032 - val_mse: 0.0032\n",
      "Epoch 485/500\n",
      "9085/9085 [==============================] - 0s 20us/sample - loss: 0.0037 - mse: 0.0037 - val_loss: 0.0029 - val_mse: 0.0029\n",
      "Epoch 486/500\n",
      "9085/9085 [==============================] - 0s 20us/sample - loss: 0.0036 - mse: 0.0036 - val_loss: 0.0034 - val_mse: 0.0034\n",
      "Epoch 487/500\n",
      "9085/9085 [==============================] - 0s 20us/sample - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0037 - val_mse: 0.0037\n",
      "Epoch 488/500\n",
      "9085/9085 [==============================] - 0s 20us/sample - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0040 - val_mse: 0.0040\n",
      "Epoch 489/500\n",
      "9085/9085 [==============================] - 0s 20us/sample - loss: 0.0038 - mse: 0.0038 - val_loss: 0.0033 - val_mse: 0.0033\n",
      "Epoch 490/500\n",
      "9085/9085 [==============================] - 0s 20us/sample - loss: 0.0042 - mse: 0.0042 - val_loss: 0.0028 - val_mse: 0.0028\n",
      "Epoch 491/500\n",
      "9085/9085 [==============================] - 0s 20us/sample - loss: 0.0043 - mse: 0.0043 - val_loss: 0.0044 - val_mse: 0.0044\n",
      "Epoch 492/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0041 - mse: 0.0041 - val_loss: 0.0053 - val_mse: 0.0053\n",
      "Epoch 493/500\n",
      "9085/9085 [==============================] - 0s 27us/sample - loss: 0.0041 - mse: 0.0041 - val_loss: 0.0030 - val_mse: 0.0030\n",
      "Epoch 494/500\n",
      "9085/9085 [==============================] - 0s 26us/sample - loss: 0.0039 - mse: 0.0039 - val_loss: 0.0039 - val_mse: 0.0039\n",
      "Epoch 495/500\n",
      "9085/9085 [==============================] - 0s 25us/sample - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0032 - val_mse: 0.0032\n",
      "Epoch 496/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0038 - mse: 0.0038 - val_loss: 0.0031 - val_mse: 0.0031\n",
      "Epoch 497/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0037 - mse: 0.0037 - val_loss: 0.0030 - val_mse: 0.0030\n",
      "Epoch 498/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0039 - mse: 0.0039 - val_loss: 0.0037 - val_mse: 0.0037\n",
      "Epoch 499/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0036 - mse: 0.0036 - val_loss: 0.0032 - val_mse: 0.0032\n",
      "Epoch 500/500\n",
      "9085/9085 [==============================] - 0s 25us/sample - loss: 0.0043 - mse: 0.0043 - val_loss: 0.0034 - val_mse: 0.0034\n",
      "[0.98478179 0.9402065  0.94714219]\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "inputs = Input(shape=(input_size,))\n",
    "f = Dense(24,activation='relu')(inputs)\n",
    "f = Dense(24,activation='relu')(f)\n",
    "f = Dense(24,activation='relu')(f)\n",
    "outputs = Dense(3)(f)\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "model.compile('Adam', loss=\"mse\", metrics=[\"mse\"])\n",
    "\n",
    "print('fitting')\n",
    "\n",
    "# Fit model\n",
    "model.fit(x_train, y_train, validation_split=0.05, epochs=500,\n",
    "          callbacks=[EarlyStopping(patience=70)], verbose=0)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_naive = model.predict(x_test)\n",
    "\n",
    "\n",
    "# Calculate the quality of the predictions with the r2 score\n",
    "r2 = r2_score(y_test.values, y_pred_naive, multioutput=\"raw_values\")\n",
    "\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Physics-guided multi-target regressor\n",
    "\n",
    "We now attempt to impose knowledge of mass conservation on our model. In order to acheive this, we need to predict all three masses simultaneously using a multi-target regressor. We will still use the same 12 input features, but this time we will be predicting three output targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9085 samples, validate on 479 samples\n",
      "Epoch 1/500\n",
      "9085/9085 [==============================] - 0s 50us/sample - loss: 34.9316 - mse: 32.3269 - val_loss: 2.4925 - val_mse: 1.7246\n",
      "Epoch 2/500\n",
      "1760/9085 [====>.........................] - ETA: 0s - loss: 2.2012 - mse: 1.5212"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seriousmaria/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9085/9085 [==============================] - 0s 31us/sample - loss: 1.6479 - mse: 1.1058 - val_loss: 1.1539 - val_mse: 0.7319\n",
      "Epoch 3/500\n",
      "9085/9085 [==============================] - 0s 28us/sample - loss: 0.8417 - mse: 0.5244 - val_loss: 0.6740 - val_mse: 0.4085\n",
      "Epoch 4/500\n",
      "9085/9085 [==============================] - 0s 28us/sample - loss: 0.5252 - mse: 0.3260 - val_loss: 0.4886 - val_mse: 0.2993\n",
      "Epoch 5/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.4135 - mse: 0.2601 - val_loss: 0.3920 - val_mse: 0.2535\n",
      "Epoch 6/500\n",
      "9085/9085 [==============================] - 0s 25us/sample - loss: 0.3438 - mse: 0.2200 - val_loss: 0.3334 - val_mse: 0.2154\n",
      "Epoch 7/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.3033 - mse: 0.1970 - val_loss: 0.3038 - val_mse: 0.1963\n",
      "Epoch 8/500\n",
      "9085/9085 [==============================] - 0s 25us/sample - loss: 0.2768 - mse: 0.1809 - val_loss: 0.2612 - val_mse: 0.1748\n",
      "Epoch 9/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.2563 - mse: 0.1692 - val_loss: 0.2592 - val_mse: 0.1799\n",
      "Epoch 10/500\n",
      "9085/9085 [==============================] - 0s 28us/sample - loss: 0.2398 - mse: 0.1597 - val_loss: 0.2379 - val_mse: 0.1545\n",
      "Epoch 11/500\n",
      "9085/9085 [==============================] - 0s 26us/sample - loss: 0.2236 - mse: 0.1501 - val_loss: 0.2161 - val_mse: 0.1579\n",
      "Epoch 12/500\n",
      "9085/9085 [==============================] - 0s 26us/sample - loss: 0.2139 - mse: 0.1442 - val_loss: 0.2094 - val_mse: 0.1395\n",
      "Epoch 13/500\n",
      "9085/9085 [==============================] - 0s 25us/sample - loss: 0.2040 - mse: 0.1381 - val_loss: 0.2071 - val_mse: 0.1389\n",
      "Epoch 14/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.1921 - mse: 0.1308 - val_loss: 0.1804 - val_mse: 0.1262\n",
      "Epoch 15/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.1846 - mse: 0.1258 - val_loss: 0.2043 - val_mse: 0.1269\n",
      "Epoch 16/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.1727 - mse: 0.1181 - val_loss: 0.1822 - val_mse: 0.1171\n",
      "Epoch 17/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.1677 - mse: 0.1150 - val_loss: 0.1584 - val_mse: 0.1143\n",
      "Epoch 18/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.1628 - mse: 0.1113 - val_loss: 0.1556 - val_mse: 0.1096\n",
      "Epoch 19/500\n",
      "9085/9085 [==============================] - 0s 25us/sample - loss: 0.1566 - mse: 0.1073 - val_loss: 0.1506 - val_mse: 0.1055\n",
      "Epoch 20/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.1456 - mse: 0.1008 - val_loss: 0.1391 - val_mse: 0.0981\n",
      "Epoch 21/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.1533 - mse: 0.1039 - val_loss: 0.1575 - val_mse: 0.0914\n",
      "Epoch 22/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.1429 - mse: 0.0984 - val_loss: 0.1440 - val_mse: 0.1104\n",
      "Epoch 23/500\n",
      "9085/9085 [==============================] - 0s 27us/sample - loss: 0.1316 - mse: 0.0924 - val_loss: 0.1229 - val_mse: 0.0905\n",
      "Epoch 24/500\n",
      "9085/9085 [==============================] - 0s 43us/sample - loss: 0.1326 - mse: 0.0930 - val_loss: 0.1350 - val_mse: 0.1152\n",
      "Epoch 25/500\n",
      "9085/9085 [==============================] - 0s 39us/sample - loss: 0.1272 - mse: 0.0903 - val_loss: 0.1193 - val_mse: 0.0949\n",
      "Epoch 26/500\n",
      "9085/9085 [==============================] - 1s 83us/sample - loss: 0.1236 - mse: 0.0888 - val_loss: 0.1129 - val_mse: 0.0857\n",
      "Epoch 27/500\n",
      "9085/9085 [==============================] - 0s 45us/sample - loss: 0.1192 - mse: 0.0864 - val_loss: 0.1111 - val_mse: 0.0854\n",
      "Epoch 28/500\n",
      "9085/9085 [==============================] - 0s 39us/sample - loss: 0.1062 - mse: 0.0807 - val_loss: 0.1043 - val_mse: 0.0827\n",
      "Epoch 29/500\n",
      "9085/9085 [==============================] - 0s 30us/sample - loss: 0.1087 - mse: 0.0817 - val_loss: 0.1273 - val_mse: 0.0849\n",
      "Epoch 30/500\n",
      "9085/9085 [==============================] - 0s 35us/sample - loss: 0.1062 - mse: 0.0813 - val_loss: 0.1069 - val_mse: 0.0771\n",
      "Epoch 31/500\n",
      "9085/9085 [==============================] - 0s 30us/sample - loss: 0.1071 - mse: 0.0817 - val_loss: 0.0929 - val_mse: 0.0785\n",
      "Epoch 32/500\n",
      "9085/9085 [==============================] - 0s 27us/sample - loss: 0.0979 - mse: 0.0779 - val_loss: 0.1229 - val_mse: 0.0788\n",
      "Epoch 33/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0951 - mse: 0.0767 - val_loss: 0.0921 - val_mse: 0.0747\n",
      "Epoch 34/500\n",
      "9085/9085 [==============================] - 0s 25us/sample - loss: 0.0926 - mse: 0.0754 - val_loss: 0.1105 - val_mse: 0.0974\n",
      "Epoch 35/500\n",
      "9085/9085 [==============================] - 0s 31us/sample - loss: 0.0946 - mse: 0.0765 - val_loss: 0.0918 - val_mse: 0.0800\n",
      "Epoch 36/500\n",
      "9085/9085 [==============================] - 0s 25us/sample - loss: 0.0900 - mse: 0.0745 - val_loss: 0.1051 - val_mse: 0.0864\n",
      "Epoch 37/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0908 - mse: 0.0748 - val_loss: 0.1034 - val_mse: 0.0898\n",
      "Epoch 38/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0882 - mse: 0.0739 - val_loss: 0.0828 - val_mse: 0.0718\n",
      "Epoch 39/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0872 - mse: 0.0733 - val_loss: 0.0858 - val_mse: 0.0717\n",
      "Epoch 40/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0827 - mse: 0.0709 - val_loss: 0.0810 - val_mse: 0.0721\n",
      "Epoch 41/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0860 - mse: 0.0724 - val_loss: 0.1019 - val_mse: 0.0927\n",
      "Epoch 42/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0850 - mse: 0.0719 - val_loss: 0.0766 - val_mse: 0.0728\n",
      "Epoch 43/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0836 - mse: 0.0707 - val_loss: 0.1241 - val_mse: 0.0771\n",
      "Epoch 44/500\n",
      "9085/9085 [==============================] - 0s 26us/sample - loss: 0.0817 - mse: 0.0702 - val_loss: 0.0814 - val_mse: 0.0756\n",
      "Epoch 45/500\n",
      "9085/9085 [==============================] - 0s 26us/sample - loss: 0.0850 - mse: 0.0718 - val_loss: 0.0965 - val_mse: 0.0711\n",
      "Epoch 46/500\n",
      "9085/9085 [==============================] - 0s 26us/sample - loss: 0.0771 - mse: 0.0678 - val_loss: 0.0824 - val_mse: 0.0703\n",
      "Epoch 47/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0803 - mse: 0.0695 - val_loss: 0.0774 - val_mse: 0.0693\n",
      "Epoch 48/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0776 - mse: 0.0679 - val_loss: 0.0845 - val_mse: 0.0723\n",
      "Epoch 49/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0788 - mse: 0.0684 - val_loss: 0.0924 - val_mse: 0.0810\n",
      "Epoch 50/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0801 - mse: 0.0693 - val_loss: 0.0800 - val_mse: 0.0748\n",
      "Epoch 51/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0815 - mse: 0.0698 - val_loss: 0.0889 - val_mse: 0.0824\n",
      "Epoch 52/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0740 - mse: 0.0664 - val_loss: 0.0857 - val_mse: 0.0774\n",
      "Epoch 53/500\n",
      "9085/9085 [==============================] - 0s 32us/sample - loss: 0.0776 - mse: 0.0679 - val_loss: 0.0923 - val_mse: 0.0845\n",
      "Epoch 54/500\n",
      "9085/9085 [==============================] - 0s 47us/sample - loss: 0.0765 - mse: 0.0675 - val_loss: 0.0974 - val_mse: 0.0866\n",
      "Epoch 55/500\n",
      "9085/9085 [==============================] - 0s 27us/sample - loss: 0.0727 - mse: 0.0657 - val_loss: 0.0731 - val_mse: 0.0676\n",
      "Epoch 56/500\n",
      "9085/9085 [==============================] - 0s 27us/sample - loss: 0.0740 - mse: 0.0660 - val_loss: 0.0734 - val_mse: 0.0692\n",
      "Epoch 57/500\n",
      "9085/9085 [==============================] - 0s 29us/sample - loss: 0.0737 - mse: 0.0660 - val_loss: 0.0727 - val_mse: 0.0695\n",
      "Epoch 58/500\n",
      "9085/9085 [==============================] - 0s 31us/sample - loss: 0.0773 - mse: 0.0678 - val_loss: 0.1006 - val_mse: 0.0699\n",
      "Epoch 59/500\n",
      "9085/9085 [==============================] - 0s 27us/sample - loss: 0.0727 - mse: 0.0657 - val_loss: 0.0725 - val_mse: 0.0681\n",
      "Epoch 60/500\n",
      "9085/9085 [==============================] - 0s 29us/sample - loss: 0.0772 - mse: 0.0677 - val_loss: 0.0741 - val_mse: 0.0705\n",
      "Epoch 61/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9085/9085 [==============================] - 0s 50us/sample - loss: 0.0721 - mse: 0.0655 - val_loss: 0.0692 - val_mse: 0.0661\n",
      "Epoch 62/500\n",
      "9085/9085 [==============================] - 0s 41us/sample - loss: 0.0682 - mse: 0.0631 - val_loss: 0.0747 - val_mse: 0.0702\n",
      "Epoch 63/500\n",
      "9085/9085 [==============================] - 0s 28us/sample - loss: 0.0747 - mse: 0.0662 - val_loss: 0.0698 - val_mse: 0.0657\n",
      "Epoch 64/500\n",
      "9085/9085 [==============================] - 0s 27us/sample - loss: 0.0703 - mse: 0.0638 - val_loss: 0.0767 - val_mse: 0.0667\n",
      "Epoch 65/500\n",
      "9085/9085 [==============================] - 0s 27us/sample - loss: 0.0675 - mse: 0.0614 - val_loss: 0.0685 - val_mse: 0.0651\n",
      "Epoch 66/500\n",
      "9085/9085 [==============================] - 0s 28us/sample - loss: 0.0681 - mse: 0.0606 - val_loss: 0.0735 - val_mse: 0.0676\n",
      "Epoch 67/500\n",
      "9085/9085 [==============================] - 0s 25us/sample - loss: 0.0666 - mse: 0.0590 - val_loss: 0.0687 - val_mse: 0.0621\n",
      "Epoch 68/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0655 - mse: 0.0582 - val_loss: 0.0658 - val_mse: 0.0620\n",
      "Epoch 69/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0644 - mse: 0.0574 - val_loss: 0.0722 - val_mse: 0.0579\n",
      "Epoch 70/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0632 - mse: 0.0560 - val_loss: 0.0686 - val_mse: 0.0636\n",
      "Epoch 71/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0609 - mse: 0.0546 - val_loss: 0.0593 - val_mse: 0.0535\n",
      "Epoch 72/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0655 - mse: 0.0562 - val_loss: 0.0556 - val_mse: 0.0515\n",
      "Epoch 73/500\n",
      "9085/9085 [==============================] - 0s 25us/sample - loss: 0.0591 - mse: 0.0526 - val_loss: 0.0756 - val_mse: 0.0657\n",
      "Epoch 74/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0586 - mse: 0.0522 - val_loss: 0.0596 - val_mse: 0.0501\n",
      "Epoch 75/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0585 - mse: 0.0517 - val_loss: 0.0639 - val_mse: 0.0574\n",
      "Epoch 76/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0563 - mse: 0.0500 - val_loss: 0.0551 - val_mse: 0.0529\n",
      "Epoch 77/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0543 - mse: 0.0489 - val_loss: 0.0740 - val_mse: 0.0622\n",
      "Epoch 78/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0552 - mse: 0.0499 - val_loss: 0.0492 - val_mse: 0.0457\n",
      "Epoch 79/500\n",
      "9085/9085 [==============================] - 0s 31us/sample - loss: 0.0528 - mse: 0.0475 - val_loss: 0.0586 - val_mse: 0.0440\n",
      "Epoch 80/500\n",
      "9085/9085 [==============================] - 0s 31us/sample - loss: 0.0532 - mse: 0.0483 - val_loss: 0.0502 - val_mse: 0.0456\n",
      "Epoch 81/500\n",
      "9085/9085 [==============================] - 0s 27us/sample - loss: 0.0509 - mse: 0.0461 - val_loss: 0.0574 - val_mse: 0.0539\n",
      "Epoch 82/500\n",
      "9085/9085 [==============================] - 0s 27us/sample - loss: 0.0499 - mse: 0.0450 - val_loss: 0.0482 - val_mse: 0.0455\n",
      "Epoch 83/500\n",
      "9085/9085 [==============================] - 0s 26us/sample - loss: 0.0467 - mse: 0.0426 - val_loss: 0.0491 - val_mse: 0.0459\n",
      "Epoch 84/500\n",
      "9085/9085 [==============================] - 0s 28us/sample - loss: 0.0490 - mse: 0.0446 - val_loss: 0.0737 - val_mse: 0.0662\n",
      "Epoch 85/500\n",
      "9085/9085 [==============================] - 0s 27us/sample - loss: 0.0475 - mse: 0.0423 - val_loss: 0.0533 - val_mse: 0.0437\n",
      "Epoch 86/500\n",
      "9085/9085 [==============================] - 0s 26us/sample - loss: 0.0485 - mse: 0.0431 - val_loss: 0.0427 - val_mse: 0.0394\n",
      "Epoch 87/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0429 - mse: 0.0384 - val_loss: 0.0457 - val_mse: 0.0379\n",
      "Epoch 88/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0416 - mse: 0.0365 - val_loss: 0.0519 - val_mse: 0.0387\n",
      "Epoch 89/500\n",
      "9085/9085 [==============================] - 0s 25us/sample - loss: 0.0384 - mse: 0.0341 - val_loss: 0.0476 - val_mse: 0.0393\n",
      "Epoch 90/500\n",
      "9085/9085 [==============================] - 0s 25us/sample - loss: 0.0374 - mse: 0.0329 - val_loss: 0.0317 - val_mse: 0.0285\n",
      "Epoch 91/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0355 - mse: 0.0315 - val_loss: 0.0482 - val_mse: 0.0434\n",
      "Epoch 92/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0355 - mse: 0.0313 - val_loss: 0.0337 - val_mse: 0.0331\n",
      "Epoch 93/500\n",
      "9085/9085 [==============================] - 0s 25us/sample - loss: 0.0347 - mse: 0.0309 - val_loss: 0.0278 - val_mse: 0.0258\n",
      "Epoch 94/500\n",
      "9085/9085 [==============================] - 0s 26us/sample - loss: 0.0323 - mse: 0.0285 - val_loss: 0.0419 - val_mse: 0.0351\n",
      "Epoch 95/500\n",
      "9085/9085 [==============================] - 0s 25us/sample - loss: 0.0312 - mse: 0.0275 - val_loss: 0.0270 - val_mse: 0.0260\n",
      "Epoch 96/500\n",
      "9085/9085 [==============================] - 0s 26us/sample - loss: 0.0320 - mse: 0.0279 - val_loss: 0.0234 - val_mse: 0.0221\n",
      "Epoch 97/500\n",
      "9085/9085 [==============================] - 0s 26us/sample - loss: 0.0295 - mse: 0.0261 - val_loss: 0.0269 - val_mse: 0.0224\n",
      "Epoch 98/500\n",
      "9085/9085 [==============================] - 0s 26us/sample - loss: 0.0281 - mse: 0.0251 - val_loss: 0.0325 - val_mse: 0.0295\n",
      "Epoch 99/500\n",
      "9085/9085 [==============================] - 0s 27us/sample - loss: 0.0306 - mse: 0.0268 - val_loss: 0.0237 - val_mse: 0.0232\n",
      "Epoch 100/500\n",
      "9085/9085 [==============================] - 0s 27us/sample - loss: 0.0282 - mse: 0.0254 - val_loss: 0.0246 - val_mse: 0.0203\n",
      "Epoch 101/500\n",
      "9085/9085 [==============================] - 0s 26us/sample - loss: 0.0264 - mse: 0.0236 - val_loss: 0.0266 - val_mse: 0.0252\n",
      "Epoch 102/500\n",
      "9085/9085 [==============================] - 0s 27us/sample - loss: 0.0287 - mse: 0.0250 - val_loss: 0.0275 - val_mse: 0.0266\n",
      "Epoch 103/500\n",
      "9085/9085 [==============================] - 0s 26us/sample - loss: 0.0282 - mse: 0.0249 - val_loss: 0.0290 - val_mse: 0.0268\n",
      "Epoch 104/500\n",
      "9085/9085 [==============================] - 0s 26us/sample - loss: 0.0278 - mse: 0.0251 - val_loss: 0.0435 - val_mse: 0.0268\n",
      "Epoch 105/500\n",
      "9085/9085 [==============================] - 0s 25us/sample - loss: 0.0271 - mse: 0.0240 - val_loss: 0.0223 - val_mse: 0.0219\n",
      "Epoch 106/500\n",
      "9085/9085 [==============================] - 0s 25us/sample - loss: 0.0247 - mse: 0.0222 - val_loss: 0.0202 - val_mse: 0.0175\n",
      "Epoch 107/500\n",
      "9085/9085 [==============================] - 0s 30us/sample - loss: 0.0277 - mse: 0.0246 - val_loss: 0.0202 - val_mse: 0.0186\n",
      "Epoch 108/500\n",
      "9085/9085 [==============================] - 0s 28us/sample - loss: 0.0272 - mse: 0.0245 - val_loss: 0.0224 - val_mse: 0.0211\n",
      "Epoch 109/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0257 - mse: 0.0232 - val_loss: 0.0245 - val_mse: 0.0187\n",
      "Epoch 110/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0274 - mse: 0.0246 - val_loss: 0.0285 - val_mse: 0.0260\n",
      "Epoch 111/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0233 - mse: 0.0210 - val_loss: 0.0186 - val_mse: 0.0178\n",
      "Epoch 112/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0236 - mse: 0.0214 - val_loss: 0.0272 - val_mse: 0.0241\n",
      "Epoch 113/500\n",
      "9085/9085 [==============================] - 0s 26us/sample - loss: 0.0273 - mse: 0.0244 - val_loss: 0.0184 - val_mse: 0.0175\n",
      "Epoch 114/500\n",
      "9085/9085 [==============================] - 0s 25us/sample - loss: 0.0266 - mse: 0.0243 - val_loss: 0.0297 - val_mse: 0.0273\n",
      "Epoch 115/500\n",
      "9085/9085 [==============================] - 0s 26us/sample - loss: 0.0251 - mse: 0.0224 - val_loss: 0.0206 - val_mse: 0.0179\n",
      "Epoch 116/500\n",
      "9085/9085 [==============================] - 0s 27us/sample - loss: 0.0236 - mse: 0.0212 - val_loss: 0.0188 - val_mse: 0.0170\n",
      "Epoch 117/500\n",
      "9085/9085 [==============================] - 0s 32us/sample - loss: 0.0240 - mse: 0.0215 - val_loss: 0.0211 - val_mse: 0.0193\n",
      "Epoch 118/500\n",
      "9085/9085 [==============================] - 0s 25us/sample - loss: 0.0228 - mse: 0.0206 - val_loss: 0.0176 - val_mse: 0.0166\n",
      "Epoch 119/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0230 - mse: 0.0208 - val_loss: 0.0243 - val_mse: 0.0219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0230 - mse: 0.0209 - val_loss: 0.0188 - val_mse: 0.0180\n",
      "Epoch 121/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0227 - mse: 0.0205 - val_loss: 0.0203 - val_mse: 0.0187\n",
      "Epoch 122/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0232 - mse: 0.0209 - val_loss: 0.0198 - val_mse: 0.0189\n",
      "Epoch 123/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0249 - mse: 0.0226 - val_loss: 0.0263 - val_mse: 0.0251\n",
      "Epoch 124/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0222 - mse: 0.0203 - val_loss: 0.0223 - val_mse: 0.0176\n",
      "Epoch 125/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0209 - mse: 0.0190 - val_loss: 0.0208 - val_mse: 0.0194\n",
      "Epoch 126/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0213 - mse: 0.0194 - val_loss: 0.0213 - val_mse: 0.0205\n",
      "Epoch 127/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0220 - mse: 0.0196 - val_loss: 0.0206 - val_mse: 0.0194\n",
      "Epoch 128/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0216 - mse: 0.0196 - val_loss: 0.0194 - val_mse: 0.0182\n",
      "Epoch 129/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0215 - mse: 0.0199 - val_loss: 0.0182 - val_mse: 0.0160\n",
      "Epoch 130/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0209 - mse: 0.0191 - val_loss: 0.0224 - val_mse: 0.0210\n",
      "Epoch 131/500\n",
      "9085/9085 [==============================] - 0s 25us/sample - loss: 0.0230 - mse: 0.0206 - val_loss: 0.0239 - val_mse: 0.0225\n",
      "Epoch 132/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0221 - mse: 0.0203 - val_loss: 0.0240 - val_mse: 0.0190\n",
      "Epoch 133/500\n",
      "9085/9085 [==============================] - 0s 25us/sample - loss: 0.0214 - mse: 0.0196 - val_loss: 0.0187 - val_mse: 0.0180\n",
      "Epoch 134/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0210 - mse: 0.0188 - val_loss: 0.0256 - val_mse: 0.0154\n",
      "Epoch 135/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0202 - mse: 0.0183 - val_loss: 0.0172 - val_mse: 0.0169\n",
      "Epoch 136/500\n",
      "9085/9085 [==============================] - 1s 56us/sample - loss: 0.0199 - mse: 0.0182 - val_loss: 0.0167 - val_mse: 0.0165\n",
      "Epoch 137/500\n",
      "9085/9085 [==============================] - 0s 27us/sample - loss: 0.0215 - mse: 0.0195 - val_loss: 0.0225 - val_mse: 0.0212\n",
      "Epoch 138/500\n",
      "9085/9085 [==============================] - 0s 25us/sample - loss: 0.0196 - mse: 0.0178 - val_loss: 0.0194 - val_mse: 0.0191\n",
      "Epoch 139/500\n",
      "9085/9085 [==============================] - 0s 25us/sample - loss: 0.0192 - mse: 0.0177 - val_loss: 0.0216 - val_mse: 0.0204\n",
      "Epoch 140/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0206 - mse: 0.0190 - val_loss: 0.0173 - val_mse: 0.0166\n",
      "Epoch 141/500\n",
      "9085/9085 [==============================] - 0s 26us/sample - loss: 0.0202 - mse: 0.0183 - val_loss: 0.0184 - val_mse: 0.0179\n",
      "Epoch 142/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0182 - mse: 0.0166 - val_loss: 0.0188 - val_mse: 0.0177\n",
      "Epoch 143/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0201 - mse: 0.0184 - val_loss: 0.0193 - val_mse: 0.0166\n",
      "Epoch 144/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0192 - mse: 0.0174 - val_loss: 0.0302 - val_mse: 0.0291\n",
      "Epoch 145/500\n",
      "9085/9085 [==============================] - 0s 29us/sample - loss: 0.0184 - mse: 0.0169 - val_loss: 0.0174 - val_mse: 0.0167\n",
      "Epoch 146/500\n",
      "9085/9085 [==============================] - 0s 25us/sample - loss: 0.0178 - mse: 0.0164 - val_loss: 0.0185 - val_mse: 0.0176\n",
      "Epoch 147/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0174 - mse: 0.0159 - val_loss: 0.0148 - val_mse: 0.0140\n",
      "Epoch 148/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0175 - mse: 0.0162 - val_loss: 0.0153 - val_mse: 0.0148\n",
      "Epoch 149/500\n",
      "9085/9085 [==============================] - 0s 25us/sample - loss: 0.0170 - mse: 0.0155 - val_loss: 0.0146 - val_mse: 0.0141\n",
      "Epoch 150/500\n",
      "9085/9085 [==============================] - 0s 26us/sample - loss: 0.0187 - mse: 0.0169 - val_loss: 0.0229 - val_mse: 0.0219\n",
      "Epoch 151/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0170 - mse: 0.0155 - val_loss: 0.0134 - val_mse: 0.0123\n",
      "Epoch 152/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0161 - mse: 0.0148 - val_loss: 0.0174 - val_mse: 0.0162\n",
      "Epoch 153/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0158 - mse: 0.0144 - val_loss: 0.0151 - val_mse: 0.0144\n",
      "Epoch 154/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0164 - mse: 0.0151 - val_loss: 0.0142 - val_mse: 0.0138\n",
      "Epoch 155/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0170 - mse: 0.0153 - val_loss: 0.0242 - val_mse: 0.0236\n",
      "Epoch 156/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0155 - mse: 0.0142 - val_loss: 0.0250 - val_mse: 0.0248\n",
      "Epoch 157/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0177 - mse: 0.0161 - val_loss: 0.0190 - val_mse: 0.0184\n",
      "Epoch 158/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0160 - mse: 0.0145 - val_loss: 0.0116 - val_mse: 0.0104\n",
      "Epoch 159/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0147 - mse: 0.0138 - val_loss: 0.0140 - val_mse: 0.0131\n",
      "Epoch 160/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0159 - mse: 0.0146 - val_loss: 0.0166 - val_mse: 0.0162\n",
      "Epoch 161/500\n",
      "9085/9085 [==============================] - 0s 25us/sample - loss: 0.0152 - mse: 0.0139 - val_loss: 0.0230 - val_mse: 0.0222\n",
      "Epoch 162/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0148 - mse: 0.0135 - val_loss: 0.0135 - val_mse: 0.0127\n",
      "Epoch 163/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0154 - mse: 0.0142 - val_loss: 0.0152 - val_mse: 0.0147\n",
      "Epoch 164/500\n",
      "9085/9085 [==============================] - 0s 27us/sample - loss: 0.0159 - mse: 0.0143 - val_loss: 0.0168 - val_mse: 0.0162\n",
      "Epoch 165/500\n",
      "9085/9085 [==============================] - 0s 33us/sample - loss: 0.0145 - mse: 0.0133 - val_loss: 0.0114 - val_mse: 0.0105\n",
      "Epoch 166/500\n",
      "9085/9085 [==============================] - 0s 30us/sample - loss: 0.0142 - mse: 0.0129 - val_loss: 0.0207 - val_mse: 0.0200\n",
      "Epoch 167/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0137 - mse: 0.0128 - val_loss: 0.0102 - val_mse: 0.0096\n",
      "Epoch 168/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0140 - mse: 0.0125 - val_loss: 0.0160 - val_mse: 0.0157\n",
      "Epoch 169/500\n",
      "9085/9085 [==============================] - 0s 26us/sample - loss: 0.0145 - mse: 0.0133 - val_loss: 0.0188 - val_mse: 0.0104\n",
      "Epoch 170/500\n",
      "9085/9085 [==============================] - 0s 29us/sample - loss: 0.0150 - mse: 0.0139 - val_loss: 0.0123 - val_mse: 0.0119\n",
      "Epoch 171/500\n",
      "9085/9085 [==============================] - 0s 31us/sample - loss: 0.0133 - mse: 0.0122 - val_loss: 0.0140 - val_mse: 0.0138\n",
      "Epoch 172/500\n",
      "9085/9085 [==============================] - 0s 37us/sample - loss: 0.0121 - mse: 0.0113 - val_loss: 0.0111 - val_mse: 0.0105\n",
      "Epoch 173/500\n",
      "9085/9085 [==============================] - 0s 36us/sample - loss: 0.0143 - mse: 0.0129 - val_loss: 0.0115 - val_mse: 0.0097\n",
      "Epoch 174/500\n",
      "9085/9085 [==============================] - 0s 28us/sample - loss: 0.0130 - mse: 0.0120 - val_loss: 0.0122 - val_mse: 0.0085\n",
      "Epoch 175/500\n",
      "9085/9085 [==============================] - 0s 28us/sample - loss: 0.0130 - mse: 0.0118 - val_loss: 0.0104 - val_mse: 0.0096\n",
      "Epoch 176/500\n",
      "9085/9085 [==============================] - 0s 32us/sample - loss: 0.0119 - mse: 0.0112 - val_loss: 0.0301 - val_mse: 0.0284\n",
      "Epoch 177/500\n",
      "9085/9085 [==============================] - 0s 37us/sample - loss: 0.0126 - mse: 0.0116 - val_loss: 0.0126 - val_mse: 0.0122\n",
      "Epoch 178/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9085/9085 [==============================] - 0s 33us/sample - loss: 0.0134 - mse: 0.0121 - val_loss: 0.0101 - val_mse: 0.0096\n",
      "Epoch 179/500\n",
      "9085/9085 [==============================] - 0s 25us/sample - loss: 0.0128 - mse: 0.0116 - val_loss: 0.0105 - val_mse: 0.0095\n",
      "Epoch 180/500\n",
      "9085/9085 [==============================] - 0s 28us/sample - loss: 0.0121 - mse: 0.0111 - val_loss: 0.0206 - val_mse: 0.0198\n",
      "Epoch 181/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0132 - mse: 0.0122 - val_loss: 0.0123 - val_mse: 0.0121\n",
      "Epoch 182/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0118 - mse: 0.0110 - val_loss: 0.0106 - val_mse: 0.0106\n",
      "Epoch 183/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0117 - mse: 0.0109 - val_loss: 0.0085 - val_mse: 0.0073\n",
      "Epoch 184/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0117 - mse: 0.0109 - val_loss: 0.0143 - val_mse: 0.0141\n",
      "Epoch 185/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0115 - mse: 0.0106 - val_loss: 0.0072 - val_mse: 0.0068\n",
      "Epoch 186/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0101 - mse: 0.0094 - val_loss: 0.0148 - val_mse: 0.0146\n",
      "Epoch 187/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0114 - mse: 0.0105 - val_loss: 0.0077 - val_mse: 0.0076\n",
      "Epoch 188/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0118 - mse: 0.0110 - val_loss: 0.0095 - val_mse: 0.0091\n",
      "Epoch 189/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0108 - mse: 0.0101 - val_loss: 0.0071 - val_mse: 0.0068\n",
      "Epoch 190/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0127 - mse: 0.0117 - val_loss: 0.0100 - val_mse: 0.0097\n",
      "Epoch 191/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0109 - mse: 0.0102 - val_loss: 0.0070 - val_mse: 0.0069\n",
      "Epoch 192/500\n",
      "9085/9085 [==============================] - 0s 26us/sample - loss: 0.0121 - mse: 0.0113 - val_loss: 0.0081 - val_mse: 0.0080\n",
      "Epoch 193/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0111 - mse: 0.0103 - val_loss: 0.0070 - val_mse: 0.0068\n",
      "Epoch 194/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0098 - mse: 0.0093 - val_loss: 0.0092 - val_mse: 0.0091\n",
      "Epoch 195/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0109 - mse: 0.0101 - val_loss: 0.0307 - val_mse: 0.0302\n",
      "Epoch 196/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0119 - mse: 0.0107 - val_loss: 0.0096 - val_mse: 0.0095\n",
      "Epoch 197/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0115 - mse: 0.0107 - val_loss: 0.0063 - val_mse: 0.0061\n",
      "Epoch 198/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0113 - mse: 0.0106 - val_loss: 0.0112 - val_mse: 0.0107\n",
      "Epoch 199/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0104 - mse: 0.0098 - val_loss: 0.0096 - val_mse: 0.0094\n",
      "Epoch 200/500\n",
      "9085/9085 [==============================] - 0s 25us/sample - loss: 0.0105 - mse: 0.0098 - val_loss: 0.0127 - val_mse: 0.0120\n",
      "Epoch 201/500\n",
      "9085/9085 [==============================] - 0s 25us/sample - loss: 0.0103 - mse: 0.0096 - val_loss: 0.0077 - val_mse: 0.0070\n",
      "Epoch 202/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0099 - mse: 0.0092 - val_loss: 0.0085 - val_mse: 0.0084\n",
      "Epoch 203/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0103 - mse: 0.0098 - val_loss: 0.0105 - val_mse: 0.0100\n",
      "Epoch 204/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0114 - mse: 0.0107 - val_loss: 0.0134 - val_mse: 0.0129\n",
      "Epoch 205/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0105 - mse: 0.0097 - val_loss: 0.0085 - val_mse: 0.0082\n",
      "Epoch 206/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0103 - mse: 0.0096 - val_loss: 0.0100 - val_mse: 0.0059\n",
      "Epoch 207/500\n",
      "9085/9085 [==============================] - 0s 25us/sample - loss: 0.0096 - mse: 0.0091 - val_loss: 0.0173 - val_mse: 0.0164\n",
      "Epoch 208/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0107 - mse: 0.0100 - val_loss: 0.0083 - val_mse: 0.0081\n",
      "Epoch 209/500\n",
      "9085/9085 [==============================] - 0s 29us/sample - loss: 0.0111 - mse: 0.0102 - val_loss: 0.0077 - val_mse: 0.0075\n",
      "Epoch 210/500\n",
      "9085/9085 [==============================] - 0s 28us/sample - loss: 0.0094 - mse: 0.0088 - val_loss: 0.0128 - val_mse: 0.0123\n",
      "Epoch 211/500\n",
      "9085/9085 [==============================] - 0s 32us/sample - loss: 0.0097 - mse: 0.0092 - val_loss: 0.0103 - val_mse: 0.0102\n",
      "Epoch 212/500\n",
      "9085/9085 [==============================] - 0s 33us/sample - loss: 0.0101 - mse: 0.0096 - val_loss: 0.0124 - val_mse: 0.0123\n",
      "Epoch 213/500\n",
      "9085/9085 [==============================] - 1s 58us/sample - loss: 0.0099 - mse: 0.0094 - val_loss: 0.0073 - val_mse: 0.0072\n",
      "Epoch 214/500\n",
      "9085/9085 [==============================] - 0s 30us/sample - loss: 0.0093 - mse: 0.0087 - val_loss: 0.0086 - val_mse: 0.0085\n",
      "Epoch 215/500\n",
      "9085/9085 [==============================] - 0s 26us/sample - loss: 0.0099 - mse: 0.0093 - val_loss: 0.0069 - val_mse: 0.0069\n",
      "Epoch 216/500\n",
      "9085/9085 [==============================] - 0s 25us/sample - loss: 0.0096 - mse: 0.0090 - val_loss: 0.0107 - val_mse: 0.0104\n",
      "Epoch 217/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0096 - mse: 0.0092 - val_loss: 0.0167 - val_mse: 0.0158\n",
      "Epoch 218/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0114 - mse: 0.0107 - val_loss: 0.0135 - val_mse: 0.0126\n",
      "Epoch 219/500\n",
      "9085/9085 [==============================] - 0s 26us/sample - loss: 0.0089 - mse: 0.0085 - val_loss: 0.0066 - val_mse: 0.0066\n",
      "Epoch 220/500\n",
      "9085/9085 [==============================] - 0s 26us/sample - loss: 0.0083 - mse: 0.0079 - val_loss: 0.0070 - val_mse: 0.0069\n",
      "Epoch 221/500\n",
      "9085/9085 [==============================] - 0s 25us/sample - loss: 0.0094 - mse: 0.0088 - val_loss: 0.0085 - val_mse: 0.0082\n",
      "Epoch 222/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0102 - mse: 0.0097 - val_loss: 0.0198 - val_mse: 0.0193\n",
      "Epoch 223/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0100 - mse: 0.0093 - val_loss: 0.0065 - val_mse: 0.0060\n",
      "Epoch 224/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0101 - mse: 0.0094 - val_loss: 0.0066 - val_mse: 0.0065\n",
      "Epoch 225/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0088 - mse: 0.0084 - val_loss: 0.0096 - val_mse: 0.0094\n",
      "Epoch 226/500\n",
      "9085/9085 [==============================] - 0s 25us/sample - loss: 0.0096 - mse: 0.0090 - val_loss: 0.0151 - val_mse: 0.0148\n",
      "Epoch 227/500\n",
      "9085/9085 [==============================] - 0s 25us/sample - loss: 0.0091 - mse: 0.0086 - val_loss: 0.0092 - val_mse: 0.0090\n",
      "Epoch 228/500\n",
      "9085/9085 [==============================] - 0s 26us/sample - loss: 0.0094 - mse: 0.0089 - val_loss: 0.0074 - val_mse: 0.0062\n",
      "Epoch 229/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0081 - mse: 0.0078 - val_loss: 0.0096 - val_mse: 0.0095\n",
      "Epoch 230/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0093 - mse: 0.0087 - val_loss: 0.0063 - val_mse: 0.0063\n",
      "Epoch 231/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0085 - mse: 0.0081 - val_loss: 0.0108 - val_mse: 0.0105\n",
      "Epoch 232/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0103 - mse: 0.0098 - val_loss: 0.0172 - val_mse: 0.0080\n",
      "Epoch 233/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0085 - mse: 0.0081 - val_loss: 0.0094 - val_mse: 0.0089\n",
      "Epoch 234/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0095 - mse: 0.0089 - val_loss: 0.0067 - val_mse: 0.0064\n",
      "Epoch 235/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0079 - mse: 0.0075 - val_loss: 0.0078 - val_mse: 0.0077\n",
      "Epoch 236/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9085/9085 [==============================] - 0s 25us/sample - loss: 0.0094 - mse: 0.0090 - val_loss: 0.0073 - val_mse: 0.0071\n",
      "Epoch 237/500\n",
      "9085/9085 [==============================] - 0s 25us/sample - loss: 0.0089 - mse: 0.0085 - val_loss: 0.0073 - val_mse: 0.0062\n",
      "Epoch 238/500\n",
      "9085/9085 [==============================] - 0s 28us/sample - loss: 0.0085 - mse: 0.0080 - val_loss: 0.0053 - val_mse: 0.0051\n",
      "Epoch 239/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0096 - mse: 0.0089 - val_loss: 0.0067 - val_mse: 0.0066\n",
      "Epoch 240/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0090 - mse: 0.0087 - val_loss: 0.0072 - val_mse: 0.0070\n",
      "Epoch 241/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0078 - mse: 0.0074 - val_loss: 0.0132 - val_mse: 0.0128\n",
      "Epoch 242/500\n",
      "9085/9085 [==============================] - 0s 25us/sample - loss: 0.0090 - mse: 0.0084 - val_loss: 0.0114 - val_mse: 0.0114\n",
      "Epoch 243/500\n",
      "9085/9085 [==============================] - 0s 28us/sample - loss: 0.0082 - mse: 0.0078 - val_loss: 0.0082 - val_mse: 0.0081\n",
      "Epoch 244/500\n",
      "9085/9085 [==============================] - 0s 35us/sample - loss: 0.0091 - mse: 0.0085 - val_loss: 0.0079 - val_mse: 0.0077\n",
      "Epoch 245/500\n",
      "9085/9085 [==============================] - 0s 30us/sample - loss: 0.0078 - mse: 0.0074 - val_loss: 0.0102 - val_mse: 0.0063\n",
      "Epoch 246/500\n",
      "9085/9085 [==============================] - 0s 27us/sample - loss: 0.0089 - mse: 0.0083 - val_loss: 0.0073 - val_mse: 0.0058\n",
      "Epoch 247/500\n",
      "9085/9085 [==============================] - 0s 26us/sample - loss: 0.0093 - mse: 0.0088 - val_loss: 0.0089 - val_mse: 0.0070\n",
      "Epoch 248/500\n",
      "9085/9085 [==============================] - 0s 34us/sample - loss: 0.0086 - mse: 0.0083 - val_loss: 0.0087 - val_mse: 0.0084\n",
      "Epoch 249/500\n",
      "9085/9085 [==============================] - 0s 33us/sample - loss: 0.0073 - mse: 0.0069 - val_loss: 0.0062 - val_mse: 0.0061\n",
      "Epoch 250/500\n",
      "9085/9085 [==============================] - 0s 27us/sample - loss: 0.0078 - mse: 0.0075 - val_loss: 0.0071 - val_mse: 0.0070\n",
      "Epoch 251/500\n",
      "9085/9085 [==============================] - 0s 28us/sample - loss: 0.0072 - mse: 0.0069 - val_loss: 0.0070 - val_mse: 0.0069\n",
      "Epoch 252/500\n",
      "9085/9085 [==============================] - 0s 27us/sample - loss: 0.0086 - mse: 0.0081 - val_loss: 0.0062 - val_mse: 0.0055\n",
      "Epoch 253/500\n",
      "9085/9085 [==============================] - 0s 27us/sample - loss: 0.0085 - mse: 0.0081 - val_loss: 0.0115 - val_mse: 0.0114\n",
      "Epoch 254/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0087 - mse: 0.0082 - val_loss: 0.0090 - val_mse: 0.0087\n",
      "Epoch 255/500\n",
      "9085/9085 [==============================] - 0s 25us/sample - loss: 0.0073 - mse: 0.0070 - val_loss: 0.0085 - val_mse: 0.0083\n",
      "Epoch 256/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0084 - mse: 0.0079 - val_loss: 0.0055 - val_mse: 0.0055\n",
      "Epoch 257/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0073 - mse: 0.0070 - val_loss: 0.0055 - val_mse: 0.0053\n",
      "Epoch 258/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0079 - mse: 0.0075 - val_loss: 0.0051 - val_mse: 0.0043\n",
      "Epoch 259/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0086 - mse: 0.0082 - val_loss: 0.0051 - val_mse: 0.0049\n",
      "Epoch 260/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0069 - mse: 0.0067 - val_loss: 0.0172 - val_mse: 0.0172\n",
      "Epoch 261/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0079 - mse: 0.0076 - val_loss: 0.0072 - val_mse: 0.0071\n",
      "Epoch 262/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0070 - mse: 0.0068 - val_loss: 0.0043 - val_mse: 0.0041\n",
      "Epoch 263/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0097 - mse: 0.0091 - val_loss: 0.0134 - val_mse: 0.0134\n",
      "Epoch 264/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0070 - mse: 0.0066 - val_loss: 0.0140 - val_mse: 0.0135\n",
      "Epoch 265/500\n",
      "9085/9085 [==============================] - 0s 26us/sample - loss: 0.0077 - mse: 0.0073 - val_loss: 0.0077 - val_mse: 0.0076\n",
      "Epoch 266/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0085 - mse: 0.0079 - val_loss: 0.0071 - val_mse: 0.0068\n",
      "Epoch 267/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0078 - mse: 0.0074 - val_loss: 0.0131 - val_mse: 0.0126\n",
      "Epoch 268/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0072 - mse: 0.0068 - val_loss: 0.0061 - val_mse: 0.0060\n",
      "Epoch 269/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0076 - mse: 0.0074 - val_loss: 0.0075 - val_mse: 0.0074\n",
      "Epoch 270/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0073 - mse: 0.0069 - val_loss: 0.0107 - val_mse: 0.0104\n",
      "Epoch 271/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0079 - mse: 0.0076 - val_loss: 0.0078 - val_mse: 0.0072\n",
      "Epoch 272/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0087 - mse: 0.0081 - val_loss: 0.0093 - val_mse: 0.0089\n",
      "Epoch 273/500\n",
      "9085/9085 [==============================] - 0s 26us/sample - loss: 0.0083 - mse: 0.0077 - val_loss: 0.0079 - val_mse: 0.0075\n",
      "Epoch 274/500\n",
      "9085/9085 [==============================] - 0s 26us/sample - loss: 0.0070 - mse: 0.0067 - val_loss: 0.0079 - val_mse: 0.0078\n",
      "Epoch 275/500\n",
      "9085/9085 [==============================] - 0s 25us/sample - loss: 0.0071 - mse: 0.0067 - val_loss: 0.0140 - val_mse: 0.0137\n",
      "Epoch 276/500\n",
      "9085/9085 [==============================] - 0s 26us/sample - loss: 0.0068 - mse: 0.0066 - val_loss: 0.0049 - val_mse: 0.0048\n",
      "Epoch 277/500\n",
      "9085/9085 [==============================] - 0s 28us/sample - loss: 0.0084 - mse: 0.0078 - val_loss: 0.0092 - val_mse: 0.0092\n",
      "Epoch 278/500\n",
      "9085/9085 [==============================] - 0s 25us/sample - loss: 0.0077 - mse: 0.0074 - val_loss: 0.0060 - val_mse: 0.0060\n",
      "Epoch 279/500\n",
      "9085/9085 [==============================] - 0s 25us/sample - loss: 0.0069 - mse: 0.0066 - val_loss: 0.0063 - val_mse: 0.0063\n",
      "Epoch 280/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0074 - mse: 0.0070 - val_loss: 0.0047 - val_mse: 0.0046\n",
      "Epoch 281/500\n",
      "9085/9085 [==============================] - 0s 28us/sample - loss: 0.0067 - mse: 0.0064 - val_loss: 0.0096 - val_mse: 0.0095\n",
      "Epoch 282/500\n",
      "9085/9085 [==============================] - 0s 25us/sample - loss: 0.0077 - mse: 0.0073 - val_loss: 0.0091 - val_mse: 0.0090\n",
      "Epoch 283/500\n",
      "9085/9085 [==============================] - 0s 26us/sample - loss: 0.0074 - mse: 0.0071 - val_loss: 0.0045 - val_mse: 0.0044\n",
      "Epoch 284/500\n",
      "9085/9085 [==============================] - 0s 28us/sample - loss: 0.0077 - mse: 0.0073 - val_loss: 0.0047 - val_mse: 0.0046\n",
      "Epoch 285/500\n",
      "9085/9085 [==============================] - 0s 28us/sample - loss: 0.0071 - mse: 0.0068 - val_loss: 0.0094 - val_mse: 0.0093\n",
      "Epoch 286/500\n",
      "9085/9085 [==============================] - 0s 28us/sample - loss: 0.0081 - mse: 0.0076 - val_loss: 0.0134 - val_mse: 0.0133\n",
      "Epoch 287/500\n",
      "9085/9085 [==============================] - 0s 27us/sample - loss: 0.0077 - mse: 0.0073 - val_loss: 0.0088 - val_mse: 0.0087\n",
      "Epoch 288/500\n",
      "9085/9085 [==============================] - 0s 26us/sample - loss: 0.0065 - mse: 0.0062 - val_loss: 0.0047 - val_mse: 0.0047\n",
      "Epoch 289/500\n",
      "9085/9085 [==============================] - 0s 26us/sample - loss: 0.0069 - mse: 0.0065 - val_loss: 0.0055 - val_mse: 0.0050\n",
      "Epoch 290/500\n",
      "9085/9085 [==============================] - 0s 26us/sample - loss: 0.0064 - mse: 0.0062 - val_loss: 0.0097 - val_mse: 0.0094\n",
      "Epoch 291/500\n",
      "9085/9085 [==============================] - 0s 26us/sample - loss: 0.0074 - mse: 0.0069 - val_loss: 0.0051 - val_mse: 0.0050\n",
      "Epoch 292/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0072 - mse: 0.0068 - val_loss: 0.0045 - val_mse: 0.0044\n",
      "Epoch 293/500\n",
      "9085/9085 [==============================] - 0s 26us/sample - loss: 0.0065 - mse: 0.0061 - val_loss: 0.0075 - val_mse: 0.0073\n",
      "Epoch 294/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9085/9085 [==============================] - 0s 25us/sample - loss: 0.0069 - mse: 0.0066 - val_loss: 0.0054 - val_mse: 0.0053\n",
      "Epoch 295/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0074 - mse: 0.0069 - val_loss: 0.0044 - val_mse: 0.0036\n",
      "Epoch 296/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0069 - mse: 0.0066 - val_loss: 0.0054 - val_mse: 0.0053\n",
      "Epoch 297/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0060 - mse: 0.0058 - val_loss: 0.0039 - val_mse: 0.0039\n",
      "Epoch 298/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0095 - mse: 0.0090 - val_loss: 0.0123 - val_mse: 0.0091\n",
      "Epoch 299/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0071 - mse: 0.0068 - val_loss: 0.0059 - val_mse: 0.0058\n",
      "Epoch 300/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0067 - mse: 0.0064 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 301/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0068 - mse: 0.0065 - val_loss: 0.0056 - val_mse: 0.0055\n",
      "Epoch 302/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0074 - mse: 0.0069 - val_loss: 0.0053 - val_mse: 0.0051\n",
      "Epoch 303/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0061 - mse: 0.0059 - val_loss: 0.0092 - val_mse: 0.0085\n",
      "Epoch 304/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0069 - mse: 0.0066 - val_loss: 0.0046 - val_mse: 0.0045\n",
      "Epoch 305/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0071 - mse: 0.0067 - val_loss: 0.0047 - val_mse: 0.0046\n",
      "Epoch 306/500\n",
      "9085/9085 [==============================] - 0s 28us/sample - loss: 0.0077 - mse: 0.0074 - val_loss: 0.0056 - val_mse: 0.0046\n",
      "Epoch 307/500\n",
      "9085/9085 [==============================] - 0s 28us/sample - loss: 0.0066 - mse: 0.0064 - val_loss: 0.0089 - val_mse: 0.0089\n",
      "Epoch 308/500\n",
      "9085/9085 [==============================] - 0s 26us/sample - loss: 0.0076 - mse: 0.0072 - val_loss: 0.0055 - val_mse: 0.0050\n",
      "Epoch 309/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0063 - mse: 0.0060 - val_loss: 0.0065 - val_mse: 0.0063\n",
      "Epoch 310/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0076 - mse: 0.0073 - val_loss: 0.0067 - val_mse: 0.0067\n",
      "Epoch 311/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0072 - mse: 0.0069 - val_loss: 0.0062 - val_mse: 0.0061\n",
      "Epoch 312/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0064 - mse: 0.0062 - val_loss: 0.0080 - val_mse: 0.0079\n",
      "Epoch 313/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0072 - mse: 0.0068 - val_loss: 0.0054 - val_mse: 0.0053\n",
      "Epoch 314/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0067 - mse: 0.0064 - val_loss: 0.0091 - val_mse: 0.0088\n",
      "Epoch 315/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0069 - mse: 0.0065 - val_loss: 0.0048 - val_mse: 0.0047\n",
      "Epoch 316/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0065 - mse: 0.0062 - val_loss: 0.0091 - val_mse: 0.0090\n",
      "Epoch 317/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0076 - mse: 0.0073 - val_loss: 0.0051 - val_mse: 0.0043\n",
      "Epoch 318/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0062 - mse: 0.0060 - val_loss: 0.0039 - val_mse: 0.0038\n",
      "Epoch 319/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0065 - mse: 0.0063 - val_loss: 0.0122 - val_mse: 0.0121\n",
      "Epoch 320/500\n",
      "9085/9085 [==============================] - 0s 27us/sample - loss: 0.0063 - mse: 0.0060 - val_loss: 0.0041 - val_mse: 0.0040\n",
      "Epoch 321/500\n",
      "9085/9085 [==============================] - 0s 30us/sample - loss: 0.0071 - mse: 0.0066 - val_loss: 0.0041 - val_mse: 0.0041\n",
      "Epoch 322/500\n",
      "9085/9085 [==============================] - 0s 34us/sample - loss: 0.0081 - mse: 0.0077 - val_loss: 0.0064 - val_mse: 0.0064\n",
      "Epoch 323/500\n",
      "9085/9085 [==============================] - 0s 37us/sample - loss: 0.0065 - mse: 0.0063 - val_loss: 0.0063 - val_mse: 0.0056\n",
      "Epoch 324/500\n",
      "9085/9085 [==============================] - 0s 28us/sample - loss: 0.0073 - mse: 0.0068 - val_loss: 0.0053 - val_mse: 0.0052\n",
      "Epoch 325/500\n",
      "9085/9085 [==============================] - 0s 27us/sample - loss: 0.0069 - mse: 0.0066 - val_loss: 0.0101 - val_mse: 0.0100\n",
      "Epoch 326/500\n",
      "9085/9085 [==============================] - 0s 26us/sample - loss: 0.0058 - mse: 0.0055 - val_loss: 0.0039 - val_mse: 0.0037\n",
      "Epoch 327/500\n",
      "9085/9085 [==============================] - 0s 26us/sample - loss: 0.0070 - mse: 0.0067 - val_loss: 0.0067 - val_mse: 0.0053\n",
      "Epoch 328/500\n",
      "9085/9085 [==============================] - 0s 26us/sample - loss: 0.0074 - mse: 0.0070 - val_loss: 0.0096 - val_mse: 0.0094\n",
      "Epoch 329/500\n",
      "9085/9085 [==============================] - 0s 26us/sample - loss: 0.0063 - mse: 0.0060 - val_loss: 0.0055 - val_mse: 0.0055\n",
      "Epoch 330/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0063 - mse: 0.0061 - val_loss: 0.0059 - val_mse: 0.0058\n",
      "Epoch 331/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0065 - mse: 0.0061 - val_loss: 0.0062 - val_mse: 0.0060\n",
      "Epoch 332/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0070 - mse: 0.0066 - val_loss: 0.0079 - val_mse: 0.0079\n",
      "Epoch 333/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0069 - mse: 0.0066 - val_loss: 0.0048 - val_mse: 0.0048\n",
      "Epoch 334/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0067 - mse: 0.0065 - val_loss: 0.0044 - val_mse: 0.0043\n",
      "Epoch 335/500\n",
      "9085/9085 [==============================] - 0s 26us/sample - loss: 0.0068 - mse: 0.0064 - val_loss: 0.0068 - val_mse: 0.0068\n",
      "Epoch 336/500\n",
      "9085/9085 [==============================] - 0s 28us/sample - loss: 0.0075 - mse: 0.0071 - val_loss: 0.0052 - val_mse: 0.0048\n",
      "Epoch 337/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0061 - mse: 0.0058 - val_loss: 0.0075 - val_mse: 0.0071\n",
      "Epoch 338/500\n",
      "9085/9085 [==============================] - 0s 26us/sample - loss: 0.0065 - mse: 0.0062 - val_loss: 0.0050 - val_mse: 0.0050\n",
      "Epoch 339/500\n",
      "9085/9085 [==============================] - 0s 30us/sample - loss: 0.0069 - mse: 0.0066 - val_loss: 0.0048 - val_mse: 0.0046\n",
      "Epoch 340/500\n",
      "9085/9085 [==============================] - 0s 33us/sample - loss: 0.0062 - mse: 0.0061 - val_loss: 0.0052 - val_mse: 0.0051\n",
      "Epoch 341/500\n",
      "9085/9085 [==============================] - 0s 27us/sample - loss: 0.0056 - mse: 0.0054 - val_loss: 0.0075 - val_mse: 0.0073\n",
      "Epoch 342/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0064 - mse: 0.0062 - val_loss: 0.0062 - val_mse: 0.0061\n",
      "Epoch 343/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0066 - mse: 0.0063 - val_loss: 0.0040 - val_mse: 0.0039\n",
      "Epoch 344/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0060 - mse: 0.0058 - val_loss: 0.0098 - val_mse: 0.0097\n",
      "Epoch 345/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0063 - mse: 0.0060 - val_loss: 0.0050 - val_mse: 0.0038\n",
      "Epoch 346/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0065 - mse: 0.0062 - val_loss: 0.0062 - val_mse: 0.0044\n",
      "Epoch 347/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0063 - mse: 0.0059 - val_loss: 0.0047 - val_mse: 0.0046\n",
      "Epoch 348/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0066 - mse: 0.0062 - val_loss: 0.0051 - val_mse: 0.0050\n",
      "Epoch 349/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0067 - mse: 0.0063 - val_loss: 0.0052 - val_mse: 0.0050\n",
      "Epoch 350/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0068 - mse: 0.0064 - val_loss: 0.0033 - val_mse: 0.0033\n",
      "Epoch 351/500\n",
      "9085/9085 [==============================] - 0s 25us/sample - loss: 0.0059 - mse: 0.0057 - val_loss: 0.0049 - val_mse: 0.0048\n",
      "Epoch 352/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9085/9085 [==============================] - 0s 26us/sample - loss: 0.0073 - mse: 0.0069 - val_loss: 0.0054 - val_mse: 0.0053\n",
      "Epoch 353/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0062 - mse: 0.0059 - val_loss: 0.0071 - val_mse: 0.0071\n",
      "Epoch 354/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0060 - mse: 0.0057 - val_loss: 0.0036 - val_mse: 0.0036\n",
      "Epoch 355/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0058 - mse: 0.0056 - val_loss: 0.0040 - val_mse: 0.0040\n",
      "Epoch 356/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0065 - mse: 0.0062 - val_loss: 0.0040 - val_mse: 0.0039\n",
      "Epoch 357/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0065 - mse: 0.0061 - val_loss: 0.0038 - val_mse: 0.0037\n",
      "Epoch 358/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0067 - mse: 0.0063 - val_loss: 0.0037 - val_mse: 0.0036\n",
      "Epoch 359/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0058 - mse: 0.0056 - val_loss: 0.0052 - val_mse: 0.0051\n",
      "Epoch 360/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0066 - mse: 0.0062 - val_loss: 0.0079 - val_mse: 0.0075\n",
      "Epoch 361/500\n",
      "9085/9085 [==============================] - 0s 25us/sample - loss: 0.0063 - mse: 0.0060 - val_loss: 0.0038 - val_mse: 0.0037\n",
      "Epoch 362/500\n",
      "9085/9085 [==============================] - 0s 25us/sample - loss: 0.0064 - mse: 0.0062 - val_loss: 0.0054 - val_mse: 0.0051\n",
      "Epoch 363/500\n",
      "9085/9085 [==============================] - 0s 28us/sample - loss: 0.0057 - mse: 0.0055 - val_loss: 0.0063 - val_mse: 0.0060\n",
      "Epoch 364/500\n",
      "9085/9085 [==============================] - 0s 31us/sample - loss: 0.0064 - mse: 0.0062 - val_loss: 0.0043 - val_mse: 0.0041\n",
      "Epoch 365/500\n",
      "9085/9085 [==============================] - 0s 37us/sample - loss: 0.0062 - mse: 0.0059 - val_loss: 0.0041 - val_mse: 0.0041\n",
      "Epoch 366/500\n",
      "9085/9085 [==============================] - 0s 26us/sample - loss: 0.0063 - mse: 0.0059 - val_loss: 0.0070 - val_mse: 0.0069\n",
      "Epoch 367/500\n",
      "9085/9085 [==============================] - 0s 26us/sample - loss: 0.0057 - mse: 0.0055 - val_loss: 0.0087 - val_mse: 0.0086\n",
      "Epoch 368/500\n",
      "9085/9085 [==============================] - 0s 26us/sample - loss: 0.0067 - mse: 0.0064 - val_loss: 0.0035 - val_mse: 0.0035\n",
      "Epoch 369/500\n",
      "9085/9085 [==============================] - 0s 31us/sample - loss: 0.0058 - mse: 0.0055 - val_loss: 0.0044 - val_mse: 0.0044\n",
      "Epoch 370/500\n",
      "9085/9085 [==============================] - 0s 27us/sample - loss: 0.0057 - mse: 0.0055 - val_loss: 0.0044 - val_mse: 0.0044\n",
      "Epoch 371/500\n",
      "9085/9085 [==============================] - 0s 25us/sample - loss: 0.0066 - mse: 0.0062 - val_loss: 0.0040 - val_mse: 0.0040\n",
      "Epoch 372/500\n",
      "9085/9085 [==============================] - 0s 26us/sample - loss: 0.0064 - mse: 0.0061 - val_loss: 0.0106 - val_mse: 0.0098\n",
      "Epoch 373/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0070 - mse: 0.0067 - val_loss: 0.0040 - val_mse: 0.0040\n",
      "Epoch 374/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0058 - mse: 0.0056 - val_loss: 0.0050 - val_mse: 0.0050\n",
      "Epoch 375/500\n",
      "9085/9085 [==============================] - 0s 26us/sample - loss: 0.0064 - mse: 0.0061 - val_loss: 0.0051 - val_mse: 0.0050\n",
      "Epoch 376/500\n",
      "9085/9085 [==============================] - 0s 30us/sample - loss: 0.0056 - mse: 0.0054 - val_loss: 0.0035 - val_mse: 0.0035\n",
      "Epoch 377/500\n",
      "9085/9085 [==============================] - 0s 29us/sample - loss: 0.0066 - mse: 0.0062 - val_loss: 0.0068 - val_mse: 0.0063\n",
      "Epoch 378/500\n",
      "9085/9085 [==============================] - 0s 29us/sample - loss: 0.0069 - mse: 0.0065 - val_loss: 0.0059 - val_mse: 0.0059\n",
      "Epoch 379/500\n",
      "9085/9085 [==============================] - 0s 28us/sample - loss: 0.0069 - mse: 0.0066 - val_loss: 0.0058 - val_mse: 0.0058\n",
      "Epoch 380/500\n",
      "9085/9085 [==============================] - 0s 26us/sample - loss: 0.0057 - mse: 0.0054 - val_loss: 0.0040 - val_mse: 0.0040\n",
      "Epoch 381/500\n",
      "9085/9085 [==============================] - 0s 26us/sample - loss: 0.0053 - mse: 0.0051 - val_loss: 0.0035 - val_mse: 0.0034\n",
      "Epoch 382/500\n",
      "9085/9085 [==============================] - 0s 32us/sample - loss: 0.0056 - mse: 0.0054 - val_loss: 0.0123 - val_mse: 0.0120\n",
      "Epoch 383/500\n",
      "9085/9085 [==============================] - 0s 32us/sample - loss: 0.0068 - mse: 0.0065 - val_loss: 0.0036 - val_mse: 0.0034\n",
      "Epoch 384/500\n",
      "9085/9085 [==============================] - 0s 29us/sample - loss: 0.0057 - mse: 0.0054 - val_loss: 0.0040 - val_mse: 0.0039\n",
      "Epoch 385/500\n",
      "9085/9085 [==============================] - 0s 27us/sample - loss: 0.0066 - mse: 0.0064 - val_loss: 0.0090 - val_mse: 0.0089\n",
      "Epoch 386/500\n",
      "9085/9085 [==============================] - 0s 25us/sample - loss: 0.0065 - mse: 0.0061 - val_loss: 0.0053 - val_mse: 0.0051\n",
      "Epoch 387/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0053 - mse: 0.0051 - val_loss: 0.0035 - val_mse: 0.0035\n",
      "Epoch 388/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0048 - mse: 0.0046 - val_loss: 0.0049 - val_mse: 0.0048\n",
      "Epoch 389/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0064 - mse: 0.0060 - val_loss: 0.0038 - val_mse: 0.0033\n",
      "Epoch 390/500\n",
      "9085/9085 [==============================] - 0s 26us/sample - loss: 0.0054 - mse: 0.0052 - val_loss: 0.0040 - val_mse: 0.0031\n",
      "Epoch 391/500\n",
      "9085/9085 [==============================] - 0s 27us/sample - loss: 0.0061 - mse: 0.0058 - val_loss: 0.0031 - val_mse: 0.0031\n",
      "Epoch 392/500\n",
      "9085/9085 [==============================] - 0s 26us/sample - loss: 0.0062 - mse: 0.0060 - val_loss: 0.0086 - val_mse: 0.0034\n",
      "Epoch 393/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0063 - mse: 0.0060 - val_loss: 0.0052 - val_mse: 0.0049\n",
      "Epoch 394/500\n",
      "9085/9085 [==============================] - 0s 25us/sample - loss: 0.0062 - mse: 0.0061 - val_loss: 0.0048 - val_mse: 0.0048\n",
      "Epoch 395/500\n",
      "9085/9085 [==============================] - 0s 28us/sample - loss: 0.0059 - mse: 0.0057 - val_loss: 0.0038 - val_mse: 0.0036\n",
      "Epoch 396/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0064 - mse: 0.0061 - val_loss: 0.0052 - val_mse: 0.0052\n",
      "Epoch 397/500\n",
      "9085/9085 [==============================] - 0s 25us/sample - loss: 0.0059 - mse: 0.0057 - val_loss: 0.0046 - val_mse: 0.0043\n",
      "Epoch 398/500\n",
      "9085/9085 [==============================] - 0s 33us/sample - loss: 0.0061 - mse: 0.0059 - val_loss: 0.0045 - val_mse: 0.0042\n",
      "Epoch 399/500\n",
      "9085/9085 [==============================] - 0s 38us/sample - loss: 0.0062 - mse: 0.0059 - val_loss: 0.0053 - val_mse: 0.0052\n",
      "Epoch 400/500\n",
      "9085/9085 [==============================] - 0s 34us/sample - loss: 0.0060 - mse: 0.0058 - val_loss: 0.0046 - val_mse: 0.0045\n",
      "Epoch 401/500\n",
      "9085/9085 [==============================] - 0s 51us/sample - loss: 0.0051 - mse: 0.0050 - val_loss: 0.0069 - val_mse: 0.0069\n",
      "Epoch 402/500\n",
      "9085/9085 [==============================] - 0s 26us/sample - loss: 0.0063 - mse: 0.0060 - val_loss: 0.0080 - val_mse: 0.0080\n",
      "Epoch 403/500\n",
      "9085/9085 [==============================] - 0s 25us/sample - loss: 0.0061 - mse: 0.0059 - val_loss: 0.0051 - val_mse: 0.0050\n",
      "Epoch 404/500\n",
      "9085/9085 [==============================] - 0s 25us/sample - loss: 0.0058 - mse: 0.0056 - val_loss: 0.0070 - val_mse: 0.0069\n",
      "Epoch 405/500\n",
      "9085/9085 [==============================] - 0s 26us/sample - loss: 0.0062 - mse: 0.0058 - val_loss: 0.0055 - val_mse: 0.0055\n",
      "Epoch 406/500\n",
      "9085/9085 [==============================] - 0s 25us/sample - loss: 0.0060 - mse: 0.0058 - val_loss: 0.0046 - val_mse: 0.0045\n",
      "Epoch 407/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0059 - mse: 0.0057 - val_loss: 0.0053 - val_mse: 0.0052\n",
      "Epoch 408/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0059 - mse: 0.0056 - val_loss: 0.0085 - val_mse: 0.0084\n",
      "Epoch 409/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0053 - mse: 0.0051 - val_loss: 0.0037 - val_mse: 0.0036\n",
      "Epoch 410/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0060 - mse: 0.0058 - val_loss: 0.0077 - val_mse: 0.0030\n",
      "Epoch 411/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0057 - mse: 0.0054 - val_loss: 0.0054 - val_mse: 0.0053\n",
      "Epoch 412/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0061 - mse: 0.0058 - val_loss: 0.0073 - val_mse: 0.0072\n",
      "Epoch 413/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0069 - mse: 0.0066 - val_loss: 0.0100 - val_mse: 0.0099\n",
      "Epoch 414/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0065 - mse: 0.0062 - val_loss: 0.0061 - val_mse: 0.0061\n",
      "Epoch 415/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0053 - mse: 0.0052 - val_loss: 0.0041 - val_mse: 0.0041\n",
      "Epoch 416/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0055 - mse: 0.0053 - val_loss: 0.0035 - val_mse: 0.0034\n",
      "Epoch 417/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0058 - mse: 0.0054 - val_loss: 0.0069 - val_mse: 0.0068\n",
      "Epoch 418/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0055 - mse: 0.0053 - val_loss: 0.0035 - val_mse: 0.0034\n",
      "Epoch 419/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0070 - mse: 0.0067 - val_loss: 0.0035 - val_mse: 0.0034\n",
      "Epoch 420/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0054 - mse: 0.0051 - val_loss: 0.0031 - val_mse: 0.0031\n",
      "Epoch 421/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0054 - mse: 0.0051 - val_loss: 0.0048 - val_mse: 0.0048\n",
      "Epoch 422/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0054 - mse: 0.0052 - val_loss: 0.0054 - val_mse: 0.0054\n",
      "Epoch 423/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0058 - mse: 0.0055 - val_loss: 0.0047 - val_mse: 0.0046\n",
      "Epoch 424/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0055 - mse: 0.0052 - val_loss: 0.0051 - val_mse: 0.0050\n",
      "Epoch 425/500\n",
      "9085/9085 [==============================] - 0s 26us/sample - loss: 0.0059 - mse: 0.0057 - val_loss: 0.0031 - val_mse: 0.0030\n",
      "Epoch 426/500\n",
      "9085/9085 [==============================] - 0s 25us/sample - loss: 0.0056 - mse: 0.0053 - val_loss: 0.0044 - val_mse: 0.0044\n",
      "Epoch 427/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0053 - mse: 0.0051 - val_loss: 0.0037 - val_mse: 0.0036\n",
      "Epoch 428/500\n",
      "9085/9085 [==============================] - 0s 27us/sample - loss: 0.0053 - mse: 0.0051 - val_loss: 0.0043 - val_mse: 0.0043\n",
      "Epoch 429/500\n",
      "9085/9085 [==============================] - 1s 94us/sample - loss: 0.0058 - mse: 0.0056 - val_loss: 0.0052 - val_mse: 0.0052\n",
      "Epoch 430/500\n",
      "9085/9085 [==============================] - 0s 49us/sample - loss: 0.0057 - mse: 0.0055 - val_loss: 0.0033 - val_mse: 0.0032\n",
      "Epoch 431/500\n",
      "9085/9085 [==============================] - 0s 34us/sample - loss: 0.0058 - mse: 0.0056 - val_loss: 0.0036 - val_mse: 0.0035\n",
      "Epoch 432/500\n",
      "9085/9085 [==============================] - 0s 31us/sample - loss: 0.0048 - mse: 0.0047 - val_loss: 0.0049 - val_mse: 0.0049\n",
      "Epoch 433/500\n",
      "9085/9085 [==============================] - 0s 29us/sample - loss: 0.0060 - mse: 0.0058 - val_loss: 0.0033 - val_mse: 0.0032\n",
      "Epoch 434/500\n",
      "9085/9085 [==============================] - 0s 30us/sample - loss: 0.0058 - mse: 0.0055 - val_loss: 0.0047 - val_mse: 0.0042\n",
      "Epoch 435/500\n",
      "9085/9085 [==============================] - 0s 30us/sample - loss: 0.0052 - mse: 0.0050 - val_loss: 0.0041 - val_mse: 0.0040\n",
      "Epoch 436/500\n",
      "9085/9085 [==============================] - 0s 29us/sample - loss: 0.0063 - mse: 0.0060 - val_loss: 0.0037 - val_mse: 0.0035\n",
      "Epoch 437/500\n",
      "9085/9085 [==============================] - 0s 27us/sample - loss: 0.0060 - mse: 0.0057 - val_loss: 0.0040 - val_mse: 0.0040\n",
      "Epoch 438/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0066 - mse: 0.0064 - val_loss: 0.0055 - val_mse: 0.0055\n",
      "Epoch 439/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0050 - mse: 0.0048 - val_loss: 0.0037 - val_mse: 0.0037\n",
      "Epoch 440/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0056 - mse: 0.0054 - val_loss: 0.0061 - val_mse: 0.0061\n",
      "Epoch 441/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0057 - mse: 0.0054 - val_loss: 0.0069 - val_mse: 0.0069\n",
      "Epoch 442/500\n",
      "9085/9085 [==============================] - 0s 30us/sample - loss: 0.0061 - mse: 0.0058 - val_loss: 0.0042 - val_mse: 0.0041\n",
      "Epoch 443/500\n",
      "9085/9085 [==============================] - 0s 30us/sample - loss: 0.0056 - mse: 0.0054 - val_loss: 0.0042 - val_mse: 0.0042\n",
      "Epoch 444/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0054 - mse: 0.0052 - val_loss: 0.0037 - val_mse: 0.0037\n",
      "Epoch 445/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0050 - mse: 0.0049 - val_loss: 0.0033 - val_mse: 0.0031\n",
      "Epoch 446/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0053 - mse: 0.0051 - val_loss: 0.0057 - val_mse: 0.0056\n",
      "Epoch 447/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0060 - mse: 0.0058 - val_loss: 0.0094 - val_mse: 0.0093\n",
      "Epoch 448/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0063 - mse: 0.0060 - val_loss: 0.0047 - val_mse: 0.0047\n",
      "Epoch 449/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0057 - mse: 0.0055 - val_loss: 0.0092 - val_mse: 0.0091\n",
      "Epoch 450/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0055 - mse: 0.0053 - val_loss: 0.0041 - val_mse: 0.0041\n",
      "Epoch 451/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0062 - mse: 0.0060 - val_loss: 0.0061 - val_mse: 0.0055\n",
      "Epoch 452/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0055 - mse: 0.0053 - val_loss: 0.0047 - val_mse: 0.0046\n",
      "Epoch 453/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0050 - mse: 0.0048 - val_loss: 0.0046 - val_mse: 0.0045\n",
      "Epoch 454/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0058 - mse: 0.0055 - val_loss: 0.0041 - val_mse: 0.0041\n",
      "Epoch 455/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0050 - mse: 0.0048 - val_loss: 0.0063 - val_mse: 0.0058\n",
      "Epoch 456/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0062 - mse: 0.0059 - val_loss: 0.0048 - val_mse: 0.0042\n",
      "Epoch 457/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0048 - mse: 0.0047 - val_loss: 0.0071 - val_mse: 0.0066\n",
      "Epoch 458/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0063 - mse: 0.0060 - val_loss: 0.0043 - val_mse: 0.0042\n",
      "Epoch 459/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0048 - mse: 0.0046 - val_loss: 0.0076 - val_mse: 0.0076\n",
      "Epoch 460/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0053 - mse: 0.0050 - val_loss: 0.0050 - val_mse: 0.0049\n",
      "Epoch 461/500\n",
      "9085/9085 [==============================] - 0s 26us/sample - loss: 0.0062 - mse: 0.0059 - val_loss: 0.0038 - val_mse: 0.0038\n",
      "Epoch 462/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0059 - mse: 0.0058 - val_loss: 0.0063 - val_mse: 0.0062\n",
      "Epoch 463/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0057 - mse: 0.0055 - val_loss: 0.0048 - val_mse: 0.0045\n",
      "Epoch 464/500\n",
      "9085/9085 [==============================] - 0s 25us/sample - loss: 0.0057 - mse: 0.0054 - val_loss: 0.0044 - val_mse: 0.0043\n",
      "Epoch 465/500\n",
      "9085/9085 [==============================] - 0s 28us/sample - loss: 0.0055 - mse: 0.0052 - val_loss: 0.0042 - val_mse: 0.0035\n",
      "Epoch 466/500\n",
      "9085/9085 [==============================] - 0s 28us/sample - loss: 0.0051 - mse: 0.0049 - val_loss: 0.0058 - val_mse: 0.0049\n",
      "Epoch 467/500\n",
      "9085/9085 [==============================] - 0s 26us/sample - loss: 0.0049 - mse: 0.0047 - val_loss: 0.0067 - val_mse: 0.0067\n",
      "Epoch 468/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9085/9085 [==============================] - 0s 26us/sample - loss: 0.0054 - mse: 0.0052 - val_loss: 0.0051 - val_mse: 0.0050\n",
      "Epoch 469/500\n",
      "9085/9085 [==============================] - 0s 26us/sample - loss: 0.0050 - mse: 0.0047 - val_loss: 0.0065 - val_mse: 0.0065\n",
      "Epoch 470/500\n",
      "9085/9085 [==============================] - 0s 30us/sample - loss: 0.0052 - mse: 0.0050 - val_loss: 0.0032 - val_mse: 0.0032\n",
      "Epoch 471/500\n",
      "9085/9085 [==============================] - 0s 31us/sample - loss: 0.0054 - mse: 0.0051 - val_loss: 0.0047 - val_mse: 0.0047\n",
      "Epoch 472/500\n",
      "9085/9085 [==============================] - 1s 60us/sample - loss: 0.0057 - mse: 0.0055 - val_loss: 0.0066 - val_mse: 0.0065\n",
      "Epoch 473/500\n",
      "9085/9085 [==============================] - 0s 28us/sample - loss: 0.0051 - mse: 0.0049 - val_loss: 0.0036 - val_mse: 0.0034\n",
      "Epoch 474/500\n",
      "9085/9085 [==============================] - 0s 26us/sample - loss: 0.0054 - mse: 0.0052 - val_loss: 0.0039 - val_mse: 0.0038\n",
      "Epoch 475/500\n",
      "9085/9085 [==============================] - 0s 26us/sample - loss: 0.0048 - mse: 0.0047 - val_loss: 0.0042 - val_mse: 0.0034\n",
      "Epoch 476/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0058 - mse: 0.0055 - val_loss: 0.0046 - val_mse: 0.0046\n",
      "Epoch 477/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0051 - mse: 0.0049 - val_loss: 0.0037 - val_mse: 0.0037\n",
      "Epoch 478/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0054 - mse: 0.0053 - val_loss: 0.0043 - val_mse: 0.0043\n",
      "Epoch 479/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0053 - mse: 0.0052 - val_loss: 0.0052 - val_mse: 0.0047\n",
      "Epoch 480/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0052 - mse: 0.0050 - val_loss: 0.0044 - val_mse: 0.0043\n",
      "Epoch 481/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0053 - mse: 0.0051 - val_loss: 0.0046 - val_mse: 0.0045\n",
      "Epoch 482/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0059 - mse: 0.0057 - val_loss: 0.0031 - val_mse: 0.0028\n",
      "Epoch 483/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0055 - mse: 0.0053 - val_loss: 0.0055 - val_mse: 0.0055\n",
      "Epoch 484/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0054 - mse: 0.0051 - val_loss: 0.0034 - val_mse: 0.0033\n",
      "Epoch 485/500\n",
      "9085/9085 [==============================] - 0s 23us/sample - loss: 0.0051 - mse: 0.0049 - val_loss: 0.0057 - val_mse: 0.0057\n",
      "Epoch 486/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0054 - mse: 0.0052 - val_loss: 0.0037 - val_mse: 0.0030\n",
      "Epoch 487/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0053 - mse: 0.0051 - val_loss: 0.0034 - val_mse: 0.0034\n",
      "Epoch 488/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0052 - mse: 0.0049 - val_loss: 0.0039 - val_mse: 0.0038\n",
      "Epoch 489/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0054 - mse: 0.0052 - val_loss: 0.0075 - val_mse: 0.0074\n",
      "Epoch 490/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0053 - mse: 0.0051 - val_loss: 0.0034 - val_mse: 0.0028\n",
      "Epoch 491/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0049 - mse: 0.0047 - val_loss: 0.0030 - val_mse: 0.0028\n",
      "Epoch 492/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0059 - mse: 0.0057 - val_loss: 0.0030 - val_mse: 0.0029\n",
      "Epoch 493/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0047 - mse: 0.0046 - val_loss: 0.0065 - val_mse: 0.0065\n",
      "Epoch 494/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0057 - mse: 0.0055 - val_loss: 0.0044 - val_mse: 0.0043\n",
      "Epoch 495/500\n",
      "9085/9085 [==============================] - 0s 25us/sample - loss: 0.0052 - mse: 0.0050 - val_loss: 0.0064 - val_mse: 0.0060\n",
      "Epoch 496/500\n",
      "9085/9085 [==============================] - 0s 26us/sample - loss: 0.0056 - mse: 0.0053 - val_loss: 0.0032 - val_mse: 0.0031\n",
      "Epoch 497/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0049 - mse: 0.0048 - val_loss: 0.0031 - val_mse: 0.0031\n",
      "Epoch 498/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0053 - mse: 0.0052 - val_loss: 0.0042 - val_mse: 0.0040\n",
      "Epoch 499/500\n",
      "9085/9085 [==============================] - 0s 22us/sample - loss: 0.0059 - mse: 0.0056 - val_loss: 0.0041 - val_mse: 0.0037\n",
      "Epoch 500/500\n",
      "9085/9085 [==============================] - 0s 24us/sample - loss: 0.0047 - mse: 0.0045 - val_loss: 0.0033 - val_mse: 0.0033\n",
      "             lr_mass\t0.9754\n",
      "            slr_mass\t0.9228\n",
      "         debris_mass\t0.9402\n"
     ]
    }
   ],
   "source": [
    "# Define model (MLP) using Keras\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "\n",
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    "\n",
    "disable_eager_execution()\n",
    "\n",
    "\n",
    "# Define custom loss function\n",
    "def custom_loss_wrapper(input_tensor, overshoot_weight=0.5,positivity_weight=0.5):\n",
    "    @tf.autograph.experimental.do_not_convert\n",
    "    def custom_loss(y_true,y_pred):\n",
    "        s1 = input_tensor[:,0]\n",
    "        sum_of_mass = K.sum(y_pred,axis=1)\n",
    "        overshoot = K.mean(ReLU()(Lambda(lambda x: x)(Subtract()([sum_of_mass,s1]))))\n",
    "        positivity_lr = K.mean(ReLU()(Lambda(lambda x: -x)(y_pred[:,0])))\n",
    "        positivity_slr = K.mean(ReLU()(Lambda(lambda x: -x)(y_pred[:,1])))\n",
    "        positivity_debris = K.mean(ReLU()(Lambda(lambda x: -x)(y_pred[:,2])))\n",
    "        return keras.losses.mean_squared_error(y_true, y_pred) + overshoot_weight*overshoot\\\n",
    "                + positivity_weight*(positivity_lr + positivity_slr + positivity_debris)\n",
    "    return custom_loss\n",
    "\n",
    "\n",
    "# Define model\n",
    "inputs = Input(shape=(input_size,))\n",
    "f = Dense(24,activation='relu')(inputs)\n",
    "f = Dense(24,activation='relu')(f)\n",
    "f = Dense(24,activation='relu')(f)\n",
    "outputs = Dense(3)(f)\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "model.compile('Adam', loss=custom_loss_wrapper(inputs), metrics=[\"mse\"])\n",
    "\n",
    "\n",
    "# Fit model\n",
    "model.fit(x_train, y_train, validation_split=0.05, epochs=500,\n",
    "          callbacks=[EarlyStopping(patience=70)], verbose=0)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_phys = model.predict(x_test)\n",
    "\n",
    "\n",
    "# Calculate the quality of the predictions with the r2 score\n",
    "r2 = r2_score(y_test, y_pred_phys, multioutput=\"raw_values\")\n",
    "\n",
    "for i, name in enumerate(targets):\n",
    "    print(f\"{name:>20}\\t{r2[i]:.4f}\")\n",
    "\n",
    "# Save the model with keras\n",
    "model.save(f\"../models/regressor_mlp_customloss_{target}.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mass residual histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1737.6099   352.61774  556.8931 ]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(np.sum(y_pred_naive, axis=0))\n",
    "\n",
    "mtot_est_naive = np.sum(y_pred_naive, axis=1)\n",
    "residual_naive = (x_test.values[:,0] - mtot_est_naive)**2.\n",
    "\n",
    "mtot_est_phys = np.sum(y_pred_phys, axis=1)\n",
    "residual_phys = (x_test.values[:,0] - mtot_est_phys)**2.\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12,4))\n",
    "\n",
    "ax.hist([residual_naive, residual_phys], bins=50, label=['Naive', 'Smart'])\n",
    "\n",
    "ax.set_xlabel('Mass Residual')\n",
    "\n",
    "ax.set_ylabel('N')\n",
    "ax.set_yscale('log')\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
