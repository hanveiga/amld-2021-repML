{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "going-typing",
   "metadata": {},
   "source": [
    "# AMLD 2021\n",
    "\n",
    "##### Machine Learning in Science: Encoding physical constraints and good development practices\n",
    "\n",
    "\n",
    "## Example 01 - Basic Reproducibility\n",
    "\n",
    "In this notebook, we start by demonstrating some of the more fundamental approaches to reproducability. The models that we will be training are simplified versions of the models used in a real astrophysics problem. Later in the workshop, we will improve these models to obey physical laws, such as the conservation of mass.\n",
    "\n",
    "### Workshop Organizers\n",
    "Dr. Maria Han Veiga (University of Michigan, USA)\n",
    "\n",
    "Dr. Miles Timpe (University of Zurich, Switzerland)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "billion-dialogue",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "strange-penalty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.7.9 (default, Aug 31 2020, 12:42:55) \n",
      "[GCC 7.3.0]\n"
     ]
    }
   ],
   "source": [
    "from joblib import dump, load\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "# We can define a global seed value to make our lives easier\n",
    "seed = 42\n",
    "\n",
    "# Set random seeds\n",
    "random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "\n",
    "# Versions can also influence reproducibility\n",
    "from sys import version\n",
    "print(f\"Python version: {version}\")\n",
    "\n",
    "\n",
    "# TensorFlow for the curious\n",
    "#import tensorflow as tf\n",
    "#tf.random.set_seed(seed) # TensorFlow2\n",
    "#tf.compat.v1.set_random_seed(random_seed)  # TensorFlow1\n",
    "#print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lyric-economy",
   "metadata": {},
   "source": [
    "### Load training data\n",
    "\n",
    "The datasets for this problem are relatively small by machine learning standards. Therefore, we provide them as CSV files. The train and test datasets together contain data on 14,856 pairwise collisions between planets. To keep things simple, we will only focus on three targets, which are subject to physical conservation laws; the mass of the largest remnant (\"lr_mass\"), the mass of the second largest remnant (\"slr_mass\"), and the mass of the collision debris (\"debris_mass\"). The mass of these three objects should match exactly the total mass that went into the collisions (\"mtotal\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "undefined-volunteer",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'lr_mass' #, 'slr_mass', 'debris_mass'\n",
    "\n",
    "features = ['collision_id', 'mtotal', 'gamma', 'b_inf', 'v_inf',\n",
    "            'targ_core_fraction', 'targ_omega', 'targ_theta', 'targ_phi',\n",
    "            'proj_core_fraction', 'proj_omega', 'proj_theta', 'proj_phi',\n",
    "            target]\n",
    "\n",
    "x_train = pd.read_csv('../datasets/train.csv', usecols=features)\n",
    "x_test  = pd.read_csv('../datasets/test.csv', usecols=features)\n",
    "\n",
    "y_train = x_train.pop(target)\n",
    "y_test  = x_test.pop(target)\n",
    "\n",
    "ids_train = list(x_train.pop('collision_id'))\n",
    "ids_test  = list(x_test.pop('collision_id'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legitimate-speed",
   "metadata": {},
   "source": [
    "### Scale data and save the scaler\n",
    "\n",
    "While most of the focus in machine learning is on the models, an important component of reproducability are the scaling methods. Here, we are using scikit-learn's standard scaler. Anyone that wants to reproduce our results will need to know exactly how the data was scaled prior to training. Therefore, once we have fit the scaler, we save it so that it can be loaded at a later date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dramatic-behalf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features\n",
    "x_scaler = preprocessing.StandardScaler()\n",
    "\n",
    "# Fit scaler to training data\n",
    "x_scaler.fit(x_train)\n",
    "\n",
    "# Save scaler\n",
    "dump(x_scaler, f\"x_scaler_{target}.joblib\")\n",
    "\n",
    "# Make sure to apply same scaler to train and test!\n",
    "scaled_x_train = x_scaler.transform(x_train)\n",
    "scaled_x_test  = x_scaler.transform(x_test)\n",
    "\n",
    "scaled_x_train = pd.DataFrame(scaled_x_train, columns=x_train.columns)\n",
    "scaled_x_test  = pd.DataFrame(scaled_x_test, columns=x_test.columns)\n",
    "\n",
    "del x_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "consecutive-reality",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale target\n",
    "y_scaler = preprocessing.StandardScaler()\n",
    "\n",
    "y_scaler.fit(y_train.values.reshape(-1, 1))\n",
    "\n",
    "# Save scaler\n",
    "dump(y_scaler, f\"y_scaler_{target}.joblib\")\n",
    "\n",
    "scaled_y_train = y_scaler.transform(y_train.values.reshape(-1, 1))\n",
    "scaled_y_test  = y_scaler.transform(y_test.values.reshape(-1, 1))\n",
    "\n",
    "scaled_y_train = pd.Series(data=np.squeeze(scaled_y_train), name=target)\n",
    "scaled_y_test  = pd.Series(data=np.squeeze(scaled_y_test), name=target)\n",
    "\n",
    "del y_scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advance-cleveland",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "\n",
    "In order to keep things simple, we are going to use a simple multi-layer perceptron (MLP), which is a type of feed-forward neural network. In a serious setting, we would employ more sophisticated training strategies, such as cross-validation and hyperparameter optimization, but for now we are going to skip those steps to keep the code readable. \n",
    "\n",
    "Notice that once the model has made its predictions, we have apply the inverse scaler to the predictions. In this case, we could have simply called on the scaler we created above, but here we show how to load the scaler object saved with joblib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "spare-banks",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction r2_score: 0.9881\n"
     ]
    }
   ],
   "source": [
    "# Define model (MLP)\n",
    "model = MLPRegressor(hidden_layer_sizes=[24,24,24],\n",
    "                     max_iter=1000, early_stopping=True,\n",
    "                     random_state=seed)\n",
    "\n",
    "\n",
    "# Fit model\n",
    "model.fit(scaled_x_train, scaled_y_train)\n",
    "\n",
    "# Make predictions\n",
    "scaled_y_pred = model.predict(scaled_x_test)\n",
    "\n",
    "# Apply inverse scaling to the model predictions\n",
    "y_scaler = load(f\"y_scaler_{target}.joblib\")\n",
    "\n",
    "y_pred = pd.Series(y_scaler.inverse_transform(scaled_y_pred), name='y_pred').values\n",
    "\n",
    "# Calculate the quality of the predictions with the r2 score\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "# Save the model with joblib\n",
    "dump(model, f\"regressor_mlp_{target}.joblib\") \n",
    "\n",
    "\n",
    "print(f\"Prediction r2_score: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controversial-medium",
   "metadata": {},
   "source": [
    "### A brief look at physical constraints: Does the vanilla model obey physics?\n",
    "\n",
    "While the data is physically consistent, our neural network has no knowledge of physical laws. Thus, its predictions will not necessarily obey even basic physical constraints (e.g., mass can't be negative). In order to make the predictions physically consistent, we can impose some quick and dirty constraints on the predictions after they have made. \n",
    "\n",
    "\n",
    "Here, we simply set any negative predictions to zero and any predictions greater than the total initial mass (\"mtot\") to mtot; any remaining mass is presumed to be in the debris. By doing this, we avoid violation to very fundamental physical principles: mass can't be negative and mass can't be created out of thin air (or the vacuum of space in this case). \n",
    "\n",
    "The simple constraints that we imposed have slightly improved the unconstrained r2 score. Later in the workshop, we will expand on these physical constraints and explore other methods for acheiving physical self-consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "knowing-danger",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Physics violations: 468 out of 2972\n",
      "Physics-aware r2 score: 0.9886\n"
     ]
    }
   ],
   "source": [
    "# Counter for violations\n",
    "i = 0\n",
    "\n",
    "y_pred_constrained = []\n",
    "\n",
    "for mtot, y in zip(x_test.mtotal, y_pred):\n",
    "    \n",
    "    # If mass is greater than the initial total mass\n",
    "    if y > mtot:\n",
    "        i += 1\n",
    "    # If mass is negative\n",
    "    elif y < 0:\n",
    "        i += 1\n",
    "        \n",
    "    y_pred_constrained.append(np.max([0.0, np.min([y, mtot])]))\n",
    "        \n",
    "        \n",
    "print(f\"Physics violations: {i} out of {len(y_pred)}\")\n",
    "\n",
    "r2_constrained = r2_score(y_test, y_pred_constrained)\n",
    "\n",
    "print(f\"Physics-aware r2 score: {r2_constrained:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinct-vacation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
