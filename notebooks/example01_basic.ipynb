{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "going-typing",
   "metadata": {},
   "source": [
    "# AMLD 2021\n",
    "\n",
    "##### Machine Learning in Science: Encoding physical constraints and good development practices\n",
    "\n",
    "\n",
    "## Example 01 - Basic Reproducability\n",
    "\n",
    "In this notebook, we start by demonstrating some of the more fundamental approaches to reproducability. The models that we will be training are simplified versions of the models used in a real astrophysics problem. Later in the workshop, we will improve these models to obey physical laws, such as the conservation of mass.\n",
    "\n",
    "### Workshop Organizers\n",
    "Dr. Maria Han Veiga (University of Michigan, USA)\n",
    "\n",
    "Dr. Miles Timpe (University of Zurich, Switzerland)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "billion-dialogue",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "strange-penalty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.7.9 (default, Aug 31 2020, 12:42:55) \n",
      "[GCC 7.3.0]\n"
     ]
    }
   ],
   "source": [
    "from joblib import dump, load\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import sklearn\n",
    "#import tensorflow as tf\n",
    "\n",
    "# We can define a global seed value to make our lives easier\n",
    "seed = 42\n",
    "\n",
    "# Set random seeds\n",
    "random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "np.random.seed(seed)  # NumPy\n",
    "#tf.random.set_seed(seed) # TensorFlow2\n",
    "#tf.compat.v1.set_random_seed(random_seed)  # TensorFlow1\n",
    "\n",
    "\n",
    "from sys import version\n",
    "print(f\"Python version: {version}\")\n",
    "#print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guilty-emphasis",
   "metadata": {},
   "source": [
    "### Load training data\n",
    "\n",
    "The datasets for this problem are relatively small by machine learning standards. Therefore, we provide them as CSV files. The train and test datasets together contain data on 14,856 pairwise collisions between planets. To keep things simple, we will only focus on three targets, which are subject to physical conservation laws; the mass of the largest remnant (\"lr_mass\"), the mass of the second largest remnant (\"slr_mass\"), and the mass of the collision debris (\"debris_mass\"). The mass of these three objects should match exactly the total mass that went into the collisions (\"mtotal\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "industrial-issue",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'lr_mass' #, 'slr_mass', 'debris_mass'\n",
    "\n",
    "features = ['collision_id', 'mtotal', 'gamma', 'b_inf', 'v_inf',\n",
    "            'targ_core_fraction', 'targ_omega', 'targ_theta', 'targ_phi',\n",
    "            'proj_core_fraction', 'proj_omega', 'proj_theta', 'proj_phi',\n",
    "            target]\n",
    "\n",
    "x_train = pd.read_csv('../datasets/train.csv', usecols=features)\n",
    "x_test  = pd.read_csv('../datasets/test.csv', usecols=features)\n",
    "\n",
    "y_train = x_train.pop(target)\n",
    "y_test  = x_test.pop(target)\n",
    "\n",
    "ids_train = list(x_train.pop('collision_id'))\n",
    "ids_test  = list(x_test.pop('collision_id'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latter-workstation",
   "metadata": {},
   "source": [
    "### Scale data and save the scaler\n",
    "\n",
    "While most of the focus in machine learning is on the models, an important component of reproducability are the scaling methods. Here, we are using scikit-learn's standard scaler. Anyone that wants to reproduce our results will need to know exactly how the data was scaled prior to training. Therefore, once we have fit the scaler, we save it so that it can be loaded at a later date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "regional-lightweight",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "# Use sklearn's StandardScaler\n",
    "x_scaler = preprocessing.StandardScaler()\n",
    "\n",
    "# Fit scaler to training data\n",
    "x_scaler.fit(x_train)\n",
    "\n",
    "# Save scalers\n",
    "dump(x_scaler, f\"x_scaler_{target}.joblib\")\n",
    "\n",
    "# Make sure to apply same scaler to train and test!\n",
    "scaled_x_train = x_scaler.transform(x_train)\n",
    "scaled_x_test  = x_scaler.transform(x_test)\n",
    "\n",
    "scaled_x_train = pd.DataFrame(scaled_x_train, columns=x_train.columns)\n",
    "scaled_x_test  = pd.DataFrame(scaled_x_test, columns=x_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "metallic-genesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale target\n",
    "y_scaler = preprocessing.StandardScaler()\n",
    "\n",
    "y_scaler.fit(y_train.values.reshape(-1, 1))\n",
    "\n",
    "scaled_y_train = y_scaler.transform(y_train.values.reshape(-1, 1))\n",
    "scaled_y_test  = y_scaler.transform(y_test.values.reshape(-1, 1))\n",
    "\n",
    "scaled_y_train = pd.Series(data=np.squeeze(scaled_y_train), name=target)\n",
    "scaled_y_test  = pd.Series(data=np.squeeze(scaled_y_test), name=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cognitive-gates",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-5-91ea661a23c9>, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-91ea661a23c9>\"\u001b[0;36m, line \u001b[0;32m8\u001b[0m\n\u001b[0;31m    dump(x_scaler, '{}/x_scaler_{:05d}.joblib'.format(\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# Add back IDs after scaling\n",
    "scaled_x_train['collision_id'] = ids_train\n",
    "scaled_x_test['collision_id']  = ids_test\n",
    "\n",
    "scaled_x_train.head()\n",
    "\n",
    "\n",
    "dump(y_scaler, f\"y_scaler_{target}.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technological-excerpt",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
